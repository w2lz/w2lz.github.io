[{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文深入探讨了 MySQL 高可用性保障的技术细节和应对策略。","date":"2025-02-16","objectID":"/posts/25.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84/","tags":["MySQL 实战 45 讲","MySQL"],"title":"25 | MySQL 是怎么保证高可用的？","uri":"/posts/25.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文深入探讨了 MySQL 高可用性保障的技术细节和应对策略。主要围绕主备库之间的数据同步和切换展开讨论，重点介绍了主备延迟的概念及可能的来源，包括备库性能差、备库压力大、大事务和备库的并行复制能力。针对主备切换过程中的不可用时间，文章提出了可靠性优先策略，并详细描述了切换流程。 ","date":"2025-02-16","objectID":"/posts/25.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"25 | MySQL 是怎么保证高可用的？","uri":"/posts/25.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 主备延迟 ","date":"2025-02-16","objectID":"/posts/25.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"25 | MySQL 是怎么保证高可用的？","uri":"/posts/25.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 主备延迟的来源 ","date":"2025-02-16","objectID":"/posts/25.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"25 | MySQL 是怎么保证高可用的？","uri":"/posts/25.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 可靠性优先策略 ","date":"2025-02-16","objectID":"/posts/25.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"25 | MySQL 是怎么保证高可用的？","uri":"/posts/25.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 可用性优先策略 ","date":"2025-02-16","objectID":"/posts/25.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"25 | MySQL 是怎么保证高可用的？","uri":"/posts/25.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 小结 ","date":"2025-02-16","objectID":"/posts/25.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"25 | MySQL 是怎么保证高可用的？","uri":"/posts/25.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"6 问题 ","date":"2025-02-16","objectID":"/posts/25.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84/:6:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"25 | MySQL 是怎么保证高可用的？","uri":"/posts/25.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文深入介绍了 MySQL 主备同步的基本原理和技术细节，重点围绕 binlog 内容、备库执行 binlog 与主库保持一致的原理展开。","date":"2025-02-16","objectID":"/posts/24.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84/","tags":["MySQL 实战 45 讲","MySQL"],"title":"24 | MySQL 是怎么保证主备一致的？","uri":"/posts/24.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文深入介绍了 MySQL 主备同步的基本原理和技术细节，重点围绕 binlog 内容、备库执行 binlog 与主库保持一致的原理展开。详细解释了主备切换流程、节点间的同步更新流程以及 binlog 的三种格式的特点和应用场景。 ","date":"2025-02-16","objectID":"/posts/24.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"24 | MySQL 是怎么保证主备一致的？","uri":"/posts/24.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 MySQL 主备的基本原理 ","date":"2025-02-16","objectID":"/posts/24.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"24 | MySQL 是怎么保证主备一致的？","uri":"/posts/24.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 binlog 的三种格式对比 ","date":"2025-02-16","objectID":"/posts/24.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"24 | MySQL 是怎么保证主备一致的？","uri":"/posts/24.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 为什么会有 mixed 格式的 binlog？ ","date":"2025-02-16","objectID":"/posts/24.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"24 | MySQL 是怎么保证主备一致的？","uri":"/posts/24.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 循环复制问题 ","date":"2025-02-16","objectID":"/posts/24.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"24 | MySQL 是怎么保证主备一致的？","uri":"/posts/24.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 小结 ","date":"2025-02-16","objectID":"/posts/24.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"24 | MySQL 是怎么保证主备一致的？","uri":"/posts/24.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"6 问题 ","date":"2025-02-16","objectID":"/posts/24.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84/:6:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"24 | MySQL 是怎么保证主备一致的？","uri":"/posts/24.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文深入介绍了 MySQL 数据保证机制的技术特点，主要围绕 binlog 的写入流程和相关参数的设置展开。","date":"2025-02-16","objectID":"/posts/23.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E7%9A%84/","tags":["MySQL 实战 45 讲","MySQL"],"title":"23 | MySQL 是怎么保证数据不丢的？","uri":"/posts/23.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 MySQL 数据保证机制的技术特点主要围绕 binlog 的写入流程和相关参数的设置展开。在 binlog 的写入机制中，事务执行过程中先将日志写入 binlog cache，事务提交时再将 binlog cache 写入 binlog 文件中。每个线程有自己的 binlog cache，但共用同一份 binlog 文件。 ","date":"2025-02-16","objectID":"/posts/23.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E7%9A%84/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"23 | MySQL 是怎么保证数据不丢的？","uri":"/posts/23.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 binlog 的写入机制 ","date":"2025-02-16","objectID":"/posts/23.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E7%9A%84/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"23 | MySQL 是怎么保证数据不丢的？","uri":"/posts/23.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 redo log 的写入机制 ","date":"2025-02-16","objectID":"/posts/23.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E7%9A%84/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"23 | MySQL 是怎么保证数据不丢的？","uri":"/posts/23.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 小结 ","date":"2025-02-16","objectID":"/posts/23.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E7%9A%84/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"23 | MySQL 是怎么保证数据不丢的？","uri":"/posts/23.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 问题 ","date":"2025-02-16","objectID":"/posts/23.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E7%9A%84/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"23 | MySQL 是怎么保证数据不丢的？","uri":"/posts/23.mysql%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文深入探讨了 MySQL 性能问题的解决方案，针对短连接风暴和查询更新语句导致的性能问题提出了解决方法。","date":"2025-02-16","objectID":"/posts/22.mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%A5%AE%E9%B8%A9%E6%AD%A2%E6%B8%B4%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%B9%E6%B3%95/","tags":["MySQL 实战 45 讲","MySQL"],"title":"22 | MySQL 有哪些“饮鸩止渴”提高性能的方法？","uri":"/posts/22.mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%A5%AE%E9%B8%A9%E6%AD%A2%E6%B8%B4%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%B9%E6%B3%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文深入探讨了 MySQL 性能问题的解决方案，针对短连接风暴和查询更新语句导致的性能问题提出了解决方法。对于短连接风暴可能导致的连接数暴涨问题，提出了通过 kill connection 命令断开不工作的线程或者重启数据库并使用–skip-grant-tables 参数跳过权限验证阶段的方法，但强调了这些方法可能存在的风险和损失。针对查询和更新语句导致的性能问题，文章提出了通过创建索引、改写 SQL 语句以及使用 force index 等方法来解决。 ","date":"2025-02-16","objectID":"/posts/22.mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%A5%AE%E9%B8%A9%E6%AD%A2%E6%B8%B4%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%B9%E6%B3%95/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"22 | MySQL 有哪些“饮鸩止渴”提高性能的方法？","uri":"/posts/22.mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%A5%AE%E9%B8%A9%E6%AD%A2%E6%B8%B4%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%B9%E6%B3%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 短连接风暴 ","date":"2025-02-16","objectID":"/posts/22.mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%A5%AE%E9%B8%A9%E6%AD%A2%E6%B8%B4%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%B9%E6%B3%95/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"22 | MySQL 有哪些“饮鸩止渴”提高性能的方法？","uri":"/posts/22.mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%A5%AE%E9%B8%A9%E6%AD%A2%E6%B8%B4%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%B9%E6%B3%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 慢查询性能问题 ","date":"2025-02-16","objectID":"/posts/22.mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%A5%AE%E9%B8%A9%E6%AD%A2%E6%B8%B4%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%B9%E6%B3%95/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"22 | MySQL 有哪些“饮鸩止渴”提高性能的方法？","uri":"/posts/22.mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%A5%AE%E9%B8%A9%E6%AD%A2%E6%B8%B4%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%B9%E6%B3%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 QPS 突增问题 ","date":"2025-02-16","objectID":"/posts/22.mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%A5%AE%E9%B8%A9%E6%AD%A2%E6%B8%B4%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%B9%E6%B3%95/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"22 | MySQL 有哪些“饮鸩止渴”提高性能的方法？","uri":"/posts/22.mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%A5%AE%E9%B8%A9%E6%AD%A2%E6%B8%B4%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%B9%E6%B3%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 小结 ","date":"2025-02-16","objectID":"/posts/22.mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%A5%AE%E9%B8%A9%E6%AD%A2%E6%B8%B4%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%B9%E6%B3%95/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"22 | MySQL 有哪些“饮鸩止渴”提高性能的方法？","uri":"/posts/22.mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%A5%AE%E9%B8%A9%E6%AD%A2%E6%B8%B4%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%B9%E6%B3%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 问题 ","date":"2025-02-16","objectID":"/posts/22.mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%A5%AE%E9%B8%A9%E6%AD%A2%E6%B8%B4%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%B9%E6%B3%95/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"22 | MySQL 有哪些“饮鸩止渴”提高性能的方法？","uri":"/posts/22.mysql%E6%9C%89%E5%93%AA%E4%BA%9B%E9%A5%AE%E9%B8%A9%E6%AD%A2%E6%B8%B4%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%B9%E6%B3%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文深入探讨了 MySQL 加锁规则，通过详细解释加锁规则的原则、优化和 bug，以及介绍了间隙锁和 next-key lock 的概念。","date":"2025-02-16","objectID":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/","tags":["MySQL 实战 45 讲","MySQL"],"title":"21 | 为什么我只改一行的语句，锁这么多？","uri":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文深入探讨了 MySQL 加锁规则，通过详细解释加锁规则的原则、优化和 bug，以及介绍了间隙锁和 next-key lock 的概念。通过案例分析，读者可以了解 MySQL 在不同情况下的加锁方式，以及加锁规则对并发操作的影响。 ","date":"2025-02-16","objectID":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"21 | 为什么我只改一行的语句，锁这么多？","uri":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 案例一：等值查询间隙锁 ","date":"2025-02-16","objectID":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"21 | 为什么我只改一行的语句，锁这么多？","uri":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 案例二：非唯一索引等值锁 ","date":"2025-02-16","objectID":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"21 | 为什么我只改一行的语句，锁这么多？","uri":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 案例三：主键索引范围锁 ","date":"2025-02-16","objectID":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"21 | 为什么我只改一行的语句，锁这么多？","uri":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 案例四：非唯一索引范围锁 ","date":"2025-02-16","objectID":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"21 | 为什么我只改一行的语句，锁这么多？","uri":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 案例五：唯一索引范围锁 bug ","date":"2025-02-16","objectID":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"21 | 为什么我只改一行的语句，锁这么多？","uri":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"6 案例六：非唯一索引上存在\"等值\"的例子 ","date":"2025-02-16","objectID":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/:6:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"21 | 为什么我只改一行的语句，锁这么多？","uri":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"7 案例七：limit 语句加锁 ","date":"2025-02-16","objectID":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/:7:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"21 | 为什么我只改一行的语句，锁这么多？","uri":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"8 案例八：一个死锁的例子 ","date":"2025-02-16","objectID":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/:8:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"21 | 为什么我只改一行的语句，锁这么多？","uri":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"9 小结 ","date":"2025-02-16","objectID":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/:9:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"21 | 为什么我只改一行的语句，锁这么多？","uri":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"10 问题 ","date":"2025-02-16","objectID":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/:10:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"21 | 为什么我只改一行的语句，锁这么多？","uri":"/posts/21.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文深入探讨了数据库中的幻读问题及其解决方案。","date":"2025-02-16","objectID":"/posts/20.%E5%B9%BB%E8%AF%BB%E6%98%AF%E4%BB%80%E4%B9%88%E5%B9%BB%E8%AF%BB%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98/","tags":["MySQL 实战 45 讲","MySQL"],"title":"20 | 幻读是什么，幻读有什么问题？","uri":"/posts/20.%E5%B9%BB%E8%AF%BB%E6%98%AF%E4%BB%80%E4%B9%88%E5%B9%BB%E8%AF%BB%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文深入探讨了数据库中的幻读问题及其解决方案。以一个小表为例，通过对加锁规则的讨论引出了幻读问题，并分析了在可重复读隔离级别下可能导致的幻读现象。通过具体的例子和图示，生动地展示了幻读的产生和影响。 为了便于说明问题，这一篇文章先使用一个小一点儿的表。建表和初始化语句如下： CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25); 这个表除了主键 id 外，还有一个索引 c，初始化语句在表中插入了 6 行数据。下面的语句序列，是怎么加锁的，加的锁又是什么时候释放的呢？ begin; select * from t where d=5 for update; commit; 比较好理解的是，这个语句会命中 d=5 的这一行，对应的主键 id=5，因此在 select 语句执行完成后，id=5 这一行会加一个写锁，而且由于两阶段锁协议，这个写锁会在执行 commit 语句的时候释放。 由于字段 d 上没有索引，因此这条查询语句会做全表扫描。那么，其他被扫描到的，但是不满足条件的 5 行记录上，会不会被加锁呢？ InnoDB 的默认事务隔离级别是可重复读，所以本文接下来没有特殊说明的部分，都是设定在可重复读隔离级别下。 ","date":"2025-02-16","objectID":"/posts/20.%E5%B9%BB%E8%AF%BB%E6%98%AF%E4%BB%80%E4%B9%88%E5%B9%BB%E8%AF%BB%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"20 | 幻读是什么，幻读有什么问题？","uri":"/posts/20.%E5%B9%BB%E8%AF%BB%E6%98%AF%E4%BB%80%E4%B9%88%E5%B9%BB%E8%AF%BB%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 幻读是什么？ 现在就来分析一下，如果只在 id=5 这一行加锁，而其他行的不加锁的话，会怎么样。下面先来看一下这个假设的场景： 可以看到，session A 里执行了三次查询，分别是 Q1、Q2 和 Q3。它们的 SQL 语句相同，都是 select * from t where d=5 for update。这个语句的意思你应该很清楚了，查所有 d=5 的行，而且使用的是当前读，并且加上写锁。现在，来看一下这三条 SQL 语句，分别会返回什么结果。 Q1 只返回 id=5 这一行； 在 T2 时刻，session B 把 id=0 这一行的 d 值改成了 5，因此 T3 时刻 Q2 查出来的是 id=0 和 id=5 这两行； 在 T4 时刻，session C 又插入一行（1,1,5），因此 T5 时刻 Q3 查出来的是 id=0、id=1 和 id=5 的这三行。 其中，Q3 读到 id=1 这一行的现象，被称为“幻读”。也就是说，幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。这里，对“幻读”做一个说明： 在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。 上面 session B 的修改结果，被 session A 之后的 select 语句用“当前读”看到，不能称为幻读。幻读仅专指“新插入的行”。 如果从之前学到的事务可见性规则来分析的话，上面这三条 SQL 语句的返回结果都没有问题。因为这三个查询都是加了 for update，都是当前读。而当前读的规则，就是要能读到所有已经提交的记录的最新值。并且，session B 和 sessionC 的两条语句，执行后就会提交，所以 Q2 和 Q3 就是应该看到这两个事务的操作效果，而且也看到了，这跟事务的可见性规则并不矛盾。但是，这是不是真的没问题呢？不，这里还真就有问题。 ","date":"2025-02-16","objectID":"/posts/20.%E5%B9%BB%E8%AF%BB%E6%98%AF%E4%BB%80%E4%B9%88%E5%B9%BB%E8%AF%BB%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"20 | 幻读是什么，幻读有什么问题？","uri":"/posts/20.%E5%B9%BB%E8%AF%BB%E6%98%AF%E4%BB%80%E4%B9%88%E5%B9%BB%E8%AF%BB%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 幻读有什么问题？ 首先是语义上的。session A 在 T1 时刻就声明了，“我要把所有 d=5 的行锁住，不准别的事务进行读写操作”。而实际上，这个语义被破坏了。 如果现在这样看感觉还不明显的话，再往 session B 和 session C 里面分别加一条 SQL 语句，再看看会出现什么现象。 session B 的第二条语句 update t set c=5 where id=0，语义是“我把 id=0、d=5 这一行的 c 值，改成了 5”。 由于在 T1 时刻，session A 还只是给 id=5 这一行加了行锁，并没有给 id=0 这行加上锁。因此，session B 在 T2 时刻，是可以执行这两条 update 语句的。这样，就破坏了 session A 里 Q1 语句要锁住所有 d=5 的行的加锁声明。session C 也是一样的道理，对 id=1 这一行的修改，也是破坏了 Q1 的加锁声明。 其次，是数据一致性的问题。 锁的设计是为了保证数据的一致性。而这个一致性，不止是数据库内部数据状态在此刻的一致性，还包含了数据和日志在逻辑上的一致性。为了说明这个问题，给 session A 在 T1 时刻再加一个更新语句，即：update t set d=100 where d=5。 update 的加锁语义和 select …for update 是一致的，所以这时候加上这条 update 语句也很合理。session A 声明说“要给 d=5 的语句加上锁”，就是为了要更新数据，新加的这条 update 语句就是把它认为加上了锁的这一行的 d 值修改成了 100。 现在，我们来分析一下上图的流程执行完成后，数据库里会是什么结果。 经过 T1 时刻，id=5 这一行变成 (5,5,100)，当然这个结果最终是在 T6 时刻正式提交的 ; 经过 T2 时刻，id=0 这一行变成 (0,5,5); 经过 T4 时刻，表里面多了一行 (1,5,5); 其他行跟这个执行序列无关，保持不变。 这样看，这些数据也没啥问题，但是我们再来看看这时候 binlog 里面的内容。 T2 时刻，session B 事务提交，写入了两条语句； T4 时刻，session C 事务提交，写入了两条语句； T6 时刻，session A 事务提交，写入了 update t set d=100 where d=5 这条语句。 统一放到一起的话，就是这样的： update t set d=5 where id=0; /*(0,0,5)*/ update t set c=5 where id=0; /*(0,5,5)*/ insert into t values(1,1,5); /*(1,1,5)*/ update t set c=5 where id=1; /*(1,5,5)*/ update t set d=100 where d=5;/*所有 d=5 的行，d 改成 100*/ 你应该看出问题了。这个语句序列，不论是拿到备库去执行，还是以后用 binlog 来克隆一个库，这三行的结果，都变成了 (0,5,100)、(1,5,100) 和 (5,5,100)。也就是说，id=0 和 id=1 这两行，发生了数据不一致。这个问题很严重，是不行的。到这里，再回顾一下，这个数据不一致到底是怎么引入的？ 分析一下可以知道，这是我们假设“select * from t where d=5 for update 这条语句只给 d=5 这一行，也就是 id=5 的这一行加锁”导致的。所以我们认为，上面的设定不合理，要改。那怎么改呢？可以把扫描过程中碰到的行，也都加上写锁，再来看看执行效果。 由于 session A 把所有的行都加了写锁，所以 session B 在执行第一个 update 语句的时候就被锁住了。需要等到 T6 时刻 session A 提交以后，session B 才能继续执行。 这样对于 id=0 这一行，在数据库里的最终结果还是 (0,5,5)。在 binlog 里面，执行序列是这样的： insert into t values(1,1,5); /*(1,1,5)*/ update t set c=5 where id=1; /*(1,5,5)*/ update t set d=100 where d=5;/*所有 d=5 的行，d 改成 100*/ update t set d=5 where id=0; /*(0,0,5)*/ update t set c=5 where id=0; /*(0,5,5)*/ 可以看到，按照日志顺序执行，id=0 这一行的最终结果也是 (0,5,5)。所以，id=0 这一行的问题解决了。 但同时你也可以看到，id=1 这一行，在数据库里面的结果是 (1,5,5)，而根据 binlog 的执行结果是 (1,5,100)，也就是说幻读的问题还是没有解决。为什么已经这么“凶残”地，把所有的记录都上了锁，还是阻止不了 id=1 这一行的插入和更新呢？ 原因很简单。在 T3 时刻，给所有行加锁的时候，id=1 这一行还不存在，不存在也就加不上锁。也就是说，即使把所有的记录都加上锁，还是阻止不了新插入的记录，这也是为什么“幻读”会被单独拿出来解决的原因。接下来，再看看 InnoDB 怎么解决幻读的问题。 ","date":"2025-02-16","objectID":"/posts/20.%E5%B9%BB%E8%AF%BB%E6%98%AF%E4%BB%80%E4%B9%88%E5%B9%BB%E8%AF%BB%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"20 | 幻读是什么，幻读有什么问题？","uri":"/posts/20.%E5%B9%BB%E8%AF%BB%E6%98%AF%E4%BB%80%E4%B9%88%E5%B9%BB%E8%AF%BB%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 如何解决幻读？ ","date":"2025-02-16","objectID":"/posts/20.%E5%B9%BB%E8%AF%BB%E6%98%AF%E4%BB%80%E4%B9%88%E5%B9%BB%E8%AF%BB%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"20 | 幻读是什么，幻读有什么问题？","uri":"/posts/20.%E5%B9%BB%E8%AF%BB%E6%98%AF%E4%BB%80%E4%B9%88%E5%B9%BB%E8%AF%BB%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 小结 ","date":"2025-02-16","objectID":"/posts/20.%E5%B9%BB%E8%AF%BB%E6%98%AF%E4%BB%80%E4%B9%88%E5%B9%BB%E8%AF%BB%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"20 | 幻读是什么，幻读有什么问题？","uri":"/posts/20.%E5%B9%BB%E8%AF%BB%E6%98%AF%E4%BB%80%E4%B9%88%E5%B9%BB%E8%AF%BB%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 问题 ","date":"2025-02-16","objectID":"/posts/20.%E5%B9%BB%E8%AF%BB%E6%98%AF%E4%BB%80%E4%B9%88%E5%B9%BB%E8%AF%BB%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"20 | 幻读是什么，幻读有什么问题？","uri":"/posts/20.%E5%B9%BB%E8%AF%BB%E6%98%AF%E4%BB%80%E4%B9%88%E5%B9%BB%E8%AF%BB%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文深入探讨了查询性能优化中可能出现的问题及解决方法。","date":"2025-02-16","objectID":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/","tags":["MySQL 实战 45 讲","MySQL"],"title":"19 | 为什么我只查一行的语句，也执行这么慢？","uri":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文深入探讨了查询性能优化中可能出现的问题及解决方法。通过构造一个包含 10 万行记录的表，展示了即使是查询单行数据，也可能出现执行缓慢的情况。文章详细分析了表被锁住的情况，包括等 MDL 锁、等 flush 或等行锁导致的情况，并提供了相应的处理方法。 一般情况下，如果说查询性能优化，你首先会想到一些复杂的语句，想到查询需要返回大量的数据。但有些情况下，“查一行”，也会执行得特别慢。 需要说明的是，如果 MySQL 数据库本身就有很大的压力，导致数据库服务器 CPU 占用率很高或 ioutil（IO 利用率）很高，这种情况下所有语句的执行都有可能变慢，不属于今天的讨论范围。 为了便于描述，还是构造一个表，基于这个表来说明今天的问题。这个表有两个字段 id 和 c，并且在里面插入了 10 万行记录。 mysql\u003e CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; delimiter ;; create procedure idata() begin declare i int; set i=1; while(i\u003c=100000) do insert into t values(i,i); set i=i+1; end while; end;; delimiter ; call idata(); ","date":"2025-02-16","objectID":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"19 | 为什么我只查一行的语句，也执行这么慢？","uri":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 第一类：查询长时间不返回 如下图所示，在表 t 执行下面的 SQL 语句： mysql\u003e select * from t where id=1; 查询结果长时间不返回。 一般碰到这种情况的话，大概率是表 t 被锁住了。接下来分析原因的时候，一般都是首先执行一下 show processlist 命令，看看当前语句处于什么状态。 然后再针对每种状态，去分析它们产生的原因、如何复现，以及如何处理。 ","date":"2025-02-16","objectID":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"19 | 为什么我只查一行的语句，也执行这么慢？","uri":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 等 MDL 锁 如下图所示，就是使用 show processlist 命令查看 Waiting for table metadata lock 的示意图。 出现这个状态表示的是，现在有一个线程正在表 t 上请求或者持有 MDL 写锁，把 select 语句堵住了。 下面来复现一下这个场景，下图就是一个简单的人复现步骤。 session A 通过 lock table 命令持有表 t 的 MDL 写锁，而 session B 的查询需要获取 MDL 读锁。所以，session B 进入等待状态。这类问题的处理方式，就是找到谁持有 MDL 写锁，然后把它 kill 掉。 但是，由于在 show processlist 的结果里面，session A 的 Command 列是“Sleep”，导致查找起来很不方便。不过有了 performance_schema 和 sys 系统库以后，就方便多了。（MySQL 启动时需要设置 performance_schema=on，相比于设置为 off 会有 10% 左右的性能损失) 通过查询 sys.schema_table_lock_waits 这张表，就可以直接找出造成阻塞的 process id，把这个连接用 kill 命令断开即可。 ","date":"2025-02-16","objectID":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"19 | 为什么我只查一行的语句，也执行这么慢？","uri":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 等 flush 接下来说一下另外一种查询被堵住的情况。在表 t 上，执行下面的 SQL 语句： mysql\u003e select * from information_schema.processlist where id=1; 可以看一下下图。查出来这个线程的状态是 Waiting for table flush，可以设想一下这是什么原因。 这个状态表示的是，现在有一个线程正要对表 t 做 flush 操作。MySQL 里面对表做 flush 操作的用法，一般有以下两个： flush tables t with read lock; flush tables with read lock; 这两个 flush 语句，如果指定表 t 的话，代表的是只关闭表 t；如果没有指定具体的表名，则表示关闭 MySQL 里所有打开的表。但是正常这两个语句执行起来都很快，除非它们也被别的线程堵住了。 所以，出现 Waiting for table flush 状态的可能情况是：有一个 flush tables 命令被别的语句堵住了，然后它又堵住了我们的 select 语句。现在一起来复现一下这种情况，复现步骤如下图所示： 在 session A 中，故意每行都调用一次 sleep(1)，这样这个语句默认要执行 10 万秒，在这期间表 t 一直是被 session A“打开”着。然后，session B 的 flush tables t 命令再要去关闭表 t，就需要等 session A 的查询结束。这样，session C 要再次查询的话，就会被 flush 命令堵住了。 下图是这个复现步骤的 show processlist 结果。这个例子的排查也很简单，看到这个 show processlist 的结果，肯定就知道应该怎么做了。 ","date":"2025-02-16","objectID":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"19 | 为什么我只查一行的语句，也执行这么慢？","uri":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 等行锁 现在，经过了表级锁的考验，我们的 select 语句终于来到引擎里了。 mysql\u003e select * from t where id=1 lock in share mode; 由于访问 id=1 这个记录时要加读锁，如果这时候已经有一个事务在这行记录上持有一个写锁，我们的 select 语句就会被堵住。复现步骤和现场如下： 显然，session A 启动了事务，占有写锁，还不提交，是导致 session B 被堵住的原因。 这个问题并不难分析，但问题是怎么查出是谁占着这个写锁。如果你用的是 MySQL 5.7 版本，可以通过 sys.innodb_lock_waits 表查到。查询方法是： mysql\u003e select * from t sys.innodb_lock_waits where locked_table='`test`.`t`'\\G 可以看到，这个信息很全，4 号线程是造成堵塞的罪魁祸首。而干掉这个罪魁祸首的方式，就是 KILL QUERY 4 或 KILL 4。 不过，这里不应该显示“KILL QUERY 4”。这个命令表示停止 4 号线程当前正在执行的语句，而这个方法其实是没有用的。因为占有行锁的是 update 语句，这个语句已经是之前执行完成了的，现在执行 KILL QUERY，无法让这个事务去掉 id=1 上的行锁。 实际上，KILL 4 才有效，也就是说直接断开这个连接。这里隐含的一个逻辑就是，连接被断开的时候，会自动回滚这个连接里面正在执行的线程，也就释放了 id=1 上的行锁。 ","date":"2025-02-16","objectID":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"19 | 为什么我只查一行的语句，也执行这么慢？","uri":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 第二类：查询慢 经过了重重封“锁”，再来看看一些查询慢的例子。先来看一条你一定知道原因的 SQL 语句： mysql\u003e select * from t where c=50000 limit 1; 由于字段 c 上没有索引，这个语句只能走 id 主键顺序扫描，因此需要扫描 5 万行。作为确认，可以看一下慢查询日志。注意，这里为了把所有语句记录到 slow log 里，在连接后先执行了 set long_query_time=0，将慢查询日志的时间阈值设置为 0。 Rows_examined 显示扫描了 50000 行。你可能会说，不是很慢呀，11.5 毫秒就返回了，线上一般都配置超过 1 秒才算慢查询。但你要记住：坏查询不一定是慢查询。这个例子里面只有 10 万行记录，数据量大起来的话，执行时间就线性涨上去了。扫描行数多，所以执行慢，这个很好理解。 但是接下来，再看一个只扫描一行，但是执行很慢的语句。如下图所示，是这个例子的 slow log。可以看到，执行的语句是 mysql\u003e select * from t where id=1； 虽然扫描行数是 1，但执行时间却长达 800 毫秒。 是不是有点奇怪呢，这些时间都花在哪里了？如果把这个 slow log 的截图再往下拉一点，可以看到下一个语句，select * from t where id=1 lock in share mode，执行时扫描行数也是 1 行，执行时间是 0.2 毫秒。 看上去是不是更奇怪了？按理说 lock in share mode 还要加锁，时间应该更长才对啊。下图是这两个语句的执行输出结果。 第一个语句的查询结果里 c=1，带 lock in share mode 的语句返回的是 c=1000001。下图是复现步骤。 可以看到，session A 先用 start transaction with consistent snapshot 命令启动了一个事务，之后 session B 才开始执行 update 语句。 session B 执行完 100 万次 update 语句后，id=1 这一行处于什么状态呢？可以从下图中找到答案。 session B 更新完 100 万次，生成了 100 万个回滚日志 (undo log)。 带 lock in share mode 的 SQL 语句，是当前读，因此会直接读到 1000001 这个结果，所以速度很快；而 select * from t where id=1 这个语句，是一致性读，因此需要从 1000001 开始，依次执行 undo log，执行了 100 万次以后，才将 1 这个结果返回。 注意，undo log 里记录的其实是“把 2 改成 1”，“把 3 改成 2”这样的操作逻辑，画成减 1 的目的是方便你看图。 ","date":"2025-02-16","objectID":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"19 | 为什么我只查一行的语句，也执行这么慢？","uri":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"6 小结 在一个简单的表上，执行“查一行”，可能会出现的被锁住和执行慢的例子。这其中涉及到了表锁、行锁和一致性读的概念。 ","date":"2025-02-16","objectID":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/:6:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"19 | 为什么我只查一行的语句，也执行这么慢？","uri":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"7 问题 问：在举例加锁读的时候，用的是这个语句，select * from t where id=1 lock in share mode。由于 id 上有索引，所以可以直接定位到 id=1 这一行，因此读锁也是只加在了这一行上。但如果是下面的 SQL 语句， begin; select * from t where c=5 for update; commit; 这个语句序列是怎么加锁的呢？加的锁又是什么时候释放呢？ ","date":"2025-02-16","objectID":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/:7:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"19 | 为什么我只查一行的语句，也执行这么慢？","uri":"/posts/19.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文深入探讨了 MySQL 中相似逻辑的 SQL 语句却存在巨大性能差异的原因。","date":"2025-02-16","objectID":"/posts/18.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%BA%9Bsql%E8%AF%AD%E5%8F%A5%E9%80%BB%E8%BE%91%E7%9B%B8%E5%90%8C%E6%80%A7%E8%83%BD%E5%8D%B4%E5%B7%AE%E5%BC%82%E5%B7%A8%E5%A4%A7/","tags":["MySQL 实战 45 讲","MySQL"],"title":"18 | 为什么这些 SQL 语句逻辑相同，性能却差异巨大？","uri":"/posts/18.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%BA%9Bsql%E8%AF%AD%E5%8F%A5%E9%80%BB%E8%BE%91%E7%9B%B8%E5%90%8C%E6%80%A7%E8%83%BD%E5%8D%B4%E5%B7%AE%E5%BC%82%E5%B7%A8%E5%A4%A7/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文深入探讨了 MySQL 中相似逻辑的 SQL 语句却存在巨大性能差异的原因。通过具体案例分析和图示，解释了对索引字段进行函数操作可能导致性能下降的情况，以及隐式类型转换对性能的影响。 在 MySQL 中，有很多看上去逻辑相同，但性能却差异巨大的 SQL 语句。对这些语句使用不当的话，就会不经意间导致整个数据库的压力变大。 ","date":"2025-02-16","objectID":"/posts/18.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%BA%9Bsql%E8%AF%AD%E5%8F%A5%E9%80%BB%E8%BE%91%E7%9B%B8%E5%90%8C%E6%80%A7%E8%83%BD%E5%8D%B4%E5%B7%AE%E5%BC%82%E5%B7%A8%E5%A4%A7/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"18 | 为什么这些 SQL 语句逻辑相同，性能却差异巨大？","uri":"/posts/18.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%BA%9Bsql%E8%AF%AD%E5%8F%A5%E9%80%BB%E8%BE%91%E7%9B%B8%E5%90%8C%E6%80%A7%E8%83%BD%E5%8D%B4%E5%B7%AE%E5%BC%82%E5%B7%A8%E5%A4%A7/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 案例一：条件字段函数操作 假设你现在维护了一个交易系统，其中交易记录表 tradelog 包含交易流水号（tradeid）、交易员 id（operator）、交易时间（t_modified）等字段。为了便于描述，先忽略其他字段。这个表的建表语句如下： mysql\u003e CREATE TABLE `tradelog` ( `id` int(11) NOT NULL, `tradeid` varchar(32) DEFAULT NULL, `operator` int(11) DEFAULT NULL, `t_modified` datetime DEFAULT NULL, PRIMARY KEY (`id`), KEY `tradeid` (`tradeid`), KEY `t_modified` (`t_modified`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 假设，现在已经记录了从 2016 年初到 2018 年底的所有数据，运营部门有一个需求是，要统计发生在所有年份中 7 月份的交易记录总数。这个逻辑看上去并不复杂，SQL 语句可能会这么写： mysql\u003e select count(*) from tradelog where month(t_modified)=7; 由于 t_modified 字段上有索引，于是就很放心地在生产库中执行了这条语句，但却发现执行了特别久，才返回了结果。 如果你问 DBA 同事为什么会出现这样的情况，他大概会告诉你：如果对字段做了函数计算，就用不上索引了，这是 MySQL 的规定。为什么条件是 where t_modified=‘2018-7-1’的时候可以用上索引，而改成 where month(t_modified)=7 的时候就不行了？ 下面是这个 t_modified 索引的示意图。方框上面的数字就是 month() 函数对应的值。 如果 SQL 语句条件用的是 where t_modified=‘2018-7-1’的话，引擎就会按照上面绿色箭头的路线，快速定位到 t_modified=‘2018-7-1’需要的结果。 实际上，B+ 树提供的这个快速定位能力，来源于同一层兄弟节点的有序性。 但是，如果计算 month() 函数的话，你会看到传入 7 的时候，在树的第一层就不知道该怎么办了。 也就是说，对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。 需要注意的是，优化器并不是要放弃使用这个索引。 在这个例子里，放弃了树搜索功能，优化器可以选择遍历主键索引，也可以选择遍历索引 t_modified，优化器对比索引大小后发现，索引 t_modified 更小，遍历这个索引比遍历主键索引来得更快。因此最终还是会选择索引 t_modified。 接下来，使用 explain 命令，查看一下这条 SQL 语句的执行结果。 key=“t_modified\"表示的是，使用了 t_modified 这个索引；在测试表数据中插入了 10 万行数据，rows=100335，说明这条语句扫描了整个索引的所有值；Extra 字段的 Using index，表示的是使用了覆盖索引。 也就是说，由于在 t_modified 字段加了 month() 函数操作，导致了全索引扫描。为了能够用上索引的快速定位能力，就要把 SQL 语句改成基于字段本身的范围查询。按照下面这个写法，优化器就能按照预期，用上 t_modified 索引的快速定位能力了。 mysql\u003e select count(*) from tradelog where -\u003e (t_modified \u003e= '2016-7-1' and t_modified\u003c'2016-8-1') or -\u003e (t_modified \u003e= '2017-7-1' and t_modified\u003c'2017-8-1') or -\u003e (t_modified \u003e= '2018-7-1' and t_modified\u003c'2018-8-1'); 当然，如果你的系统上线时间更早，或者后面又插入了之后年份的数据的话，你就需要再把其他年份补齐。到这里我给你说明了，由于加了 month() 函数操作，MySQL 无法再使用索引快速定位功能，而只能使用全索引扫描。 不过优化器在个问题上确实有“偷懒”行为，即使是对于不改变有序性的函数，也不会考虑使用索引。比如，对于 select * from tradelog where id + 1 = 10000 这个 SQL 语句，这个加 1 操作并不会改变有序性，但是 MySQL 优化器还是不能用 id 索引快速定位到 9999 这一行。所以，需要你在写 SQL 语句的时候，手动改写成 where id = 10000 -1 才可以。 ","date":"2025-02-16","objectID":"/posts/18.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%BA%9Bsql%E8%AF%AD%E5%8F%A5%E9%80%BB%E8%BE%91%E7%9B%B8%E5%90%8C%E6%80%A7%E8%83%BD%E5%8D%B4%E5%B7%AE%E5%BC%82%E5%B7%A8%E5%A4%A7/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"18 | 为什么这些 SQL 语句逻辑相同，性能却差异巨大？","uri":"/posts/18.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%BA%9Bsql%E8%AF%AD%E5%8F%A5%E9%80%BB%E8%BE%91%E7%9B%B8%E5%90%8C%E6%80%A7%E8%83%BD%E5%8D%B4%E5%B7%AE%E5%BC%82%E5%B7%A8%E5%A4%A7/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 案例二：隐式类型转换 再一起看一下下面的这条 SQL 语句： mysql\u003e select * from tradelog where tradeid=110717; 交易编号 tradeid 这个字段上，本来就有索引，但是 explain 的结果却显示，这条语句需要走全表扫描。你可能也发现了，tradeid 的字段类型是 varchar(32)，而输入的参数却是整型，所以需要做类型转换。那么，现在这里就有两个问题： 数据类型转换的规则是什么？ 为什么有数据类型转换，就需要走全索引扫描？ 数据库里面类型这么多，这种数据类型转换规则更多，记不住，应该怎么办呢？这里有一个简单的方法，看 select“10” \u003e 9 的结果： 如果规则是“将字符串转成数字”，那么就是做数字比较，结果应该是 1； 如果规则是“将数字转成字符串”，那么就是做字符串比较，结果应该是 0。 验证结果如下图所示。 从图中可知，select“10” \u003e 9 返回的是 1，所以就能确认 MySQL 里的转换规则了：在 MySQL 中，字符串和数字做比较的话，是将字符串转换成数字。这时，再看这个全表扫描的语句： mysql\u003e select * from tradelog where tradeid=110717; 就知道对于优化器来说，这个语句相当于： mysql\u003e select * from tradelog where CAST(tradid AS signed int) = 110717; 那么如果 id 的类型是 int，执行下面这个语句，是否会导致全表扫描呢？ select * from tradelog where id=\"83126\"; 上面的 SQL 会使用到 id 这个索引的，因为字符串和数字在进行比较的时候，是将字符串转换为数字。 ","date":"2025-02-16","objectID":"/posts/18.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%BA%9Bsql%E8%AF%AD%E5%8F%A5%E9%80%BB%E8%BE%91%E7%9B%B8%E5%90%8C%E6%80%A7%E8%83%BD%E5%8D%B4%E5%B7%AE%E5%BC%82%E5%B7%A8%E5%A4%A7/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"18 | 为什么这些 SQL 语句逻辑相同，性能却差异巨大？","uri":"/posts/18.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%BA%9Bsql%E8%AF%AD%E5%8F%A5%E9%80%BB%E8%BE%91%E7%9B%B8%E5%90%8C%E6%80%A7%E8%83%BD%E5%8D%B4%E5%B7%AE%E5%BC%82%E5%B7%A8%E5%A4%A7/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 案例三：隐式字符编码转换 假设系统里还有另外一个表 trade_detail，用于记录交易的操作细节。为了便于量化分析和复现，往交易日志表 tradelog 和交易详情表 trade_detail 这两个表里插入一些数据。 mysql\u003e CREATE TABLE `trade_detail` ( `id` int(11) NOT NULL, `tradeid` varchar(32) DEFAULT NULL, `trade_step` int(11) DEFAULT NULL, /*操作步骤*/ `step_info` varchar(32) DEFAULT NULL, /*步骤信息*/ PRIMARY KEY (`id`), KEY `tradeid` (`tradeid`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; insert into tradelog values(1, 'aaaaaaaa', 1000, now()); insert into tradelog values(2, 'aaaaaaab', 1000, now()); insert into tradelog values(3, 'aaaaaaac', 1000, now()); insert into trade_detail values(1, 'aaaaaaaa', 1, 'add'); insert into trade_detail values(2, 'aaaaaaaa', 2, 'update'); insert into trade_detail values(3, 'aaaaaaaa', 3, 'commit'); insert into trade_detail values(4, 'aaaaaaab', 1, 'add'); insert into trade_detail values(5, 'aaaaaaab', 2, 'update'); insert into trade_detail values(6, 'aaaaaaab', 3, 'update again'); insert into trade_detail values(7, 'aaaaaaab', 4, 'commit'); insert into trade_detail values(8, 'aaaaaaac', 1, 'add'); insert into trade_detail values(9, 'aaaaaaac', 2, 'update'); insert into trade_detail values(10, 'aaaaaaac', 3, 'update again'); insert into trade_detail values(11, 'aaaaaaac', 4, 'commit'); 这时候，如果要查询 id=2 的交易的所有操作步骤信息，SQL 语句可以这么写： mysql\u003e select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2; /*语句 Q1*/ 一起来看下这个结果： 第一行显示优化器会先在交易记录表 tradelog 上查到 id=2 的行，这个步骤用上了主键索引，rows=1 表示只扫描一行； 第二行 key=NULL，表示没有用上交易详情表 trade_detail 上的 tradeid 索引，进行了全表扫描。 在这个执行计划里，是从 tradelog 表中取 tradeid 字段，再去 trade_detail 表里查询匹配字段。因此把 tradelog 称为驱动表，把 trade_detail 称为被驱动表，把 tradeid 称为关联字段。接下来，看下这个 explain 结果表示的执行流程： 图中： 第 1 步，是根据 id 在 tradelog 表里找到 L2 这一行； 第 2 步，是从 L2 中取出 tradeid 字段的值； 第 3 步，是根据 tradeid 值到 trade_detail 表中查找条件匹配的行。explain 的结果里面第二行的 key=NULL 表示的就是，这个过程是通过遍历主键索引的方式，一个一个地判断 tradeid 的值是否匹配。 进行到这里，你会发现第 3 步不符合预期。因为表 trade_detail 里 tradeid 字段上是有索引的，本来是希望通过使用 tradeid 索引能够快速定位到等值的行。但，这里并没有。 如果你去问 DBA 同学，他们可能会告诉你，因为这两个表的字符集不同，一个是 utf8，一个是 utf8mb4，所以做表连接查询的时候用不上关联字段的索引。为什么字符集不同就用不上索引呢？ 我们说问题是出在执行步骤的第 3 步，如果单独把这一步改成 SQL 语句的话，那就是： mysql\u003e select * from trade_detail where tradeid=$L2.tradeid.value; 其中，$L2.tradeid.value 的字符集是 utf8mb4。 参照前面的两个例子，你肯定就想到了，字符集 utf8mb4 是 utf8 的超集，所以当这两个类型的字符串在做比较的时候，MySQL 内部的操作是，先把 utf8 字符串转成 utf8mb4 字符集，再做比较。 这个设定很好理解，utf8mb4 是 utf8 的超集。类似地，在程序设计语言里面，做自动类型转换的时候，为了避免数据在转换过程中由于截断导致数据错误，也都是“按数据长度增加的方向”进行转换的。 因此，在执行上面这个语句的时候，需要将被驱动数据表里的字段一个个地转换成 utf8mb4，再跟 L2 做比较。也就是说，实际上这个语句等同于下面这个写法： select * from trade_detail where CONVERT(traideid USING utf8mb4)=$L2.tradeid.value; CONVERT() 函数，在这里的意思是把输入的字符串转成 utf8mb4 字符集。这就再次触发了上面说到的原则：对索引字段做函数操作，优化器会放弃走树搜索功能。 到这里，终于明确了，字符集不同只是条件之一，连接过程中要求在被驱动表的索引字段上加函数操作，是直接导致对被驱动表做全表扫描的原因。 作为对比验证，“查找 trade_detail 表里 id=4 的操作，对应的操作者是谁”，再来看下这个语句和它的执行计划。 mysql\u003eselect l.operator from tradelog l , trade_detail d where d.tradeid=l.tradeid and d.id=4; 这个语句里 trade_detail 表成了驱动表，但是 explain 结果的第二行显示，这次的查询操作用上了被驱动表 tradelog 里的索引 (tradeid)，扫描行数是 1。 这也是两个 tradeid 字段的 join 操作，为什么这次能用上被驱动表的 tradeid 索引呢？ 假设驱动表 trade_detail 里 id=4 的行记为 R4，那么在连接的时候，被驱动表 tradelog 上执行的就是类似这样的 SQL 语句： select operator from tradelog where traideid =$R4.tradeid.value; 这时候 $R4.tradeid.value 的字符集是 utf8, 按照字符集转换规则，要转成 utf8mb4，所以这个过程就被改写成： select operator from tradelog where traideid =CONVERT($R4.tradeid.value USING utf8mb4); 这里的 CONVERT 函数是加在输入参数上的，这样就可以用上被驱动表的 traideid 索引。理解了原理以后，就可以用来指导操作了。如果要优化语句 select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2; 的执行过程，有两种做法： 比较常见的优化方法是，把 trade_detail 表上的 tradeid 字段的字符集也改成 utf8mb4，这样就没有字符集转换的问题了。 alter table trade_detail modify tradeid varchar(32) CHARACTER SET utf8mb4 default null; 如果能够修改字段的字符集的话，是最好不过了。但如果数据量比较大，或者业务上暂时不能做这个 DDL 的话，那就只能采用修改 SQL 语句的方法了。 mysql\u003e select d.* from tradelog l , trade_detail d where d.tradeid=CONVERT(l.tradeid USING utf8) and l.id=2; 这里，主动把 l.tradeid 转成 utf8，就避免了被驱动表上的字符编码转换，从 explain 结果可以看到，这次索引走对了。 ","date":"2025-02-16","objectID":"/posts/18.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%BA%9Bsql%E8%AF%AD%E5%8F%A5%E9%80%BB%E8%BE%91%E7%9B%B8%E5%90%8C%E6%80%A7%E8%83%BD%E5%8D%B4%E5%B7%AE%E5%BC%82%E5%B7%A8%E5%A4%A7/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"18 | 为什么这些 SQL 语句逻辑相同，性能却差异巨大？","uri":"/posts/18.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%BA%9Bsql%E8%AF%AD%E5%8F%A5%E9%80%BB%E8%BE%91%E7%9B%B8%E5%90%8C%E6%80%A7%E8%83%BD%E5%8D%B4%E5%B7%AE%E5%BC%82%E5%B7%A8%E5%A4%A7/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 小结 文中的三个例子，其实是在说同一件事儿，即：对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。 MySQL 的优化器确实有“偷懒”的嫌疑，即使简单地把 where id+1=1000 改写成 where id=1000-1 就能够用上索引快速查找，也不会主动做这个语句重写。 ","date":"2025-02-16","objectID":"/posts/18.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%BA%9Bsql%E8%AF%AD%E5%8F%A5%E9%80%BB%E8%BE%91%E7%9B%B8%E5%90%8C%E6%80%A7%E8%83%BD%E5%8D%B4%E5%B7%AE%E5%BC%82%E5%B7%A8%E5%A4%A7/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"18 | 为什么这些 SQL 语句逻辑相同，性能却差异巨大？","uri":"/posts/18.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%BA%9Bsql%E8%AF%AD%E5%8F%A5%E9%80%BB%E8%BE%91%E7%9B%B8%E5%90%8C%E6%80%A7%E8%83%BD%E5%8D%B4%E5%B7%AE%E5%BC%82%E5%B7%A8%E5%A4%A7/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 问题 问：你遇到过别的、类似今天我们提到的性能问题吗？你认为原因是什么，又是怎么解决的呢？ 答：有一张表，表结构如下： mysql\u003e CREATE TABLE `table_a` ( `id` int(11) NOT NULL, `b` varchar(10) DEFAULT NULL, PRIMARY KEY (`id`), KEY `b` (`b`) ) ENGINE=InnoDB; 假设现在表里面，有 100 万行数据，其中有 10 万行数据的 b 的值是’1234567890’，假设现在执行语句是这么写的： mysql\u003e select * from table_a where b='1234567890abcd'; 这时候，MySQL 会怎么执行呢？最理想的情况是，MySQL 看到字段 b 定义的是 varchar(10)，那肯定返回空呀。可惜，MySQL 并没有这么做。 那要不，就是把’1234567890abcd’拿到索引里面去做匹配，肯定也没能够快速判断出索引树 b 上并没有这个值，也很快就能返回空结果。 但实际上，MySQL 也不是这么做的。这条 SQL 语句的执行很慢，流程是这样的： 在传给引擎执行的时候，做了字符截断。因为引擎里面这个行只定义了长度是 10，所以只截了前 10 个字节，就是’1234567890’进去做匹配； 这样满足条件的数据有 10 万行； 因为是 select *，所以要做 10 万次回表； 但是每次回表以后查出整行，到 server 层一判断，b 的值都不是’1234567890abcd’; 返回结果是空。 ","date":"2025-02-16","objectID":"/posts/18.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%BA%9Bsql%E8%AF%AD%E5%8F%A5%E9%80%BB%E8%BE%91%E7%9B%B8%E5%90%8C%E6%80%A7%E8%83%BD%E5%8D%B4%E5%B7%AE%E5%BC%82%E5%B7%A8%E5%A4%A7/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"18 | 为什么这些 SQL 语句逻辑相同，性能却差异巨大？","uri":"/posts/18.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%BA%9Bsql%E8%AF%AD%E5%8F%A5%E9%80%BB%E8%BE%91%E7%9B%B8%E5%90%8C%E6%80%A7%E8%83%BD%E5%8D%B4%E5%B7%AE%E5%BC%82%E5%B7%A8%E5%A4%A7/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文深入介绍了在 MySQL 中实现随机消息显示的技术特点和优化方法。","date":"2025-02-16","objectID":"/posts/17.%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%9C%B0%E6%98%BE%E7%A4%BA%E9%9A%8F%E6%9C%BA%E6%B6%88%E6%81%AF/","tags":["MySQL 实战 45 讲","MySQL"],"title":"17 | 如何正确地显示随机消息？","uri":"/posts/17.%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%9C%B0%E6%98%BE%E7%A4%BA%E9%9A%8F%E6%9C%BA%E6%B6%88%E6%81%AF/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文深入介绍了在 MySQL 中实现随机消息显示的技术特点和优化方法。以一个英语学习 App 的性能问题为例，详细讲解了随机选择单词的 SQL 语句设计、执行流程和优化方法。文章首先介绍了内存临时表排序方法，并分析了其执行流程和扫描行数。作者还解释了内存临时表排序使用的 rowid 排序方法和 rowid 的概念。 假设现在让你做一个英语学习的 APP，这个英语学习 App 首页有一个随机显示单词的功能，也就是根据每个用户的级别有一个单词表，然后这个用户每次访问首页的时候，都会随机滚动显示三个单词。你会发现随着单词表变大，选单词这个逻辑变得越来越慢，甚至影响到了首页的打开速度。现在，如果让你来设计这个 SQL 语句，你会怎么写呢？ 为了便于理解，对这个例子进行了简化：去掉每个级别的用户都有一个对应的单词表这个逻辑，直接就是从一个单词表中随机选出三个单词。这个表的建表语句和初始数据的命令如下： mysql\u003e CREATE TABLE `words` ( `id` int(11) NOT NULL AUTO_INCREMENT, `word` varchar(64) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; delimiter ;; create procedure idata() begin declare i int; set i=0; while i\u003c10000 do insert into words(word) values(concat(char(97+(i div 1000)), char(97+(i % 1000 div 100)), char(97+(i % 100 div 10)), char(97+(i % 10)))); set i=i+1; end while; end;; delimiter ; call idata(); 为了便于量化说明，在这个表里面插入了 10000 行记录。接下来我们来看看要随机选择 3 个单词，有什么方法实现，存在什么问题以及如何改进。 ","date":"2025-02-16","objectID":"/posts/17.%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%9C%B0%E6%98%BE%E7%A4%BA%E9%9A%8F%E6%9C%BA%E6%B6%88%E6%81%AF/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"17 | 如何正确地显示随机消息？","uri":"/posts/17.%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%9C%B0%E6%98%BE%E7%A4%BA%E9%9A%8F%E6%9C%BA%E6%B6%88%E6%81%AF/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 内存临时表 首先，你会想到用 order by rand() 来实现这个逻辑。 mysql\u003e select word from words order by rand() limit 3; 这个语句的意思很直白，随机排序取前 3 个。虽然这个 SQL 语句写法很简单，但执行流程却有点复杂的。先用 explain 命令来看看这个语句的执行情况。 Extra 字段显示 Using temporary，表示的是需要使用临时表；Using filesort，表示的是需要执行排序操作。因此这个 Extra 的意思就是，需要临时表，并且需要在临时表上排序。你觉得对于临时内存表的排序来说，它会选择哪一种算法呢？对于 InnoDB 表来说，执行全字段排序会减少磁盘访问，因此会被优先选择。 这里强调了“InnoDB 表”，你肯定想到了，对于内存表，回表过程只是简单地根据数据行的位置，直接访问内存得到数据，根本不会导致多访问磁盘。优化器没有了这一层顾虑，那么它会优先考虑的，就是用于排序的行越小越好了，所以，MySQL 这时就会选择 rowid 排序。 理解了这个算法选择的逻辑，再来看看语句的执行流程。同时，通过今天的这个例子来尝试分析一下语句的扫描行数。这条语句的执行流程是这样的： 创建一个临时表。这个临时表使用的是 memory 引擎，表里有两个字段，第一个字段是 double 类型，记为字段 R，第二个字段是 varchar(64) 类型，记为字段 W。并且，这个表没有建索引。 从 words 表中，按主键顺序取出所有的 word 值。对于每一个 word 值，调用 rand() 函数生成一个大于 0 小于 1 的随机小数，并把这个随机小数和 word 分别存入临时表的 R 和 W 字段中，到此，扫描行数是 10000。 现在临时表有 10000 行数据了，接下来你要在这个没有索引的内存临时表上，按照字段 R 排序。 初始化 sort_buffer。sort_buffer 中有两个字段，一个是 double 类型，另一个是整型。 从内存临时表中一行一行地取出 R 值和位置信息，分别存入 sort_buffer 中的两个字段里。这个过程要对内存临时表做全表扫描，此时扫描行数增加 10000，变成了 20000。 在 sort_buffer 中根据 R 的值进行排序。注意，这个过程没有涉及到表操作，所以不会增加扫描行数。 排序完成后，取出前三个结果的位置信息，依次到内存临时表中取出 word 值，返回给客户端。这个过程中，访问了表的三行数据，总扫描行数变成了 20003。 接下来，通过慢查询日志（slow log）来验证一下我们分析得到的扫描行数是否正确。 # Query_time: 0.900376 Lock_time: 0.000347 Rows_sent: 3 Rows_examined: 20003 SET timestamp=1541402277; select word from words order by rand() limit 3; 其中，Rows_examined：20003 就表示这个语句执行过程中扫描了 20003 行，也就验证了分析得出的结论。现在，我们来把完整的排序执行流程图画出来。 图中的 pos 就是位置信息，你可能会觉得奇怪，这里的“位置信息”是个什么概念？在上一篇文章中，我们对 InnoDB 表排序的时候，明明用的还是 ID 字段。这时候，就要回到一个基本概念：MySQL 的表是用什么方法来定位“一行数据”的。 如果把一个 InnoDB 表的主键删掉，是不是就没有主键，就没办法回表了？其实不是的。如果创建的表没有主键，或者把一个表的主键删掉了，那么 InnoDB 会自己生成一个长度为 6 字节的 rowid 来作为主键。这也就是排序模式里面，rowid 名字的来历。实际上它表示的是：每个引擎用来唯一标识数据行的信息。 对于有主键的 InnoDB 表来说，这个 rowid 就是主键 ID； 对于没有主键的 InnoDB 表来说，这个 rowid 就是由系统生成的； MEMORY 引擎不是索引组织表。在这个例子里面，可以认为它就是一个数组。因此，这个 rowid 其实就是数组的下标。 到这里，稍微小结一下：order by rand() 使用了内存临时表，内存临时表排序的时候使用了 rowid 排序方法。 ","date":"2025-02-16","objectID":"/posts/17.%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%9C%B0%E6%98%BE%E7%A4%BA%E9%9A%8F%E6%9C%BA%E6%B6%88%E6%81%AF/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"17 | 如何正确地显示随机消息？","uri":"/posts/17.%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%9C%B0%E6%98%BE%E7%A4%BA%E9%9A%8F%E6%9C%BA%E6%B6%88%E6%81%AF/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 磁盘临时表 那么，是不是所有的临时表都是内存表呢？其实不是的。tmp_table_size 这个配置限制了内存临时表的大小，默认值是 16M。如果临时表大小超过了 tmp_table_size，那么内存临时表就会转成磁盘临时表。磁盘临时表使用的引擎默认是 InnoDB，是由参数 internal_tmp_disk_storage_engine 控制的。当使用磁盘临时表的时候，对应的就是一个没有显式索引的 InnoDB 表的排序过程。 为了复现这个过程，可以把 tmp_table_size 设置成 1024，把 sort_buffer_size 设置成 32768, 把 max_length_for_sort_data 设置成 16。 set tmp_table_size=1024; set sort_buffer_size=32768; set max_length_for_sort_data=16; /* 打开 optimizer_trace，只对本线程有效 */ SET optimizer_trace='enabled=on'; /* 执行语句 */ select word from words order by rand() limit 3; /* 查看 OPTIMIZER_TRACE 输出 */ SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\\G 然后，来看一下这次 OPTIMIZER_TRACE 的结果。 因为将 max_length_for_sort_data 设置成 16，小于 word 字段的长度定义，所以可以看到 sort_mode 里面显示的是 rowid 排序，这个是符合预期的，参与排序的是随机值 R 字段和 rowid 字段组成的行。 你可能会发现不对。R 字段存放的随机值就 8 个字节，rowid 是 6 个字节，数据总行数是 10000，这样算出来就有 140000 字节，超过了 sort_buffer_size 定义的 32768 字节了。但是，number_of_tmp_files 的值居然是 0，难道不需要用临时文件吗？ 这个 SQL 语句的排序确实没有用到临时文件，采用是 MySQL 5.6 版本引入的一个新的排序算法，即：优先队列排序算法。接下来，就看看为什么没有使用临时文件的算法，也就是归并排序算法，而是采用了优先队列排序算法。 其实，现在的 SQL 语句，只需要取 R 值最小的 3 个 rowid。但是，如果使用归并排序算法的话，虽然最终也能得到前 3 个值，但是这个算法结束后，已经将 10000 行数据都排好序了。 也就是说，后面的 9997 行也是有序的了。但，我们的查询并不需要这些数据是有序的。所以，想一下就明白了，这浪费了非常多的计算量。而优先队列算法，就可以精确地只得到三个最小值，执行流程如下： 对于这 10000 个准备排序的 (R,rowid)，先取前三行，构造成一个堆； 取下一个行 (R’,rowid’)，跟当前堆里面最大的 R 比较，如果 R’小于 R，把这个 (R,rowid) 从堆中去掉，换成 (R’,rowid’)； 重复第 2 步，直到第 10000 个 (R’,rowid’) 完成比较。 下面简单画了一个优先队列排序过程的示意图。 上图是模拟 6 个 (R,rowid) 行，通过优先队列排序找到最小的三个 R 值的行的过程。整个排序过程中，为了最快地拿到当前堆的最大值，总是保持最大值在堆顶，因此这是一个最大堆。 第五幅图的 OPTIMIZER_TRACE 结果中，filesort_priority_queue_optimization 这个部分的 chosen=true，就表示使用了优先队列排序算法，这个过程不需要临时文件，因此对应的 number_of_tmp_files 是 0。 这个流程结束后，构造的堆里面，就是这个 10000 行里面 R 值最小的三行。然后，依次把它们的 rowid 取出来，去临时表里面拿到 word 字段，这个过程就跟上一篇文章的 rowid 排序的过程一样了。再看一下上面一篇文章的 SQL 查询语句： select city,name,age from t where city='杭州' order by name limit 1000 ; 你可能会问，这里也用到了 limit，为什么没用优先队列排序算法呢？原因是，这条 SQL 语句是 limit 1000，如果使用优先队列算法的话，需要维护的堆的大小就是 1000 行的 (name,rowid)，超过了设置的 sort_buffer_size 大小，所以只能使用归并排序算法。 总之，不论是使用哪种类型的临时表，order by rand() 这种写法都会让计算过程非常复杂，需要大量的扫描行数，因此排序过程的资源消耗也会很大。 ","date":"2025-02-16","objectID":"/posts/17.%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%9C%B0%E6%98%BE%E7%A4%BA%E9%9A%8F%E6%9C%BA%E6%B6%88%E6%81%AF/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"17 | 如何正确地显示随机消息？","uri":"/posts/17.%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%9C%B0%E6%98%BE%E7%A4%BA%E9%9A%8F%E6%9C%BA%E6%B6%88%E6%81%AF/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 随机排序方法 先把问题简化一下，如果只随机选择 1 个 word 值，可以怎么做呢？思路上是这样的： 取得这个表的主键 id 的最大值 M 和最小值 N; 用随机函数生成一个最大值到最小值之间的数 X = (M-N)*rand() + N; 取不小于 X 的第一个 ID 的行。 把这个算法，暂时称作随机算法 1。这里直接贴一下执行语句的序列： mysql\u003e select max(id),min(id) into @M,@N from t ; set @X= floor((@M-@N+1)*rand() + @N); select * from t where id \u003e= @X limit 1; 这个方法效率很高，因为取 max(id) 和 min(id) 都是不需要扫描索引的，而第三步的 select 也可以用索引快速定位，可以认为就只扫描了 3 行。但实际上，这个算法本身并不严格满足题目的随机要求，因为 ID 中间可能有空洞，因此选择不同行的概率不一样，不是真正的随机。 比如有 4 个 id，分别是 1、2、4、5，如果按照上面的方法，那么取到 id=4 的这一行的概率是取得其他行概率的两倍。如果这四行的 id 分别是 1、2、40000、40001 呢？这个算法基本就能当 bug 来看待了。所以，为了得到严格随机的结果，可以用下面这个流程： 取得整个表的行数，并记为 C。 取得 Y = floor(C * rand())。floor 函数在这里的作用，就是取整数部分。 再用 limit Y,1 取得一行。 把这个算法，称为随机算法 2。下面这段代码，就是上面流程的执行语句的序列。 mysql\u003e select count(*) into @C from t; set @Y = floor(@C * rand()); set @sql = concat(\"select * from t limit \", @Y, \",1\"); prepare stmt from @sql; execute stmt; DEALLOCATE prepare stmt; 由于 limit 后面的参数不能直接跟变量，所以上面的代码中使用了 prepare+execute 的方法。也可以把拼接 SQL 语句的方法写在应用程序中，会更简单些。这个随机算法 2，解决了算法 1 里面明显的概率不均匀问题。 MySQL 处理 limit Y,1 的做法就是按顺序一个一个地读出来，丢掉前 Y 个，然后把下一个记录作为返回结果，因此这一步需要扫描 Y+1 行。再加上，第一步扫描的 C 行，总共需要扫描 C+Y+1 行，执行代价比随机算法 1 的代价要高。当然，随机算法 2 跟直接 order by rand() 比起来，执行代价还是小很多的。 现在再来看看，如果按照随机算法 2 的思路，要随机取 3 个 word 值呢？可以这么做： 取得整个表的行数，记为 C； 根据相同的随机方法得到 Y1、Y2、Y3； 再执行三个 limit Y, 1 语句得到三行数据。 把这个算法，称作随机算法 3。下面这段代码，就是上面流程的执行语句的序列。 mysql\u003e select count(*) into @C from t; set @Y1 = floor(@C * rand()); set @Y2 = floor(@C * rand()); set @Y3 = floor(@C * rand()); select * from t limit @Y1，1； //在应用代码里面取Y1、Y2、Y3值，拼出SQL后执行 select * from t limit @Y2，1； select * from t limit @Y3，1； ","date":"2025-02-16","objectID":"/posts/17.%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%9C%B0%E6%98%BE%E7%A4%BA%E9%9A%8F%E6%9C%BA%E6%B6%88%E6%81%AF/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"17 | 如何正确地显示随机消息？","uri":"/posts/17.%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%9C%B0%E6%98%BE%E7%A4%BA%E9%9A%8F%E6%9C%BA%E6%B6%88%E6%81%AF/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 小结 如果直接使用 order by rand()，这个语句需要 Using temporary 和 Using filesort，查询的执行代价往往是比较大的。所以，在设计的时候要尽量避开这种写法。 文章的例子里面，不是仅仅在数据库内部解决问题，还会让应用代码配合拼接 SQL 语句。在实际应用的过程中，比较规范的用法就是：尽量将业务逻辑写在业务代码中，让数据库只做“读写数据”的事情。因此，这类方法的应用还是比较广泛的。 ","date":"2025-02-16","objectID":"/posts/17.%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%9C%B0%E6%98%BE%E7%A4%BA%E9%9A%8F%E6%9C%BA%E6%B6%88%E6%81%AF/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"17 | 如何正确地显示随机消息？","uri":"/posts/17.%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%9C%B0%E6%98%BE%E7%A4%BA%E9%9A%8F%E6%9C%BA%E6%B6%88%E6%81%AF/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 问题 问：上面的随机算法 3 的总扫描行数是 C+(Y1+1)+(Y2+1)+(Y3+1)，实际上它还是可以继续优化，来进一步减少扫描行数的。如果你是这个需求的开发人员，你会怎么做，来减少扫描行数呢？说说你的方案，并说明你的方案需要的扫描行数。 答：这里给出一种方法，取 Y1、Y2 和 Y3 里面最大的一个数，记为 M，最小的一个数记为 N，然后执行下面这条 SQL 语句： mysql\u003e select * from t limit N, M-N+1; 再加上取整个表总行数的 C 行，这个方案的扫描行数总共只需要 C+M+1 行。 ","date":"2025-02-16","objectID":"/posts/17.%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%9C%B0%E6%98%BE%E7%A4%BA%E9%9A%8F%E6%9C%BA%E6%B6%88%E6%81%AF/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"17 | 如何正确地显示随机消息？","uri":"/posts/17.%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%9C%B0%E6%98%BE%E7%A4%BA%E9%9A%8F%E6%9C%BA%E6%B6%88%E6%81%AF/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文深入探讨了在开发应用中常见的根据指定字段排序来显示结果的需求，以及针对这种需求的 SQL 语句“order by”是如何执行的。","date":"2025-02-16","objectID":"/posts/16.order-by%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84/","tags":["MySQL 实战 45 讲","MySQL"],"title":"16 | “order by”是怎么工作的？","uri":"/posts/16.order-by%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文深入探讨了在开发应用中常见的根据指定字段排序来显示结果的需求，以及针对这种需求的 SQL 语句“order by”是如何执行的。文章首先介绍了全字段排序的执行流程，包括使用 explain 命令查看语句的执行情况、sort_buffer 的内存排序和临时文件排序等细节。接着讨论了当排序的单行长度较大时，MySQL 采用的另一种算法——rowid 排序，详细解释了其执行流程和优化效果。通过对比两种排序算法的执行过程和优化效果，可以更好地理解“order by”语句的执行原理和影响因素。 在你开发应用的时候，一定会经常碰到需要根据指定的字段排序来显示结果的需求。还是以前面举例用过的市民表为例，假设要查询城市是“杭州”的所有人名字，并且按照姓名排序返回前 1000 个人的姓名、年龄。假设这个表的部分定义是这样的： CREATE TABLE `t` ( `id` int(11) NOT NULL, `city` varchar(16) NOT NULL, `name` varchar(16) NOT NULL, `age` int(11) NOT NULL, `addr` varchar(128) DEFAULT NULL, PRIMARY KEY (`id`), KEY `city` (`city`) ) ENGINE=InnoDB; 这时，你的 SQL 语句可以这么写： select city,name,age from t where city='杭州' order by name limit 1000; 那么这个语句是怎么执行的呢？以及有什么参数会影响执行的行为？ ","date":"2025-02-16","objectID":"/posts/16.order-by%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"16 | “order by”是怎么工作的？","uri":"/posts/16.order-by%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 全字段排序 为避免全表扫描，需要在 city 字段加上索引。在 city 字段上创建索引之后，用 explain 命令来看看这个语句的执行情况。 Extra 这个字段中的“Using filesort”表示的就是需要排序，MySQL 会给每个线程分配一块内存用于排序，称为 sort_buffer。为了说明这个 SQL 查询语句的执行过程，先来看一下 city 这个索引的示意图。 从图中可以看到，满足 city=‘杭州’条件的行，是从 ID_X 到 ID_(X+N) 的这些记录。通常情况下，这个语句执行流程如下所示： 初始化 sort_buffer，确定放入 name、city、age 这三个字段； 从索引 city 找到第一个满足 city=‘杭州’条件的主键 id，也就是图中的 ID_X； 到主键 id 索引取出整行，取 name、city、age 三个字段的值，存入 sort_buffer 中； 从索引 city 取下一个记录的主键 id； 重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的 ID_Y； 对 sort_buffer 中的数据按照字段 name 做快速排序； 按照排序结果取前 1000 行返回给客户端。 暂且把这个排序过程，称为全字段排序，执行流程的示意图如下所示。 图中“按 name 排序”这个动作，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数 sort_buffer_size。 sort_buffer_size，就是 MySQL 为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。可以用下面介绍的方法，来确定一个排序语句是否使用了临时文件。 /* 打开 optimizer_trace，只对本线程有效 */ SET optimizer_trace='enabled=on'; /* @a 保存 Innodb_rows_read 的初始值 */ select VARIABLE_VALUE into @a from performance_schema.session_status where variable_name = 'Innodb_rows_read'; /* 执行语句 */ select city, name,age from t where city='杭州' order by name limit 1000; /* 查看 OPTIMIZER_TRACE 输出 */ SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\\G /* @b 保存 Innodb_rows_read 的当前值 */ select VARIABLE_VALUE into @b from performance_schema.session_status where variable_name = 'Innodb_rows_read'; /* 计算 Innodb_rows_read 差值 */ select @b-@a; 这个方法是通过查看 OPTIMIZER_TRACE 的结果来确认的，可以从 number_of_tmp_files 中看到是否使用了临时文件。 number_of_tmp_files 表示的是，排序过程中使用的临时文件数。你一定奇怪，为什么需要 12 个文件？内存放不下时，就需要使用外部排序，外部排序一般使用归并排序算法。可以这么简单理解，MySQL 将需要排序的数据分成 12 份，每一份单独排序后存在这些临时文件中。然后把这 12 个有序文件再合并成一个有序的大文件。 如果 sort_buffer_size 超过了需要排序的数据量的大小，number_of_tmp_files 就是 0，表示排序可以直接在内存中完成。否则就需要放在临时文件中排序。sort_buffer_size 越小，需要分成的份数越多，number_of_tmp_files 的值就越大。 接下来，再来解释一下上图中其他两个值的意思。 示例表中有 4000 条满足 city=‘杭州’的记录，所以可以看到 examined_rows=4000，表示参与排序的行数是 4000 行。 sort_mode 里面的 packed_additional_fields 的意思是，排序过程对字符串做了“紧凑”处理。即使 name 字段的定义是 varchar(16)，在排序过程中还是要按照实际长度来分配空间的。 同时，最后一个查询语句 select @b-@a 的返回结果是 4000，表示整个执行过程只扫描了 4000 行。 这里需要注意的是，为了避免对结论造成干扰，把 internal_tmp_disk_storage_engine 设置成 MyISAM。否则，select @b-@a 的结果会显示为 4001。 这是因为查询 OPTIMIZER_TRACE 这个表时，需要用到临时表，而 internal_tmp_disk_storage_engine 的默认值是 InnoDB。如果使用的是 InnoDB 引擎的话，把数据从临时表取出来的时候，会让 Innodb_rows_read 的值加 1。 ","date":"2025-02-16","objectID":"/posts/16.order-by%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"16 | “order by”是怎么工作的？","uri":"/posts/16.order-by%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 rowid 排序 在上面这个算法过程里面，只对原表的数据读了一遍，剩下的操作都是在 sort_buffer 和临时文件中执行的。但这个算法有一个问题，就是如果查询要返回的字段很多的话，那么 sort_buffer 里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。所以如果单行很大，这个方法效率不够好。那么，如果 MySQL 认为排序的单行长度太大会怎么做呢？接下来，修改一个参数，让 MySQL 采用另外一种算法。 SET max_length_for_sort_data = 16; max_length_for_sort_data，是 MySQL 中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL 就认为单行太大，要换一个算法。 city、name、age 这三个字段的定义总长度是 36，把 max_length_for_sort_data 设置为 16，再来看看计算过程有什么改变。新的算法放入 sort_buffer 的字段，只有要排序的列（即 name 字段）和主键 id。 但这时，排序的结果就因为少了 city 和 age 字段的值，不能直接返回了，整个执行流程就变成如下所示的样子： 初始化 sort_buffer，确定放入两个字段，即 name 和 id； 从索引 city 找到第一个满足 city=‘杭州’条件的主键 id，也就是图中的 ID_X； 到主键 id 索引取出整行，取 name、id 这两个字段，存入 sort_buffer 中； 从索引 city 取下一个记录的主键 id； 重复步骤 3、4 直到不满足 city=‘杭州’条件为止，也就是图中的 ID_Y； 对 sort_buffer 中的数据按照字段 name 进行排序； 遍历排序结果，取前 1000 行，并按照 id 的值回到原表中取出 city、name 和 age 三个字段返回给客户端。 这个执行流程的示意图如下，可以把它称为 rowid 排序。 对比第三幅图的全字段排序流程图你会发现，rowid 排序多访问了一次表 t 的主键索引，就是步骤 7。 需要说明的是，最后的“结果集”是一个逻辑概念，实际上 MySQL 服务端从排序后的 sort_buffer 中依次取出 id，然后到原表查到 city、name 和 age 这三个字段的结果，不需要在服务端再耗费内存存储结果，是直接返回给客户端的。根据这个说明过程和图示，可以想一下，这个时候执行 select @b-@a，结果会是多少呢？现在，我们就来看看结果有什么不同。 首先，图中的 examined_rows 的值还是 4000，表示用于排序的数据是 4000 行。但是 select @b-@a 这个语句的值变成 5000 了。 因为这时候除了排序过程外，在排序完成后，还要根据 id 去原表取值。由于语句是 limit 1000，因此会多读 1000 行。 从 OPTIMIZER_TRACE 的结果中，还能看到另外两个信息也变了。 sort_mode 变成了 ，表示参与排序的只有 name 和 id 这两个字段。 number_of_tmp_files 变成 10 了，是因为这时候参与排序的行数虽然仍然是 4000 行，但是每一行都变小了，因此需要排序的总数据量就变小了，需要的临时文件也相应地变少了。 ","date":"2025-02-16","objectID":"/posts/16.order-by%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"16 | “order by”是怎么工作的？","uri":"/posts/16.order-by%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 全字段排序 VS rowid 排序 如果 MySQL 实在是担心排序内存太小，会影响排序效率，才会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。 如果 MySQL 认为内存足够大，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。 这也就体现了 MySQL 的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。对于 InnoDB 表来说，rowid 排序会要求回表多造成磁盘读，因此不会被优先选择。 MySQL 做排序是一个成本比较高的操作。那么你会问，是不是所有的 order by 都需要排序操作呢？如果不排序就能得到正确的结果，那对系统的消耗会小很多，语句的执行时间也会变得更短。 其实，并不是所有的 order by 语句，都需要排序操作的。从上面分析的执行过程，可以看到，MySQL 之所以需要生成临时表，并且在临时表上做排序操作，其原因是原来的数据都是无序的。如果能够保证从 city 这个索引上取出来的行，天然就是按照 name 递增排序的话，是不是就可以不用再排序了呢？确实是这样的。所以，可以在这个市民表上创建一个 city 和 name 的联合索引，对应的 SQL 语句是： alter table t add index city_user(city, name); 作为与 city 索引的对比，来看看这个索引的示意图。 在这个索引里面，依然可以用树搜索的方式定位到第一个满足 city=‘杭州’的记录，并且额外确保了，接下来按顺序取“下一条记录”的遍历过程中，只要 city 的值是杭州，name 的值就一定是有序的。这样整个查询过程的流程就变成了： 从索引 (city,name) 找到第一个满足 city=‘杭州’条件的主键 id； 到主键 id 索引取出整行，取 name、city、age 三个字段的值，作为结果集的一部分直接返回； 从索引 (city,name) 取下一个记录主键 id； 重复步骤 2、3，直到查到第 1000 条记录，或者是不满足 city=‘杭州’条件时循环结束。 可以看到，这个查询过程不需要临时表，也不需要排序。接下来，用 explain 的结果来印证一下。 从图中可以看到，Extra 字段中没有 Using filesort 了，也就是不需要排序了。而且由于 (city,name) 这个联合索引本身有序，所以这个查询也不用把 4000 行全都读一遍，只要找到满足条件的前 1000 条记录就可以退出了。也就是说，在这个例子里，只需要扫描 1000 次。 这个语句的执行流程有没有可能进一步简化呢？是否可以使用覆盖索引呢？覆盖索引是指，索引上的信息足够满足查询请求，不需要再回到主键索引上去取数据。按照覆盖索引的概念，可以再优化一下这个查询语句的执行流程。针对这个查询，可以创建一个 city、name 和 age 的联合索引，对应的 SQL 语句就是： alter table t add index city_user_age(city, name, age); 这时，对于 city 字段的值相同的行来说，还是按照 name 字段的值递增排序的，此时的查询语句也就不再需要排序了。这样整个查询语句的执行流程就变成了： 从索引 (city,name,age) 找到第一个满足 city=‘杭州’条件的记录，取出其中的 city、name 和 age 这三个字段的值，作为结果集的一部分直接返回； 从索引 (city,name,age) 取下一个记录，同样取出这三个字段的值，作为结果集的一部分直接返回； 重复执行步骤 2，直到查到第 1000 条记录，或者是不满足 city=‘杭州’条件时循环结束。 然后，再来看看 explain 的结果。 可以看到，Extra 字段里面多了“Using index”，表示的就是使用了覆盖索引，性能上会快很多。 当然，这里并不是说要为了每个查询能用上覆盖索引，就要把语句中涉及的字段都建上联合索引，毕竟索引还是有维护代价的。这是一个需要权衡的决定。 ","date":"2025-02-16","objectID":"/posts/16.order-by%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"16 | “order by”是怎么工作的？","uri":"/posts/16.order-by%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 小结 在开发系统的时候，总是不可避免地会使用到 order by 语句。心里要清楚每个语句的排序逻辑是怎么实现的，还要能够分析出在最坏情况下，每个语句的执行对系统资源的消耗，这样才能做到下笔如有神，不犯低级错误。 ","date":"2025-02-16","objectID":"/posts/16.order-by%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"16 | “order by”是怎么工作的？","uri":"/posts/16.order-by%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 问题 问：假设表里面已经有了 city_name(city, name) 这个联合索引，然后要查杭州和苏州两个城市中所有的市民的姓名，并且按名字排序，显示前 100 条记录。如果 SQL 查询语句是这么写的： mysql\u003e select * from t where city in ('杭州',\"苏州\") order by name limit 100; 那么，这个语句执行的时候会有排序过程吗，为什么？ 如果业务端代码由你来开发，需要实现一个在数据库端不需要排序的方案，你会怎么实现呢？ 进一步地，如果有分页需求，要显示第 101 页，也就是说语句最后要改成“limit 10000,100”，你的实现方法又会是什么呢？ 答：虽然有 (city,name) 联合索引，对于单个 city 内部，name 是递增的。但是由于这条 SQL 语句不是要单独地查一个 city 的值，而是同时查了\"杭州\"和\" 苏州 “两个城市，因此所有满足条件的 name 就不是递增的了。也就是说，这条 SQL 语句需要排序。 那怎么避免排序呢？这里，要用到 (city,name) 联合索引的特性，把这一条语句拆成两条语句，执行流程如下： 执行 select * from t where city=“杭州”order by name limit 100; 这个语句是不需要排序的，客户端用一个长度为 100 的内存数组 A 保存结果。 执行 select * from t where city=“苏州”order by name limit 100; 用相同的方法，假设结果被存进了内存数组 B。 现在 A 和 B 是两个有序数组，然后可以用归并排序的思想，得到 name 最小的前 100 值，就是需要的结果了。 如果把这条 SQL 语句里“limit 100”改成“limit 10000,100”的话，处理方式其实也差不多，即：要把上面的两条语句改成写： select * from t where city=\"杭州\" order by name limit 10100; select * from t where city=\"苏州\" order by name limit 10100。 这时候数据量较大，可以同时起两个连接一行行读结果，用归并排序算法拿到这两个结果集里，按顺序取第 10001~10100 的 name 值，就是需要的结果了。 当然这个方案有一个明显的损失，就是从数据库返回给客户端的数据量变大了。所以，如果数据的单行比较大的话，可以考虑把这两条 SQL 语句改成下面这种写法： select id,name from t where city=\"杭州\" order by name limit 10100; select id,name from t where city=\"苏州\" order by name limit 10100。 然后，再用归并排序的方法取得按 name 顺序第 10001~10100 的 name、id 的值，然后拿着这 100 个 id 到数据库中去查出所有记录。上面这些方法，需要根据性能需求和开发的复杂度做出权衡。 ","date":"2025-02-16","objectID":"/posts/16.order-by%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"16 | “order by”是怎么工作的？","uri":"/posts/16.order-by%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"答疑文章。","date":"2025-02-12","objectID":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/","tags":["MySQL 实战 45 讲","MySQL"],"title":"15 | 答疑文章（一）：日志和索引相关问题","uri":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文是 MySQL 实战专栏的答疑文章。 ","date":"2025-02-12","objectID":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"15 | 答疑文章（一）：日志和索引相关问题","uri":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 日志相关问题 在两阶段提交的不同瞬间，MySQL 如果发生异常重启，是怎么保证数据完整性的？ 这里，先和你解释一个误会式的问题。这个图不是一个 update 语句的执行流程吗，怎么还会调用 commit 语句？这是因为把两个“commit”的概念混淆了： “commit 语句”，是指 MySQL 语法中，用于提交一个事务的命令。一般跟 begin/start transaction 配对使用。 图中用到的这个“commit 步骤”，指的是事务提交过程中的一个小步骤，也是最后一步。当这个步骤执行完成后，这个事务就提交完成了。 “commit 语句”执行的时候，会包含“commit 步骤”。 而我们这个例子里面，没有显式地开启事务，因此这个 update 语句自己就是一个事务，在执行完成后提交事务时，就会用到这个“commit 步骤“。接下来分析一下在两阶段提交的不同时刻，MySQL 异常重启会出现什么现象。 如果在图中时刻 A 的地方，也就是写入 redo log 处于 prepare 阶段之后、写 binlog 之前，发生了崩溃（crash），由于此时 binlog 还没写，redo log 也还没提交，所以崩溃恢复的时候，这个事务会回滚。这时候，binlog 还没写，所以也不会传到备库。 那时刻 B，binlog 写完，redo log 还没 commit 前发生 crash，那崩溃恢复的时候 MySQL 会怎么处理？先来看一下崩溃恢复时的判断规则。 如果 redo log 里面的事务是完整的，也就是已经有了 commit 标识，则直接提交； 如果 redo log 里面的事务只有完整的 prepare，则判断对应的事务 binlog 是否存在并完整： a. 如果是，则提交事务； b. 否则，回滚事务。 这里，时刻 B 发生 crash 对应的就是 2(a) 的情况，崩溃恢复过程中事务会被提交。 ","date":"2025-02-12","objectID":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"15 | 答疑文章（一）：日志和索引相关问题","uri":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 追问 1：MySQL 怎么知道 binlog 是完整的？ 一个事务的 binlog 是有完整格式的： statement 格式的 binlog，最后会有 COMMIT； row 格式的 binlog，最后会有一个 XID event。 另外，在 MySQL 5.6.2 版本以后，还引入了 binlog-checksum 参数，用来验证 binlog 内容的正确性。对于 binlog 日志由于磁盘原因，可能会在日志中间出错的情况，MySQL 可以通过校验 checksum 的结果来发现。所以，MySQL 还是有办法验证事务 binlog 的完整性的。 ","date":"2025-02-12","objectID":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"15 | 答疑文章（一）：日志和索引相关问题","uri":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 追问 2：redo log 和 binlog 是怎么关联起来的？ 它们有一个共同的数据字段，叫 XID。崩溃恢复的时候，会按顺序扫描 redo log： 如果碰到既有 prepare、又有 commit 的 redo log，就直接提交； 如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。 ","date":"2025-02-12","objectID":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"15 | 答疑文章（一）：日志和索引相关问题","uri":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 追问 3：处于 prepare 阶段的 redo log 加上完整 binlog，重启就能恢复，MySQL 为什么要这么设计？ 其实，这个问题还是跟反证法中说到的数据与备份的一致性有关。在时刻 B，也就是 binlog 写完以后 MySQL 发生崩溃，这时候 binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。 所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。 ","date":"2025-02-12","objectID":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"15 | 答疑文章（一）：日志和索引相关问题","uri":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 追问 4：如果这样的话，为什么还要两阶段提交呢？干脆先 redo log 写完，再写 binlog。崩溃恢复的时候，必须得两个日志都完整才可以。是不是一样的逻辑？ 其实，两阶段提交是经典的分布式系统问题，并不是 MySQL 独有的。如果必须要举一个场景，来说明这么做的必要性的话，那就是事务的持久性问题。 对于 InnoDB 引擎来说，如果 redo log 提交完成了，事务就不能回滚（如果这还允许回滚，就可能覆盖掉别的事务的更新）。而如果 redo log 直接提交，然后 binlog 写入的时候失败，InnoDB 又回滚不了，数据和 binlog 日志又不一致了。 两阶段提交就是为了给所有人一个机会，当每个人都说“我 ok”的时候，再一起提交。 ","date":"2025-02-12","objectID":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"15 | 答疑文章（一）：日志和索引相关问题","uri":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"6 追问 5：不引入两个日志，也就没有两阶段提交的必要了。只用 binlog 来支持崩溃恢复，又能支持归档，不就可以了？ 只保留 binlog，然后可以把提交流程改成这样：… -\u003e “数据更新到内存” -\u003e “写 binlog” -\u003e “提交事务”，是不是也可以提供崩溃恢复的能力？答案是不可以。 如果说历史原因的话，那就是 InnoDB 并不是 MySQL 的原生存储引擎。MySQL 的原生引擎是 MyISAM，设计之初就有没有支持崩溃恢复。 InnoDB 在作为 MySQL 的插件加入 MySQL 引擎家族之前，就已经是一个提供了崩溃恢复和事务支持的引擎了。 InnoDB 接入了 MySQL 后，发现既然 binlog 没有崩溃恢复的能力，那就用 InnoDB 原有的 redo log 好了。 而如果说实现上的原因的话，就有很多了。就按照问题中说的，只用 binlog 来实现崩溃恢复的流程，画了一张示意图，这里就没有 redo log 了。 这样的流程下，binlog 还是不能支持崩溃恢复的。先 说一个不支持的点吧：binlog 没有能力恢复“数据页”。如果在图中标的位置，也就是 binlog2 写完了，但是整个事务还没有 commit 的时候，MySQL 发生了 crash。 重启后，引擎内部事务 2 会回滚，然后应用 binlog2 可以补回来；但是对于事务 1 来说，系统已经认为提交完成了，不会再应用一次 binlog1。 但是，InnoDB 引擎使用的是 WAL 技术，执行事务的时候，写完内存和日志，事务就算完成了。如果之后崩溃，要依赖于日志来恢复数据页。 也就是说在图中这个位置发生崩溃的话，事务 1 也是可能丢失了的，而且是数据页级的丢失。此时，binlog 里面并没有记录数据页的更新细节，是补不回来的。 你如果要说，那优化一下 binlog 的内容，让它来记录数据页的更改可以吗？但，这其实就是又做了一个 redo log 出来。所以，至少现在的 binlog 能力，还不能支持崩溃恢复。 ","date":"2025-02-12","objectID":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/:6:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"15 | 答疑文章（一）：日志和索引相关问题","uri":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"7 追问 6：那能不能反过来，只用 redo log，不要 binlog？ 如果只从崩溃恢复的角度来讲是可以的。可以把 binlog 关掉，这样就没有两阶段提交了，但系统依然是 crash-safe 的。 但是，如果你了解一下业界各个公司的使用场景的话，就会发现在正式的生产库上，binlog 都是开着的。因为 binlog 有着 redo log 无法替代的功能。 一个是归档。redo log 是循环写，写到末尾是要回到开头继续写的。这样历史日志没法保留，redo log 也就起不到归档的作用。 一个就是 MySQL 系统依赖于 binlog。binlog 作为 MySQL 一开始就有的功能，被用在了很多地方。其中，MySQL 系统高可用的基础，就是 binlog 复制。 还有很多公司有异构系统（比如一些数据分析系统），这些系统就靠消费 MySQL 的 binlog 来更新自己的数据。关掉 binlog 的话，这些下游系统就没法输入了。 总之，由于现在包括 MySQL 高可用在内的很多系统机制都依赖于 binlog，所以“鸠占鹊巢”redo log 还做不到。 ","date":"2025-02-12","objectID":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/:7:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"15 | 答疑文章（一）：日志和索引相关问题","uri":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"8 追问 7：redo log 一般设置多大？ redo log 太小的话，会导致很快就被写满，然后不得不强行刷 redo log，这样 WAL 机制的能力就发挥不出来了。 所以，如果是现在常见的几个 TB 的磁盘的话，就不要太小气了，直接将 redo log 设置为 4 个文件、每个文件 1GB 吧。 ","date":"2025-02-12","objectID":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/:8:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"15 | 答疑文章（一）：日志和索引相关问题","uri":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"9 追问 8：正常运行中的实例，数据写入后的最终落盘，是从 redo log 更新过来的还是从 buffer pool 更新过来的呢？ 这个问题其实问得非常好。这里涉及到了，“redo log 里面到底是什么”的问题。实际上，redo log 并没有记录数据页的完整数据，所以它并没有能力自己去更新磁盘数据页，也就不存在“数据最终落盘，是由 redo log 更新过去”的情况。 如果是正常运行的实例的话，数据页被修改以后，跟磁盘的数据页不一致，称为脏页。最终数据落盘，就是把内存中的数据页写盘。这个过程，甚至与 redo log 毫无关系。 在崩溃恢复场景中，InnoDB 如果判断到一个数据页可能在崩溃恢复的时候丢失了更新，就会将它读到内存，然后让 redo log 更新内存内容。更新完成后，内存页变成脏页，就回到了第一种情况的状态。 ","date":"2025-02-12","objectID":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/:9:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"15 | 答疑文章（一）：日志和索引相关问题","uri":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"10 追问 9：redo log buffer 是什么？是先修改内存，还是先写 redo log 文件？ 在一个事务的更新过程中，日志是要写多次的。比如下面这个事务： begin; insert into t1 ... insert into t2 ... commit; 这个事务要往两个表中插入记录，插入数据的过程中，生成的日志都得先保存起来，但又不能在还没 commit 的时候就直接写到 redo log 文件里。 所以，redo log buffer 就是一块内存，用来先存 redo 日志的。也就是说，在执行第一个 insert 的时候，数据的内存被修改了，redo log buffer 也写入了日志。 但是，真正把日志写到 redo log 文件（文件名是 ib_logfile+ 数字），是在执行 commit 语句的时候做的。 单独执行一个更新语句的时候，InnoDB 会自己启动一个事务，在语句执行完成的时候提交。过程跟上面是一样的，只不过是“压缩”到了一个语句里面完成。 ","date":"2025-02-12","objectID":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/:10:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"15 | 答疑文章（一）：日志和索引相关问题","uri":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"11 业务设计问题 业务上有这样的需求，A、B 两个用户，如果互相关注，则成为好友。设计上是有两张表，一个是 like 表，一个是 friend 表，like 表有 user_id、liker_id 两个字段，设置为复合唯一索引即 uk_user_id_liker_id。语句执行逻辑是这样的： 以 A 关注 B 为例：第一步，先查询对方有没有关注自己（B 有没有关注 A）select * from like where user_id = B and liker_id = A; 如果有，则成为好友 insert into friend; 没有，则只是单向关注关系 insert into like; 但是如果 A、B 同时关注对方，会出现不会成为好友的情况。因为上面第 1 步，双方都没关注对方。第 1 步即使使用了排他锁也不行，因为记录不存在，行锁无法生效。请问这种情况，在 MySQL 锁层面有没有办法处理？ CREATE TABLE `like` ( `id` int(11) NOT NULL AUTO_INCREMENT, `user_id` int(11) NOT NULL, `liker_id` int(11) NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `uk_user_id_liker_id` (`user_id`,`liker_id`) ) ENGINE=InnoDB; CREATE TABLE `friend` ( `id` int(11) NOT NULL AUTO_INCREMENT, `friend_1_id` int(11) NOT NULL, `friend_2_id` int(11) NOT NULL, UNIQUE KEY `uk_friend` (`friend_1_id`,`friend_2_id`), PRIMARY KEY (`id`) ) ENGINE=InnoDB; 在并发场景下，同时有两个人，设置为关注对方，就可能导致无法成功加为朋友关系。下面是这两个事务的执行语句列出来： 由于一开始 A 和 B 之间没有关注关系，所以两个事务里面的 select 语句查出来的结果都是空。因此，session 1 的逻辑就是“既然 B 没有关注 A，那就只插入一个单向关注关系”。session 2 也同样是这个逻辑。 这个结果对业务来说就是 bug 了。因为在业务设定里面，这两个逻辑都执行完成以后，是应该在 friend 表里面插入一行记录的。 “第 1 步即使使用了排他锁也不行，因为记录不存在，行锁无法生效”。不过，还有另外一个方法，来解决这个问题。 首先，要给“like”表增加一个字段，比如叫作 relation_ship，并设为整型，取值 1、2、3。 值是 1 的时候，表示 user_id 关注 liker_id; 值是 2 的时候，表示 liker_id 关注 user_id; 值是 3 的时候，表示互相关注。 然后，当 A 关注 B 的时候，逻辑改成如下所示的样子：应用代码里面，比较 A 和 B 的大小，如果 A\u003cB，就执行下面的逻辑 mysql\u003e begin; /*启动事务*/ insert into `like`(user_id, liker_id, relation_ship) values(A, B, 1) on duplicate key update relation_ship=relation_ship | 1; select relation_ship from `like` where user_id=A and liker_id=B; /*代码中判断返回的 relation_ship， 如果是 1，事务结束，执行 commit 如果是 3，则执行下面这两个语句： */ insert ignore into friend(friend_1_id, friend_2_id) values(A,B); commit; 如果 A\u003eB，则执行下面的逻辑 mysql\u003e begin; /*启动事务*/ insert into `like`(user_id, liker_id, relation_ship) values(B, A, 2) on duplicate key update relation_ship=relation_ship | 2; select relation_ship from `like` where user_id=B and liker_id=A; /*代码中判断返回的 relation_ship， 如果是 2，事务结束，执行 commit 如果是 3，则执行下面这两个语句： */ insert ignore into friend(friend_1_id, friend_2_id) values(B,A); commit; 这个设计里，让“like”表里的数据保证 user_id \u003c liker_id，这样不论是 A 关注 B，还是 B 关注 A，在操作“like”表的时候，如果反向的关系已经存在，就会出现行锁冲突。 然后，insert … on duplicate 语句，确保了在事务内部，执行了这个 SQL 语句后，就强行占住了这个行锁，之后的 select 判断 relation_ship 这个逻辑时就确保了是在行锁保护下的读操作。 操作符“|”是按位或，连同最后一句 insert 语句里的 ignore，是为了保证重复调用时的幂等性。 这样，即使在双方“同时”执行关注操作，最终数据库里的结果，也是 like 表里面有一条关于 A 和 B 的记录，而且 relation_ship 的值是 3，并且 friend 表里面也有了 A 和 B 的这条记录。 ","date":"2025-02-12","objectID":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/:11:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"15 | 答疑文章（一）：日志和索引相关问题","uri":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"12 问题 问：创建了一个简单的表 t，并插入一行，然后对这一行做修改。 mysql\u003e CREATE TABLE `t` ( `id` int(11) NOT NULL primary key auto_increment, `a` int(11) DEFAULT NULL ) ENGINE=InnoDB; insert into t values(1,2); 这时候，表 t 里有唯一的一行数据 (1,2)。假设现在要执行： mysql\u003e update t set a=2 where id=1; 你会看到这样的结果： 结果显示，匹配 (rows matched) 了一行，修改 (Changed) 了 0 行。仅从现象上看，MySQL 内部在处理这个命令的时候，可以有以下三种选择： 更新都是先读后写的，MySQL 读出数据，发现 a 的值本来就是 2，不更新，直接返回，执行结束； MySQL 调用了 InnoDB 引擎提供的“修改为 (1,2)”这个接口，但是引擎发现值与原来相同，不更新，直接返回； InnoDB 认真执行了“把这个值修改成 (1,2)“这个操作，该加锁的加锁，该更新的更新。 你觉得实际情况会是以上哪种呢？你可否用构造实验的方式，来证明你的结论？进一步地，可以思考一下，MySQL 为什么要选择这种策略呢？ 答：第一个选项是，MySQL 读出数据，发现值与原来相同，不更新，直接返回，执行结束。这里可以用一个锁实验来确认。假设，当前表 t 里的值是 (1,2)。 session B 的 update 语句被 blocked 了，加锁这个动作是 InnoDB 才能做的，所以排除选项 1。B 被阻塞，代表 A 拿到了写锁，如果只是读数据，发现相同，不更新直接返回，干嘛还拿写锁呢，既然拿了写锁，就不是第一个选项。 第二个选项是，MySQL 调用了 InnoDB 引擎提供的接口，但是引擎发现值与原来相同，不更新，直接返回。有没有这种可能呢？这里用一个可见性实验来确认。假设当前表里的值是 (1,2)。 session A 的第二个 select 语句是一致性读（快照读)，它是不能看见 session B 的更新的。现在它返回的是 (1,3)，表示它看见了某个新的版本，这个版本只能是 session A 自己的 update 语句做更新的时候生成。 所以答案应该是选项 3，即：InnoDB 认真执行了“把这个值修改成 (1,2)“这个操作，该加锁的加锁，该更新的更新。 MySQL 怎么这么笨，就不会更新前判断一下值是不是相同吗？如果判断一下，不就不用浪费 InnoDB 操作，多去更新一次了？ 其实 MySQL 是确认了的。只是在这个语句里面，MySQL 认为读出来的值，只有一个确定的 (id=1), 而要写的是 (a=3)，只从这两个信息是看不出来“不需要修改”的。作为验证，可以看一下下面这个例子。 ","date":"2025-02-12","objectID":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/:12:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"15 | 答疑文章（一）：日志和索引相关问题","uri":"/posts/15.%E7%AD%94%E7%96%91%E6%96%87%E7%AB%A0%E4%B8%80%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文深入探讨了 MySQL 中 InnoDB 引擎下的数据库表空间回收问题，特别是在删除数据后表文件大小未发生变化的情况。","date":"2025-02-12","objectID":"/posts/13.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/","tags":["MySQL 实战 45 讲","MySQL"],"title":"13 | 为什么表数据删掉一半，表文件大小不变？","uri":"/posts/13.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文深入探讨了 MySQL 中 InnoDB 引擎下的数据库表空间回收问题，特别是在删除数据后表文件大小未发生变化的情况。首先介绍了 InnoDB 表的组成结构和参数 innodb_file_per_table 的作用，建议将该参数设置为 ON 以便更好地管理表空间。随后详细说明了数据删除流程，包括记录和数据页的复用，以及删除和插入数据可能导致的空洞问题。最后，介绍了通过重建表来收缩表空间的方法，包括使用 alter table 命令和优化流程。 当数据库占用空间太大，把一个最大的表删掉了一半的数据，怎么表文件的大小还是没变？ 这里，还是针对 MySQL 中应用最广泛的 InnoDB 引擎展开讨论。一个 InnoDB 表包含两部分，即：表结构定义和数据。在 MySQL 8.0 版本以前，表结构是存在以.frm 为后缀的文件里。而 MySQL 8.0 版本，则已经允许把表结构定义放在系统数据表中了。因为表结构定义占用的空间很小。 ","date":"2025-02-12","objectID":"/posts/13.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"13 | 为什么表数据删掉一半，表文件大小不变？","uri":"/posts/13.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 参数 innodb_file_per_table 表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数 innodb_file_per_table 控制的： 这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起； 这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。 从 MySQL 5.6.6 版本开始，它的默认值就是 ON 了。建议你不论使用 MySQL 的哪个版本，都将这个值设置为 ON。因为，一个表单独存储为一个文件更容易管理，而且在不需要这个表的时候，通过 drop table 命令，系统就会直接删除这个文件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的。所以，将 innodb_file_per_table 设置为 ON，是推荐做法，接下来的讨论都是基于这个设置展开的。 在删除整个表的时候，可以使用 drop table 命令回收表空间。但是，遇到的更多的删除数据的场景是删除某些行，这时就遇到了文章开头的问题：表中的数据被删除了，但是表空间却没有被回收。要彻底搞明白这个问题的话，就要从数据删除流程说起了。 ","date":"2025-02-12","objectID":"/posts/13.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"13 | 为什么表数据删掉一半，表文件大小不变？","uri":"/posts/13.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 数据删除流程 先再来看一下 InnoDB 中一个索引的示意图。InnoDB 里的数据都是用 B+ 树的结构组织的。 假设，要删掉 R4 这个记录，InnoDB 引擎只会把 R4 这个记录标记为删除。如果之后要再插入一个 ID 在 300 和 600 之间的记录时，可能会复用这个位置。但是，磁盘文件的大小并不会缩小。 现在，你已经知道了 InnoDB 的数据是按页存储的，那么如果删掉了一个数据页上的所有记录，会怎么样？答案是，整个数据页就可以被复用了。但是，数据页的复用跟记录的复用是不同的。 记录的复用，只限于符合范围条件的数据。比如上面的这个例子，R4 这条记录被删除后，如果插入一个 ID 是 400 的行，可以直接复用这个空间。但如果插入的是一个 ID 是 800 的行，就不能复用这个位置了。 而当整个页从 B+ 树里面摘掉以后，可以复用到任何位置。以上图为例，如果将数据页 page A 上的所有记录删除以后，page A 会被标记为可复用。这时候如果要插入一条 ID=50 的记录需要使用新页的时候，page A 是可以被复用的。 如果相邻的两个数据页利用率都很小，系统就会把这两个页上的数据合到其中一个页上，另外一个数据页就被标记为可复用。 进一步地，如果用 delete 命令把整个表的数据删除呢？结果就是，所有的数据页都会被标记为可复用。但是磁盘上，文件不会变小。 所以 delete 命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过 delete 命令是不能回收表空间的。这些可以复用，而没有被使用的空间，看起来就像是“空洞”。实际上，不止是删除数据会造成空洞，插入数据也会。 如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂。假设上图中 page A 已经满了，这时要再插入一行数据，会怎样呢？ 可以看到，由于 page A 满了，再插入一个 ID 是 550 的数据时，就不得不再申请一个新的页面 page B 来保存数据了。页分裂完成后，page A 的末尾就留下了空洞（注意：实际上，可能不止 1 个记录的位置是空洞）。另外，更新索引上的值，可以理解为删除一个旧的值，再插入一个新值。不难理解，这也是会造成空洞的。 也就是说，经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。而重建表，就可以达到这样的目的。 ","date":"2025-02-12","objectID":"/posts/13.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"13 | 为什么表数据删掉一半，表文件大小不变？","uri":"/posts/13.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 重建表 如果现在有一个表 A，需要做空间收缩，为了把表中存在的空洞去掉，可以怎么做呢？ 可以新建一个与表 A 结构相同的表 B，然后按照主键 ID 递增的顺序，把数据一行一行地从表 A 里读出来再插入到表 B 中。 由于表 B 是新建的表，所以表 A 主键索引上的空洞，在表 B 中就都不存在了。显然地，表 B 的主键索引更紧凑，数据页的利用率也更高。如果把表 B 作为临时表，数据从表 A 导入表 B 的操作完成后，用表 B 替换 A，从效果上看，就起到了收缩表 A 空间的作用。 这里，可以使用 alter table A engine=InnoDB 命令来重建表。在 MySQL 5.5 版本之前，这个命令的执行流程跟前面描述的差不多，区别只是这个临时表 B 不需要自己创建，MySQL 会自动完成转存数据、交换表名、删除旧表的操作。 显然，花时间最多的步骤是往临时表插入数据的过程，如果在这个过程中，有新的数据要写入到表 A 的话，就会造成数据丢失。因此，在整个 DDL 过程中，表 A 中不能有更新。也就是说，这个 DDL 不是 Online 的。 而在 MySQL 5.6 版本开始引入的 Online DDL，对这个操作流程做了优化。简单描述一下引入了 Online DDL 之后，重建表的流程： 建立一个临时文件，扫描表 A 主键的所有数据页； 用数据页中表 A 的记录生成 B+ 树，存储到临时文件中； 生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中，对应的是图中 state2 的状态； 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件，对应的就是图中 state3 的状态； 用临时文件替换表 A 的数据文件。 可以看到，两幅图的不同之处在于，由于日志文件记录和重放操作这个功能的存在，这个方案在重建表的过程中，允许对表 A 做增删改操作。这也就是 Online DDL 名字的来源。 上图的流程中，alter 语句在启动的时候需要获取 MDL 写锁，但是这个写锁在真正拷贝数据之前就退化成读锁了。为什么要退化呢？为了实现 Online，MDL 读锁不会阻塞增删改操作。那为什么不干脆直接解锁呢？为了保护自己，禁止其他线程对这个表同时做 DDL。 而对于一个大表来说，Online DDL 最耗时的过程就是拷贝数据到临时表的过程，这个步骤的执行期间可以接受增删改操作。所以，相对于整个 DDL 过程来说，锁的时间非常短。对业务来说，就可以认为是 Online 的。 需要补充说明的是，上述的这些重建方法都会扫描原表数据和构建临时文件。对于很大的表来说，这个操作是很消耗 IO 和 CPU 资源的。因此，如果是线上服务，要很小心地控制操作时间。如果想要比较安全的操作的话，推荐你使用 GitHub 开源的 gh-ost 来做。 ","date":"2025-02-12","objectID":"/posts/13.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"13 | 为什么表数据删掉一半，表文件大小不变？","uri":"/posts/13.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 Online 和 inplace 在第三幅图中，把表 A 中的数据导出来的存放位置叫作 tmp_table。这是一个临时表，是在 server 层创建的。 在第四幅图中，根据表 A 重建出来的数据是放在“tmp_file”里的，这个临时文件是 InnoDB 在内部创建出来的。整个 DDL 过程都在 InnoDB 内部完成。对于 server 层来说，没有把数据挪动到临时表，是一个“原地”操作，这就是“inplace”名称的来源。 如果你有一个 1TB 的表，现在磁盘间是 1.2TB，能不能做一个 inplace 的 DDL 呢？答案是不能。因为，tmp_file 也是要占用临时空间的。重建表的这个语句 alter table t engine=InnoDB，其实隐含的意思是： alter table t engine=innodb,ALGORITHM=inplace; 跟 inplace 对应的就是拷贝表的方式了，用法是： alter table t engine=innodb,ALGORITHM=copy; 当使用 ALGORITHM=copy 的时候，表示的是强制拷贝表，对应的流程就是第三幅图的操作过程。你可能会觉得，inplace 跟 Online 是不是就是一个意思？其实不是的，只是在重建表这个逻辑中刚好是这样而已。比如，如果要给 InnoDB 表的一个字段加全文索引，写法是： alter table t add FULLTEXT(field_name); 这个过程是 inplace 的，但会阻塞增删改操作，是非 Online 的。如果说这两个逻辑之间的关系是什么的话，可以概括为： DDL 过程如果是 Online 的，就一定是 inplace 的； 反过来未必，也就是说 inplace 的 DDL，有可能不是 Online 的。截止到 MySQL 8.0，添加全文索引（FULLTEXT index）和空间索引 (SPATIAL index) 就属于这种情况。 使用 optimize table、analyze table 和 alter table 这三种方式重建表有什么区别？ 从 MySQL 5.6 版本开始，alter table t engine = InnoDB（也就是 recreate）默认的就是上面第四幅图的流程了； analyze table t 其实不是重建表，只是对表的索引信息做重新统计，没有修改数据，这个过程中加了 MDL 读锁； optimize table t 等于 recreate+analyze。 ","date":"2025-02-12","objectID":"/posts/13.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"13 | 为什么表数据删掉一半，表文件大小不变？","uri":"/posts/13.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 小结 如果要收缩一个表，只是 delete 掉表里面不用的数据的话，表文件的大小是不会变的，还要通过 alter table 命令重建表，才能达到表文件变小的目的。重建表有两种实现方式，Online DDL 的方式是可以考虑在业务低峰期使用的，而 MySQL 5.5 及之前的版本，这个命令是会阻塞 DML 的，这个需要特别小心。 ","date":"2025-02-12","objectID":"/posts/13.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"13 | 为什么表数据删掉一半，表文件大小不变？","uri":"/posts/13.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"6 问题 问：假设现在有人碰到了一个“想要收缩表空间，结果适得其反”的情况，看上去是这样的： 一个表 t 文件大小为 1TB； 对这个表执行 alter table t engine=InnoDB； 发现执行完成后，空间不仅没变小，还稍微大了一点儿，比如变成了 1.01TB。 你觉得可能是什么原因呢？ 答：这个表本身就已经没有空洞的了，比如说刚刚做过一次重建表操作。 将表 t 重建一次； 插入一部分数据，但是插入的这些数据，用掉了一部分的预留空间； 这种情况下，再重建一次表 t，就可能会出现问题中的现象。 ","date":"2025-02-12","objectID":"/posts/13.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/:6:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"13 | 为什么表数据删掉一半，表文件大小不变？","uri":"/posts/13.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文讲解了 MySQL 中的 count(*) 语句在不同引擎中有不同的实现方式。","date":"2025-02-12","objectID":"/posts/14.count%E8%BF%99%E4%B9%88%E6%85%A2%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E/","tags":["MySQL 实战 45 讲","MySQL"],"title":"14 | count(*) 这么慢，我该怎么办？","uri":"/posts/14.count%E8%BF%99%E4%B9%88%E6%85%A2%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 MySQL 中的 count(*) 语句在不同引擎中有不同的实现方式。MyISAM 引擎直接返回存储在磁盘上的总行数，效率高；而 InnoDB 引擎需要逐行读取数据并累积计数，导致执行速度变慢。针对频繁统计表行数的需求，建议自行计数或使用缓存系统保存计数，如 Redis 服务，但存在数据不一致和丢失更新的问题。 在开发系统的时候，可能经常需要计算一个表的行数，比如一个交易系统的所有变更记录总数。这时候你可能会想，一条 select count(*) from t 语句不就解决了吗？但是，你会发现随着系统中记录数越来越多，这条语句执行得也会越来越慢。count(*) 语句到底是怎样实现的呢？以及 MySQL 为什么会这么实现呢？ ","date":"2025-02-12","objectID":"/posts/14.count%E8%BF%99%E4%B9%88%E6%85%A2%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"14 | count(*) 这么慢，我该怎么办？","uri":"/posts/14.count%E8%BF%99%E4%B9%88%E6%85%A2%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 count(*) 的实现方式 首先要明确的是，在不同的 MySQL 引擎中，count(*) 有不同的实现方式。 MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高； 而 InnoDB 引擎就麻烦了，它执行 count(*) 的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。 这里需要注意的是，这篇文章里讨论的是没有过滤条件的 count(*)，如果加了 where 条件的话，MyISAM 表也是不能返回得这么快的。 因为不论是在事务支持、并发能力还是在数据安全方面，InnoDB 都优于 MyISAM。你的表也一定是用了 InnoDB 引擎。这就是当你的记录数越来越多的时候，计算一个表的总行数会越来越慢的原因。那为什么 InnoDB 不跟 MyISAM 一样，也把数字存起来呢？ 这是因为即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的。这里，用一个算 count(*) 的例子来解释一下。 假设表 t 中现在有 10000 条记录，设计了三个用户并行的会话。 会话 A 先启动事务并查询一次表的总行数； 会话 B 启动事务，插入一行后记录后，查询表的总行数； 会话 C 先启动一个单独的语句，插入一行记录后，查询表的总行数。 假设从上到下是按照时间顺序执行的，同一行语句是在同一时刻执行的。 你会看到，在最后一个时刻，三个会话 A、B、C 会同时查询表 t 的总行数，但拿到的结果却不同。这和 InnoDB 的事务设计有关系，可重复读是它默认的隔离级别，在代码上就是通过多版本并发控制，也就是 MVCC 来实现的。每一行记录都要判断自己是否对这个会话可见，因此对于 count(*) 请求来说，InnoDB 只好把数据一行一行地读出依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数。 当然，现在这个看上去笨笨的 MySQL，在执行 count(*) 操作的时候还是做了优化的。 InnoDB 是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是主键值。所以，普通索引树比主键索引树小很多。对于 count(*) 这样的操作，遍历哪个索引树得到的结果逻辑上都是一样的。因此，MySQL 优化器会找到最小的那棵树来遍历。在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。 如果你用过 show table status 命令的话，就会发现这个命令的输出结果里面也有一个 TABLE_ROWS 用于显示这个表当前有多少行，这个命令执行挺快的，那这个 TABLE_ROWS 能代替 count(*) 吗？ 索引统计的值是通过采样来估算的。实际上，TABLE_ROWS 就是从这个采样估算得来的，因此它也很不准。有多不准呢，官方文档说误差可能达到 40% 到 50%。所以，show table status 命令显示的行数也不能直接使用。到这里我们小结一下： MyISAM 表虽然 count(*) 很快，但是不支持事务； show table status 命令虽然返回很快，但是不准确； InnoDB 表直接 count(*) 会遍历全表，虽然结果准确，但会导致性能问题。 那么如果你现在有一个页面经常要显示交易系统的操作记录总数，到底应该怎么办呢？答案是，只能自己计数。这些方法的基本思路就是：需要自己找一个地方，把操作记录表的行数存起来。 ","date":"2025-02-12","objectID":"/posts/14.count%E8%BF%99%E4%B9%88%E6%85%A2%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"14 | count(*) 这么慢，我该怎么办？","uri":"/posts/14.count%E8%BF%99%E4%B9%88%E6%85%A2%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 用缓存系统保存计数 对于更新很频繁的库来说，你可能会第一时间想到，用缓存系统来支持。可以用一个 Redis 服务来保存这个表的总行数。这个表每被插入一行 Redis 计数就加 1，每被删除一行 Redis 计数就减 1。这种方式下，读和更新操作都很快，但这种方式存在什么问题吗？没错，缓存系统可能会丢失更新。 Redis 的数据不能永久地留在内存里，所以你会找一个地方把这个值定期地持久化存储起来。但即使这样，仍然可能丢失更新。试想如果刚刚在数据表中插入了一行，Redis 中保存的值也加了 1，然后 Redis 异常重启了，重启后要从存储 redis 数据的地方把这个值读回来，而刚刚加 1 的这个计数操作却丢失了。 当然了，这还是有解的。比如，Redis 异常重启以后，到数据库里面单独执行一次 count(*) 获取真实的行数，再把这个值写回到 Redis 里就可以了。异常重启毕竟不是经常出现的情况，这一次全表扫描的成本，还是可以接受的。 但实际上，将计数保存在缓存系统中的方式，还不只是丢失更新的问题。即使 Redis 正常工作，这个值还是逻辑上不精确的。 你可以设想一下有这么一个页面，要显示操作记录的总数，同时还要显示最近操作的 100 条记录。那么，这个页面的逻辑就需要先到 Redis 里面取出计数，再到数据表里面取数据记录。是这么定义不精确的： 一种是，查到的 100 行结果里面有最新插入记录，而 Redis 的计数里还没加 1； 另一种是，查到的 100 行结果里没有最新插入的记录，而 Redis 的计数里已经加了 1。 这两种情况，都是逻辑不一致的。我们一起来看看这个时序图。 上图中，会话 A 是一个插入交易记录的逻辑，往数据表里插入一行 R，然后 Redis 计数加 1；会话 B 就是查询页面显示时需要的数据。 在上图的这个时序里，在 T3 时刻会话 B 来查询的时候，会显示出新插入的 R 这个记录，但是 Redis 的计数还没加 1。这时候，就会出现数据不一致。 你一定会说，这是因为执行新增记录逻辑时候，是先写数据表，再改 Redis 计数。而读的时候是先读 Redis，再读数据表，这个顺序是相反的。那么，如果保持顺序一样的话，是不是就没问题了？现在把会话 A 的更新顺序换一下，再看看执行结果。 这时候反过来了，会话 B 在 T3 时刻查询的时候，Redis 计数加了 1 了，但还查不到新插入的 R 这一行，也是数据不一致的情况。 在并发系统里面，无法精确控制不同线程的执行时刻的，因为存在图中的这种操作序列，所以，即使 Redis 正常工作，这个计数值还是逻辑上不精确的。 ","date":"2025-02-12","objectID":"/posts/14.count%E8%BF%99%E4%B9%88%E6%85%A2%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"14 | count(*) 这么慢，我该怎么办？","uri":"/posts/14.count%E8%BF%99%E4%B9%88%E6%85%A2%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 在数据库保存计数 根据上面的分析，用缓存系统保存计数有丢失数据和计数不精确的问题。那么，如果把这个计数直接放到数据库里单独的一张计数表 C 中，又会怎么样呢？首先，这解决了崩溃丢失的问题，InnoDB 是支持崩溃恢复不丢数据的。然后，再看看能不能解决计数不精确的问题。 由于 InnoDB 要支持事务，从而导致 InnoDB 表不能把 count(*) 直接存起来，然后查询的时候直接返回形成的。所谓以子之矛攻子之盾，现在就利用“事务”这个特性，把问题解决掉。 我们来看下现在的执行结果。虽然会话 B 的读操作仍然是在 T3 执行的，但是因为这时候更新事务还没有提交，所以计数值加 1 这个操作对会话 B 还不可见。因此，会话 B 看到的结果里，查计数值和“最近 100 条记录”看到的结果，逻辑上就是一致的。 ","date":"2025-02-12","objectID":"/posts/14.count%E8%BF%99%E4%B9%88%E6%85%A2%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"14 | count(*) 这么慢，我该怎么办？","uri":"/posts/14.count%E8%BF%99%E4%B9%88%E6%85%A2%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 不同的 count 用法 在 select count(?) from t 这样的查询语句里面，count(*)、count(主键 id)、count(字段) 和 count(1) 等不同用法的性能，有哪些差别。需要注意的是，下面的讨论还是基于 InnoDB 引擎的。 这里，首先你要弄清楚 count() 的语义。count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值。 所以，count(*)、count(主键 id) 和 count(1) 都表示返回满足条件的结果集的总行数；而 count(字段），则表示返回满足条件的数据行里面，参数“字段”不为 NULL 的总个数。至于分析性能差别的时候，可以记住这么几个原则： server 层要什么就给什么； InnoDB 只给必要的值； 现在的优化器只优化了 count(*) 的语义为“取行数”，其他“显而易见”的优化并没有做。 对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。 对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。 单看这两个用法的差别的话，能对比出来，count(1) 执行得要比 count(主键 id) 快。因为从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。 对于 count(字段) 来说： 如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加； 如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。 也就是前面的第一条原则，server 层要什么字段，InnoDB 就返回什么字段。 但是 count(*) 是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加。 优化器就不能自己判断一下吗，主键 id 肯定非空啊，为什么不能按照 count(*) 来处理，多么简单的优化啊。当然，MySQL 专门针对这个语句进行优化，也不是不可以。但是这种需要专门优化的情况太多了，而且 MySQL 已经优化过 count(*) 了，直接使用这种用法就可以了。 所以结论是：按照效率排序的话，count(字段)\u003ccount(主键 id)\u003ccount(1)≈count(*)，所以建议你，尽量使用 count(*)。 ","date":"2025-02-12","objectID":"/posts/14.count%E8%BF%99%E4%B9%88%E6%85%A2%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"14 | count(*) 这么慢，我该怎么办？","uri":"/posts/14.count%E8%BF%99%E4%B9%88%E6%85%A2%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 小结 其实，把计数放在 Redis 里面，不能够保证计数和 MySQL 表里的数据精确一致的原因，是这两个不同的存储构成的系统，不支持分布式事务，无法拿到精确一致的视图。而把计数值也放在 MySQL 中，就解决了一致性视图的问题。 InnoDB 引擎支持事务，利用好事务的原子性和隔离性，就可以简化在业务开发时的逻辑。这也是 InnoDB 引擎备受青睐的原因之一。 ","date":"2025-02-12","objectID":"/posts/14.count%E8%BF%99%E4%B9%88%E6%85%A2%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"14 | count(*) 这么慢，我该怎么办？","uri":"/posts/14.count%E8%BF%99%E4%B9%88%E6%85%A2%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"6 问题 问：在刚刚讨论的方案中，用了事务来确保计数准确。由于事务可以保证中间结果不被别的事务读到，因此修改计数值和插入新记录的顺序是不影响逻辑结果的。但是，从并发系统性能的角度考虑，你觉得在这个事务序列里，应该先插入操作记录，还是应该先更新计数表呢？ 答：并发系统性能的角度考虑，应该先插入操作记录，再更新计数表。因为更新计数表涉及到行锁的竞争，先插入再更新能最大程度地减少事务之间的锁等待，提升并发度。 ","date":"2025-02-12","objectID":"/posts/14.count%E8%BF%99%E4%B9%88%E6%85%A2%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E/:6:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"14 | count(*) 这么慢，我该怎么办？","uri":"/posts/14.count%E8%BF%99%E4%B9%88%E6%85%A2%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文通过对 InnoDB 的工作机制进行比喻，解释了数据库“抖动”现象的原因。","date":"2025-02-12","objectID":"/posts/12.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E7%9A%84mysql%E4%BC%9A%E6%8A%96%E4%B8%80%E4%B8%8B/","tags":["MySQL 实战 45 讲","MySQL"],"title":"12 | 为什么我的 MySQL 会“抖”一下？","uri":"/posts/12.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E7%9A%84mysql%E4%BC%9A%E6%8A%96%E4%B8%80%E4%B8%8B/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文通过对 InnoDB 的工作机制进行比喻，解释了数据库“抖动”现象的原因。首先介绍了 InnoDB 的 WAL 机制，即写日志和内存数据页的刷新过程，分析了导致数据库刷新过程的几种情况，如 redo log 写满、系统内存不足等。指出这些情况会明显影响数据库性能，尤其是当查询需要淘汰大量脏页或者日志写满时，会导致查询响应时间明显变长甚至更新操作完全堵塞。最后，提到 InnoDB 需要有控制脏页比例的机制来尽量避免性能问题的发生。 平时的工作中，你可能遇到过这样的场景，一条 SQL 语句，正常执行的时候特别快，但是有时也不知道怎么回事，它就会变得特别慢，并且这样的场景很难复现，它不只随机，而且持续时间还很短。看上去，这就像是数据库“抖”了一下。 ","date":"2025-02-12","objectID":"/posts/12.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E7%9A%84mysql%E4%BC%9A%E6%8A%96%E4%B8%80%E4%B8%8B/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"12 | 为什么我的 MySQL 会“抖”一下？","uri":"/posts/12.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E7%9A%84mysql%E4%BC%9A%E6%8A%96%E4%B8%80%E4%B8%8B/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 你的 SQL 语句为什么变“慢”了 InnoDB 在处理更新语句的时候，只做了写日志这一个磁盘操作。这个日志叫作 redo log（重做日志），在更新内存写完 redo log 后，就返回给客户端，本次更新成功。 做下类比的话，掌柜记账的账本是数据文件，记账用的粉板是日志文件（redo log），掌柜的记忆就是内存。 掌柜总要找时间把账本更新一下，这对应的就是把内存里的数据写入磁盘的过程，术语就是 flush。在这个 flush 操作执行之前，孔乙己的赊账总额，其实跟掌柜手中账本里面的记录是不一致的。因为孔乙己今天的赊账金额还只在粉板上，而账本里的记录是老的，还没把今天的赊账算进去。 当内存数据页跟磁盘数据页内容不一致的时候，称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。不论是脏页还是干净页，都在内存中。在这个例子里，内存对应的就是掌柜的记忆。 接下来，用一个示意图来展示一下“孔乙己赊账”的整个操作过程。假设原来孔乙己欠账 10 文，这次又要赊 9 文。 回到文章开头的问题，平时执行很快的更新操作，其实就是在写内存和日志，而 MySQL 偶尔“抖”一下的那个瞬间，可能就是在刷脏页（flush）。那么，什么情况会引发数据库的 flush 过程呢？ 还是继续用咸亨酒店掌柜的这个例子，想一想：掌柜在什么情况下会把粉板上的赊账记录改到账本上？ 第一种场景是，粉板满了，记不下了。这时候如果再有人来赊账，掌柜就只得放下手里的活儿，将粉板上的记录擦掉一些，留出空位以便继续记账。当然在擦掉之前，他必须先将正确的账目记录到账本中才行。这个场景，对应的就是 InnoDB 的 redo log 写满了。这时候系统会停止所有更新操作，把 checkpoint 往前推进，redo log 留出空间可以继续写。checkpoint 可不是随便往前修改一下位置就可以的。比如下图中，把 checkpoint 位置从 CP 推进到 CP’，就需要将两个点之间的日志（浅绿色部分），对应的所有脏页都 flush 到磁盘上。之后，图中从 write pos 到 CP’之间就是可以再写入的 redo log 的区域。 第二种场景是，这一天生意太好，要记住的事情太多，掌柜发现自己快记不住了，赶紧找出账本把孔乙己这笔账先加进去。这种场景，对应的就是系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。这时候难道不能直接把内存淘汰掉，下次需要请求的时候，从磁盘读入数据页，然后拿 redo log 出来应用不就行了？这里其实是从性能考虑的。如果刷脏页一定会写盘，就保证了每个数据页有两种状态： 一种是内存里存在，内存里就肯定是正确的结果，直接返回； 另一种是内存里没有数据，就可以肯定数据文件上是正确的结果，读入内存后返回。这样的效率最高。 第三种场景是，生意不忙的时候，或者打烊之后。这时候柜台没事，掌柜闲着也是闲着，不如更新账本。这种场景，对应的就是 MySQL 认为系统“空闲”的时候。当然，MySQL“这家酒店”的生意好起来可是会很快就能把粉板记满的，所以“掌柜”要合理地安排时间，即使是“生意好”的时候，也要见缝插针地找时间，只要有机会就刷一点“脏页”。 第四种场景是，年底了咸亨酒店要关门几天，需要把账结清一下。这时候掌柜要把所有账都记到账本上，这样过完年重新开张的时候，就能就着账本明确账目情况了。这种场景，对应的就是 MySQL 正常关闭的情况。这时候，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。 接下来可以分析一下上面四种场景对性能的影响。 其中，第三种情况是属于 MySQL 空闲时的操作，这时系统没什么压力，而第四种场景是数据库本来就要关闭了。这两种情况下也不会太关注“性能”问题。所以这里，主要来分析一下前两种场景下的性能问题。 第一种是“redo log 写满了，要 flush 脏页”，这种情况是 InnoDB 要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果从监控上看，这时候更新数会跌为 0。 第二种是“内存不够用了，要先将脏页写到磁盘”，这种情况其实是常态。InnoDB 用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态： 第一种是，还没有使用的； 第二种是，使用了并且是干净页； 第三种是，使用了并且是脏页。 InnoDB 的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；但如果是脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用。所以，刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的： 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长； 日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的。 所以，InnoDB 需要有控制脏页比例的机制，来尽量避免上面的这两种情况。 ","date":"2025-02-12","objectID":"/posts/12.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E7%9A%84mysql%E4%BC%9A%E6%8A%96%E4%B8%80%E4%B8%8B/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"12 | 为什么我的 MySQL 会“抖”一下？","uri":"/posts/12.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E7%9A%84mysql%E4%BC%9A%E6%8A%96%E4%B8%80%E4%B8%8B/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 InnoDB 刷脏页的控制策略 首先，你要正确地告诉 InnoDB 所在主机的 IO 能力，这样 InnoDB 才能知道需要全力刷脏页的时候，可以刷多快。 这就要用到 innodb_io_capacity 这个参数了，它会告诉 InnoDB 你的磁盘能力。这个值建议设置成磁盘的 IOPS。磁盘的 IOPS 可以通过 fio 这个工具来测试，下面的语句是用来测试磁盘随机读写的命令： fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest 其实，因为没能正确地设置 innodb_io_capacity 参数，而导致的性能问题也比比皆是。如果说你的 MySQL 的写入速度很慢，TPS 很低，但是数据库主机的 IO 压力并不大。很可能罪魁祸首就是这个参数的设置出了问题。 假如你的主机磁盘用的是 SSD，但是 innodb_io_capacity 的值设置的是 300。于是，InnoDB 认为这个系统的能力就这么差，所以刷脏页刷得特别慢，甚至比脏页生成的速度还慢，这样就造成了脏页累积，影响了查询和更新性能。 虽然现在已经定义了“全力刷脏页”的行为，但平时总不能一直是全力刷吧？毕竟磁盘能力不能只用来刷脏页，还需要服务用户请求。所以接下来，我们看看 InnoDB 怎么控制引擎按照“全力”的百分比来刷脏页。试想一下，如果你来设计策略控制刷脏页的速度，会参考哪些因素呢？ 这个问题可以这么想，如果刷太慢，会出现什么情况？首先是内存脏页太多，其次是 redo log 写满。所以，InnoDB 的刷盘速度就是要参考这两个因素：一个是脏页比例，一个是 redo log 写盘速度。InnoDB 会根据这两个因素先单独算出两个数字。 参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认值是 75%。InnoDB 会根据当前的脏页比例（假设为 M），算出一个范围在 0 到 100 之间的数字，计算这个数字的伪代码类似这样： F1(M) { if M\u003e=innodb_max_dirty_pages_pct then return 100; return 100*M/innodb_max_dirty_pages_pct; } InnoDB 每次写入的日志都有一个序号，当前写入的序号跟 checkpoint 对应的序号之间的差值，假设为 N。InnoDB 会根据这个 N 算出一个范围在 0 到 100 之间的数字，这个计算公式可以记为 F2(N)。F2(N) 算法比较复杂，你只要知道 N 越大，算出来的值越大就好了。 然后，根据上述算得的 F1(M) 和 F2(N) 两个值，取其中较大的值记为 R，之后引擎就可以按照 innodb_io_capacity 定义的能力乘以 R% 来控制刷脏页的速度。上述的计算流程比较抽象，不容易理解，下面是一个简单的流程图。图中的 F1、F2 就是上面通过脏页比例和 redo log 写入速度算出来的两个值。 InnoDB 会在后台刷脏页，而刷脏页的过程是要将内存页写入磁盘。所以，无论是你的查询语句在需要内存的时候可能要求淘汰一个脏页，还是由于刷脏页的逻辑会占用 IO 资源并可能影响到了你的更新语句，都可能是造成你从业务端感知到 MySQL“抖”了一下的原因。 要尽量避免这种情况，就要合理地设置 innodb_io_capacity 的值，并且平时要多关注脏页比例，不要让它经常接近 75%。其中，脏页比例是通过 Innodb_buffer_pool_pages_dirty/Innodb_buffer_pool_pages_total 得到的，具体的命令参考下面的代码： mysql\u003e select VARIABLE_VALUE into @a from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_dirty'; select VARIABLE_VALUE into @b from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_total'; select @a/@b; 接下来，再看一个有趣的策略。 一旦一个查询请求需要在执行过程中先 flush 掉一个脏页时，这个查询就可能要比平时慢了。而 MySQL 中的一个机制，可能让查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。 在 InnoDB 中，innodb_flush_neighbors 参数就是用来控制这个行为的，值为 1 的时候会有上述的“连坐”机制，值为 0 时表示不找邻居，自己刷自己的。 找“邻居”这个优化在机械硬盘时代是很有意义的，可以减少很多随机 IO。机械硬盘的随机 IOPS 一般只有几百，相同的逻辑操作减少随机 IO 就意味着系统性能的大幅度提升。 而如果使用的是 SSD 这类 IOPS 比较高的设备的话，建议你把 innodb_flush_neighbors 的值设置成 0。因为这时候 IOPS 往往不是瓶颈，而“只刷自己”，就能更快地执行完必要的刷脏页操作，减少 SQL 语句响应时间。 在 MySQL 8.0 中，innodb_flush_neighbors 参数的默认值已经是 0 了。 ","date":"2025-02-12","objectID":"/posts/12.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E7%9A%84mysql%E4%BC%9A%E6%8A%96%E4%B8%80%E4%B8%8B/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"12 | 为什么我的 MySQL 会“抖”一下？","uri":"/posts/12.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E7%9A%84mysql%E4%BC%9A%E6%8A%96%E4%B8%80%E4%B8%8B/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 小结 脏页会被后台线程自动 flush，也会由于数据页淘汰而触发 flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些。 ","date":"2025-02-12","objectID":"/posts/12.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E7%9A%84mysql%E4%BC%9A%E6%8A%96%E4%B8%80%E4%B8%8B/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"12 | 为什么我的 MySQL 会“抖”一下？","uri":"/posts/12.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E7%9A%84mysql%E4%BC%9A%E6%8A%96%E4%B8%80%E4%B8%8B/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 问题 问：一个内存配置为 128GB、innodb_io_capacity 设置为 20000 的大规格实例，正常会建议你将 redo log 设置成 4 个 1GB 的文件。但如果在配置的时候不慎将 redo log 设置成了 1 个 100M 的文件，会发生什么情况呢？又为什么会出现这样的情况呢？ 答：每次事务提交都要写 redo log，如果设置太小，很快就会被写满，也就是下面这个图的状态，这个“环”将很快被写满，write pos 一直追着 CP。这时候系统不得不停止所有更新，去推进 checkpoint。 这时，看到的现象就是磁盘压力很小，但是数据库出现间歇性的性能下跌。 ","date":"2025-02-12","objectID":"/posts/12.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E7%9A%84mysql%E4%BC%9A%E6%8A%96%E4%B8%80%E4%B8%8B/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"12 | 为什么我的 MySQL 会“抖”一下？","uri":"/posts/12.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E7%9A%84mysql%E4%BC%9A%E6%8A%96%E4%B8%80%E4%B8%8B/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文介绍了如何给字符串字段加索引以及前缀索引对查询性能的影响。","date":"2025-02-12","objectID":"/posts/11.%E6%80%8E%E4%B9%88%E7%BB%99%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%AD%97%E6%AE%B5%E5%8A%A0%E7%B4%A2%E5%BC%95/","tags":["MySQL 实战 45 讲","MySQL"],"title":"11 | 怎么给字符串字段加索引？","uri":"/posts/11.%E6%80%8E%E4%B9%88%E7%BB%99%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%AD%97%E6%AE%B5%E5%8A%A0%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文介绍了如何给字符串字段加索引以及前缀索引对查询性能的影响。首先，文章讨论了在支持邮箱登录的系统中，如何在邮箱字段上建立合理的索引。通过对比全字段索引和前缀索引的执行过程，阐述了前缀索引可能增加查询成本的情况。接着，文章提出了确定前缀长度的方法，即通过统计索引上不同值的数量来选择合适的前缀长度。 现在，几乎所有的系统都支持邮箱登录，那如何在邮箱这样的字段上建立合理的索引呢？假设，你现在维护一个支持邮箱登录的系统，用户表是这么定义的： mysql\u003e create table SUser( ID bigint unsigned primary key, email varchar(64), ... )engine=innodb; 由于要使用邮箱登录，所以业务代码中一定会出现类似于这样的语句： mysql\u003e select f1, f2 from SUser where email='xxx'; 如果 email 这个字段上没有索引，那么这个语句就只能做全表扫描。同时，MySQL 是支持前缀索引的，也就是说，可以定义字符串的一部分作为索引。默认地，如果创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。比如，这两个在 email 字段上创建索引的语句： mysql\u003e alter table SUser add index index1(email); 或 mysql\u003e alter table SUser add index index2(email(6)); 第一个语句创建的 index1 索引里面，包含了每个记录的整个字符串；而第二个语句创建的 index2 索引里面，对于每个记录都是只取前 6 个字节。那么，这两种不同的定义在数据结构和存储上有什么区别呢？如下面两幅图所示，就是这两个索引的示意图。 从图中可以看到，由于 email(6) 这个索引结构中每个邮箱字段都只取前 6 个字节（即：zhangs），所以占用的空间会更小，这就是使用前缀索引的优势。但，这同时带来的损失是，可能会增加额外的记录扫描次数。接下来，再看看下面这个语句，在这两个索引定义下分别是怎么执行的。 select id,name,email from SUser where email='zhangssxyz@xxx.com'; 如果使用的是 index1（即 email 整个字符串的索引结构），执行顺序是这样的： 从 index1 索引树找到满足索引值是’zhangssxyz@xxx.com’的这条记录，取得 ID2 的值； 到主键上查到主键值是 ID2 的行，判断 email 的值是正确的，将这行记录加入结果集； 取 index1 索引树上刚刚查到的位置的下一条记录，发现已经不满足 email='zhangssxyz@xxx.com’的条件了，循环结束。 这个过程中，只需要回主键索引取一次数据，所以系统认为只扫描了一行。 如果使用的是 index2（即 email(6) 索引结构），执行顺序是这样的： 从 index2 索引树找到满足索引值是’zhangs’的记录，找到的第一个是 ID1； 到主键上查到主键值是 ID1 的行，判断出 email 的值不是’zhangssxyz@xxx.com’，这行记录丢弃； 取 index2 上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出 ID2，再到 ID 索引上取整行然后判断，这次值对了，将这行记录加入结果集； 重复上一步，直到在 idxe2 上取到的值不是’zhangs’时，循环结束。 在这个过程中，要回主键索引取 4 次数据，也就是扫描了 4 行。通过这个对比，很容易就可以发现，使用前缀索引后，可能会导致查询语句读数据的次数变多。 但是，对于这个查询语句来说，如果定义的 index2 不是 email(6) 而是 email(7），也就是说取 email 字段的前 7 个字节来构建索引的话，即满足前缀’zhangss’的记录只有一个，也能够直接查到 ID2，只扫描一行就结束了。 也就是说使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。于是，就有个问题：当要给字符串创建前缀索引时，有什么方法能够确定应该使用多长的前缀呢？ 实际上，在建立索引时关注的是区分度，区分度越高越好。因为区分度越高，意味着重复的键值越少。因此，可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。首先，可以使用下面这个语句，算出这个列上有多少个不同的值： mysql\u003e select count(distinct email) as L from SUser; 然后，依次选取不同长度的前缀来看这个值，比如要看一下 4~7 个字节的前缀索引，可以用这个语句： mysql\u003e select count(distinct left(email,4)）as L4, count(distinct left(email,5)）as L5, count(distinct left(email,6)）as L6, count(distinct left(email,7)）as L7, from SUser; 当然，使用前缀索引很可能会损失区分度，所以需要预先设定一个可以接受的损失比例，比如 5%。然后，在返回的 L4~L7 中，找出不小于 L * 95% 的值，假设这里 L6、L7 都满足，就可以选择前缀长度为 6。 ","date":"2025-02-12","objectID":"/posts/11.%E6%80%8E%E4%B9%88%E7%BB%99%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%AD%97%E6%AE%B5%E5%8A%A0%E7%B4%A2%E5%BC%95/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"11 | 怎么给字符串字段加索引？","uri":"/posts/11.%E6%80%8E%E4%B9%88%E7%BB%99%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%AD%97%E6%AE%B5%E5%8A%A0%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 前缀索引对覆盖索引的影响 前面说了使用前缀索引可能会增加扫描行数，这会影响到性能。其实，前缀索引的影响不止如此，再看一下另外一个场景。先来看看这个 SQL 语句： select id,email from SUser where email='zhangssxyz@xxx.com'; 与前面例子中的 SQL 语句 select id,name,email from SUser where email='zhangssxyz@xxx.com'; 相比，这个语句只要求返回 id 和 email 字段。 所以，如果使用 index1（即 email 整个字符串的索引结构）的话，可以利用覆盖索引，从 index1 查到结果后直接就返回了，不需要回到 ID 索引再去查一次。而如果使用 index2（即 email(6) 索引结构）的话，就不得不回到 ID 索引再去判断 email 字段的值。 即使你将 index2 的定义修改为 email(18) 的前缀索引，这时候虽然 index2 已经包含了所有的信息，但 InnoDB 还是要回到 id 索引再查一下，因为系统并不确定前缀索引的定义是否截断了完整信息。 也就是说，使用前缀索引就用不上覆盖索引对查询性能的优化了，这也是在选择是否使用前缀索引时需要考虑的一个因素。 ","date":"2025-02-12","objectID":"/posts/11.%E6%80%8E%E4%B9%88%E7%BB%99%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%AD%97%E6%AE%B5%E5%8A%A0%E7%B4%A2%E5%BC%95/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"11 | 怎么给字符串字段加索引？","uri":"/posts/11.%E6%80%8E%E4%B9%88%E7%BB%99%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%AD%97%E6%AE%B5%E5%8A%A0%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 其他方式 对于类似于邮箱这样的字段来说，使用前缀索引的效果可能还不错。但是，遇到前缀的区分度不够好的情况时，要怎么办呢？ 比如，我们国家的身份证号，一共 18 位，其中前 6 位是地址码，所以同一个县的人的身份证号前 6 位一般会是相同的。假设维护的数据库是一个市的公民信息系统，这时候如果对身份证号做长度为 6 的前缀索引的话，这个索引的区分度就非常低了。 按照前面说的方法，可能需要创建长度为 12 以上的前缀索引，才能够满足区分度要求。但是，索引选取的越长，占用的磁盘空间就越大，相同的数据页能放下的索引值就越少，搜索的效率也就会越低。 那么，如果能够确定业务需求里面只有按照身份证进行等值查询的需求，还有没有别的处理方法呢？这种方法，既可以占用更小的空间，也能达到相同的查询效率。答案是，有的。 第一种方式是使用倒序存储。如果存储身份证号的时候把它倒过来存，每次查询的时候，可以这么写： mysql\u003e select field_list from t where id_card = reverse('input_id_card_string'); 由于身份证号的最后 6 位没有地址码这样的重复逻辑，所以最后这 6 位很可能就提供了足够的区分度。当然了，实践中不要忘记使用 count(distinct) 方法去做个验证。 第二种方式是使用 hash 字段。可以在表上再创建一个整数字段，来保存身份证的校验码，同时在这个字段上创建索引。 mysql\u003e alter table t add id_card_crc int unsigned, add index(id_card_crc); 然后每次插入新记录的时候，都同时用 crc32() 这个函数得到校验码填到这个新字段。由于校验码可能存在冲突，也就是说两个不同的身份证号通过 crc32() 函数得到的结果可能是相同的，所以查询语句 where 部分要判断 id_card 的值是否精确相同。 mysql\u003e select field_list from t where id_card_crc=crc32('input_id_card_string') and id_card='input_id_card_string' 这样，索引的长度变成了 4 个字节，比原来小了很多。接下来，再一起看看使用倒序存储和使用 hash 字段这两种方法的异同点。 首先，它们的相同点是，都不支持范围查询。倒序存储的字段上创建的索引是按照倒序字符串的方式排序的，已经没有办法利用索引方式查出身份证号码在[ID_X, ID_Y]的所有市民了。同样地，hash 字段的方式也只能支持等值查询。 它们的区别，主要体现在以下三个方面： 从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而 hash 字段方法需要增加一个字段。当然，倒序存储方式使用 4 个字节的前缀长度应该是不够的，如果再长一点，这个消耗跟额外这个 hash 字段也差不多抵消了。 在 CPU 消耗方面，倒序方式每次写和读的时候，都需要额外调用一次 reverse 函数，而 hash 字段的方式需要额外调用一次 crc32() 函数。如果只从这两个函数的计算复杂度来看的话，reverse 函数额外消耗的 CPU 资源会更小些。 从查询效率上看，使用 hash 字段方式的查询性能相对更稳定一些。因为 crc32 算出来的值虽然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近 1。而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。 ","date":"2025-02-12","objectID":"/posts/11.%E6%80%8E%E4%B9%88%E7%BB%99%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%AD%97%E6%AE%B5%E5%8A%A0%E7%B4%A2%E5%BC%95/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"11 | 怎么给字符串字段加索引？","uri":"/posts/11.%E6%80%8E%E4%B9%88%E7%BB%99%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%AD%97%E6%AE%B5%E5%8A%A0%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 小结 字符串字段创建索引的场景可以使用的方式有： 直接创建完整索引，这样可能比较占用空间； 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引； 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题； 创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。 ","date":"2025-02-12","objectID":"/posts/11.%E6%80%8E%E4%B9%88%E7%BB%99%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%AD%97%E6%AE%B5%E5%8A%A0%E7%B4%A2%E5%BC%95/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"11 | 怎么给字符串字段加索引？","uri":"/posts/11.%E6%80%8E%E4%B9%88%E7%BB%99%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%AD%97%E6%AE%B5%E5%8A%A0%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 ## 问题 问：如果你在维护一个学校的学生信息数据库，学生登录名的统一格式是”学号 @gmail.com\", 而学号的规则是：十五位的数字，其中前三位是所在城市编号、第四到第六位是学校编号、第七位到第十位是入学年份、最后五位是顺序编号。 系统登录的时候都需要学生输入登录名和密码，验证正确后才能继续使用系统。就只考虑登录验证这个行为的话，应该怎么设计这个登录名的索引呢？ 答：由于这个学号的规则，无论是正向还是反向的前缀索引，重复度都比较高。因为维护的只是一个学校的，因此前面 6 位（其中，前三位是所在城市编号、第四到第六位是学校编号）其实是固定的，邮箱后缀都是 @gamil.com，因此可以只存入学年份加顺序编号，它们的长度是 9 位。 而其实在此基础上，可以用数字类型来存这 9 位数字。比如 201100001，这样只需要占 4 个字节。其实这个就是一种 hash，只是它用了最简单的转换规则：字符串转数字的规则，而刚好设定的这个背景，可以保证这个转换后结果的唯一性。 ","date":"2025-02-12","objectID":"/posts/11.%E6%80%8E%E4%B9%88%E7%BB%99%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%AD%97%E6%AE%B5%E5%8A%A0%E7%B4%A2%E5%BC%95/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"11 | 怎么给字符串字段加索引？","uri":"/posts/11.%E6%80%8E%E4%B9%88%E7%BB%99%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%AD%97%E6%AE%B5%E5%8A%A0%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文深入探讨了 MySQL 索引选择问题，通过一个案例展示了错误的索引选择可能导致查询性能下降的情况。","date":"2025-02-12","objectID":"/posts/10.mysql%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%97%B6%E5%80%99%E4%BC%9A%E9%80%89%E9%94%99%E7%B4%A2%E5%BC%95/","tags":["MySQL 实战 45 讲","MySQL"],"title":"10 | MySQL 为什么有时候会选错索引？","uri":"/posts/10.mysql%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%97%B6%E5%80%99%E4%BC%9A%E9%80%89%E9%94%99%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文深入探讨了 MySQL 索引选择问题，通过一个案例展示了错误的索引选择可能导致查询性能下降的情况。首先介绍了一个简单的表结构和插入数据的存储过程，然后展示了一条查询语句的执行情况。通过对表进行数据操作后再次执行相同的查询语句，发现 MySQL 选择了错误的索引，导致了性能下降。 在 MySQL 中一张表其实是可以支持多个索引的。但是写 SQL 语句的时候，并没有主动指定使用哪个索引。也就是说，使用哪个索引是由 MySQL 来确定的。你有没有碰到过这种情况，一条本来可以执行得很快的语句，却由于 MySQL 选错了索引，而导致执行速度变得很慢？ 举个例子，先建一个简单的表，表里有 a、b 两个字段，并分别建上索引： CREATE TABLE `t` ( `id` int(11) NOT NULL AUTO_INCREMENT, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `a` (`a`), KEY `b` (`b`) ) ENGINE=InnoDB; 然后，往表 t 中插入 10 万行记录，取值按整数递增，即：(1,1,1)，(2,2,2)，(3,3,3) 直到 (100000,100000,100000)。用存储过程来插入数据的。 delimiter ;; create procedure idata() begin declare i int; set i=1; while(i\u003c=100000)do insert into t values(i, i, i); set i=i+1; end while; end;; delimiter ; call idata(); 接下来，我们分析一条 SQL 语句： mysql\u003e select * from t where a between 10000 and 20000; 你一定会说，这个语句还用分析吗，很简单呀，a 上有索引，肯定是要使用索引 a 的。下图显示的就是使用 explain 命令看到的这条语句的执行情况。 从上图看上去，这条查询语句的执行也确实符合预期，key 这个字段值是’a’，表示优化器选择了索引 a。不过别急，这个案例不会这么简单。在已经准备好的包含了 10 万行数据的表上，再做如下操作。 这里，session A 的操作就是开启了一个事务。随后，session B 把数据都删除后，又调用了 idata 这个存储过程，插入了 10 万行数据。 这时候，session B 的查询语句 select * from t where a between 10000 and 20000 就不会再选择索引 a 了。可以通过慢查询日志（slow log）来查看一下具体的执行情况。 为了说明优化器选择的结果是否正确，增加了一个对照，即：使用 force index(a) 来让优化器强制使用索引 a。下面的三条 SQL 语句，就是这个实验过程。 set long_query_time=0; select * from t where a between 10000 and 20000; /*Q1*/ select * from t force index(a) where a between 10000 and 20000;/*Q2*/ 第一句，是将慢查询日志的阈值设置为 0，表示这个线程接下来的语句都会被记录入慢查询日志中； 第二句，Q1 是 session B 原来的查询； 第三句，Q2 是加了 force index(a) 来和 session B 原来的查询语句执行情况对比。 如下图所示是这三条 SQL 语句执行完成后的慢查询日志。 可以看到，Q1 扫描了 10 万行，显然是走了全表扫描，执行时间是 40 毫秒。Q2 扫描了 10001 行，执行了 21 毫秒。也就是说，在没有使用 force index 的时候，MySQL 用错了索引，导致了更长的执行时间。这个例子对应的是平常不断地删除历史数据和新增数据的场景。这时，MySQL 就会选错索引。 ","date":"2025-02-12","objectID":"/posts/10.mysql%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%97%B6%E5%80%99%E4%BC%9A%E9%80%89%E9%94%99%E7%B4%A2%E5%BC%95/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"10 | MySQL 为什么有时候会选错索引？","uri":"/posts/10.mysql%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%97%B6%E5%80%99%E4%BC%9A%E9%80%89%E9%94%99%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 优化器的逻辑 选择索引是优化器的工作。而优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。 当然，扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。这个简单的查询语句并没有涉及到临时表和排序，所以 MySQL 选错索引肯定是在判断扫描行数的时候出问题了。那么，问题就是：扫描行数是怎么判断的？ MySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。 这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好。 可以使用 show index 方法，看到一个索引的基数。如下图所示，就是表 t 的 show index 的结果。虽然这个表的每一行的三个字段值都是一样的，但是在统计信息中，这三个索引的基数值并不同，而且其实都不准确。 那么，MySQL 是怎样得到索引的基数的呢？这里简单介绍一下 MySQL 采样统计的方法。为什么要采样统计呢？因为把整张表取出来一行行统计，虽然可以得到精确的结果，但是代价太高了，所以只能选择“采样统计”。 采样统计的时候，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。 而数据表是会持续更新的，索引统计信息也不会固定不变。所以，当变更的数据行数超过 1/M 的时候，会自动触发重新做一次索引统计。 在 MySQL 中，有两种存储索引统计的方式，可以通过设置参数 innodb_stats_persistent 的值来选择： 设置为 on 的时候，表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10。 设置为 off 的时候，表示统计信息只存储在内存中。这时，默认的 N 是 8，M 是 16。 由于是采样统计，所以不管 N 是 20 还是 8，这个基数都是很容易不准的。但，这还不是全部。可以从上图中看到，这次的索引统计值（cardinality 列）虽然不够精确，但大体上还是差不多的，选错索引一定还有别的原因。 其实索引统计只是一个输入，对于一个具体的语句来说，优化器还要判断，执行这个语句本身要扫描多少行。接下来，再看看优化器预估的，这两个语句的扫描行数是多少。 rows 这个字段表示的是预计扫描行数。 其中，Q1 的结果还是符合预期的，rows 的值是 104620；但是 Q2 的 rows 值是 37116，偏差就大了。而图 1 中我们用 explain 命令看到的 rows 是只有 10001 行，是这个偏差误导了优化器的判断。 到这里，可能你的第一个疑问不是为什么不准，而是优化器为什么放着扫描 37000 行的执行计划不用，却选择了扫描行数是 100000 的执行计划呢？ 这是因为，如果使用索引 a，每次从索引 a 上拿到一个值，都要回到主键索引上查出整行数据，这个代价优化器也要算进去的。而如果选择扫描 10 万行，是直接在主键索引上扫描的，没有额外的代价。 优化器会估算这两个选择的代价，从结果看来，优化器认为直接扫描主键索引更快。当然，从执行时间看来，这个选择并不是最优的。 使用普通索引需要把回表的代价算进去，在第一幅图中执行 explain 的时候，也考虑了这个策略的代价，但第一幅图中的选择是对的。也就是说，这个策略并没有问题。 所以冤有头债有主，MySQL 选错索引，这件事儿还得归咎到没能准确地判断出扫描行数。至于为什么会得到错误的扫描行数。既然是统计信息不对，那就修正。analyze table t 命令，可以用来重新统计索引信息。 这回对了。所以在实践中，如果发现 explain 的结果预估的 rows 值跟实际情况差距比较大，可以采用这个方法来处理。其实，如果只是索引统计不准确，通过 analyze 命令可以解决很多问题，但是优化器可不止是看扫描行数。依然是基于这个表 t，看看另外一个语句： mysql\u003e select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1; 从条件上看，这个查询没有符合条件的记录，因此会返回空集合。在开始执行这条语句之前，可以先设想一下，如果你来选择索引，会选择哪一个呢？为了便于分析，先来看一下 a、b 这两个索引的结构图。 如果使用索引 a 进行查询，那么就是扫描索引 a 的前 1000 个值，然后取到对应的 id，再到主键索引上去查出每一行，然后根据字段 b 来过滤。显然这样需要扫描 1000 行。 如果使用索引 b 进行查询，那么就是扫描索引 b 的最后 50001 个值，与上面的执行过程相同，也是需要回到主键索引上取值再判断，所以需要扫描 50001 行。 所以你一定会想，如果使用索引 a 的话，执行速度明显会快很多。那么，下面就来看看到底是不是这么一回事儿。下图是执行 explain 的结果。 mysql\u003e explain select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1; 可以看到，返回结果中 key 字段显示，这次优化器选择了索引 b，而 rows 字段显示需要扫描的行数是 50198。从这个结果中，可以得到两个结论： 扫描行数的估计值依然不准确； 这个例子里 MySQL 又选错了索引。 ","date":"2025-02-12","objectID":"/posts/10.mysql%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%97%B6%E5%80%99%E4%BC%9A%E9%80%89%E9%94%99%E7%B4%A2%E5%BC%95/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"10 | MySQL 为什么有时候会选错索引？","uri":"/posts/10.mysql%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%97%B6%E5%80%99%E4%BC%9A%E9%80%89%E9%94%99%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 索引选择异常和处理 其实大多数时候优化器都能找到正确的索引，但偶尔还是会碰到上面举例的这两种情况：原本可以执行得很快的 SQL 语句，执行速度却比预期的慢很多，应该怎么办呢？ 一种方法是，像我们第一个例子一样，采用 force index 强行选择一个索引。MySQL 会根据词法解析的结果分析出可能可以使用的索引作为候选项，然后在候选列表中依次判断每个索引需要扫描多少行。如果 force index 指定的索引在候选索引列表中，就直接选择这个索引，不再评估其他索引的执行代价。 再来看看第二个例子。刚开始分析时，我们认为选择索引 a 会更好。现在，我们就来看看执行效果： 可以看到，原本语句需要执行 2.23 秒，而当使用 force index(a) 的时候，只用了 0.05 秒，比优化器的选择快了 40 多倍。也就是说，优化器没有选择正确的索引，force index 起到了“矫正”的作用。 不过很多程序员不喜欢使用 force index，一来这么写不优美，二来如果索引改了名字，这个语句也得改，显得很麻烦。而且如果以后迁移到别的数据库的话，这个语法还可能会不兼容。 但其实使用 force index 最主要的问题还是变更的及时性。因为选错索引的情况还是比较少出现的，所以开发的时候通常不会先写上 force index。而是等到线上出现问题的时候，才会再去修改 SQL 语句、加上 force index。但是修改之后还要测试和发布，对于生产系统来说，这个过程不够敏捷。所以，数据库的问题最好还是在数据库内部来解决。那么，在数据库里面该怎样解决呢？ 既然优化器放弃了使用索引 a，说明 a 还不够合适，所以第二种方法就是，可以考虑修改语句，引导 MySQL 使用我们期望的索引。比如，在这个例子里，显然把“order by b limit 1”改成“order by b,a limit 1” ，语义的逻辑是相同的。再来看看改之后的效果： 之前优化器选择使用索引 b，是因为它认为使用索引 b 可以避免排序（b 本身是索引，已经是有序的了，如果选择索引 b 的话，不需要再做排序，只需要遍历），所以即使扫描行数多，也判定为代价更小。 现在 order by b,a 这种写法，要求按照 b,a 排序，就意味着使用这两个索引都需要排序。因此，扫描行数成了影响决策的主要条件，于是此时优化器选了只需要扫描 1000 行的索引 a。 当然，这种修改并不是通用的优化手段，只是刚好在这个语句里面有 limit 1，因此如果有满足条件的记录，order by b limit 1 和 order by b,a limit 1 都会返回 b 是最小的那一行，逻辑上一致，才可以这么做。如果你觉得修改语义这件事儿不太好，这里还有一种改法，下图是执行效果。 mysql\u003e select * from (select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 100)alias limit 1; 在这个例子里，用 limit 100 让优化器意识到，使用 b 索引代价是很高的。其实是根据数据特征诱导了一下优化器，也不具备通用性。 第三种方法是，在有些场景下，可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。不过，在这个例子中，没有找到通过新增索引来改变优化器行为的方法。这种情况其实比较少，尤其是经过 DBA 索引优化过的库，再碰到这个 bug，找到一个更合适的索引一般比较难。 如果说还有一个方法是删掉索引 b，你可能会觉得好笑。但实际上有时候真的会发现优化器错误选择的索引其实根本没有必要存在，于是就删掉了这个索引，优化器也就重新选择到了正确的索引。 ","date":"2025-02-12","objectID":"/posts/10.mysql%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%97%B6%E5%80%99%E4%BC%9A%E9%80%89%E9%94%99%E7%B4%A2%E5%BC%95/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"10 | MySQL 为什么有时候会选错索引？","uri":"/posts/10.mysql%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%97%B6%E5%80%99%E4%BC%9A%E9%80%89%E9%94%99%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 小结 对于由于索引统计信息不准确导致的问题，可以用 analyze table 来解决。 而对于其他优化器误判的情况，可以在应用端用 force index 来强行指定索引，也可以通过修改语句来引导优化器，还可以通过增加或者删除索引来绕过这个问题。 ","date":"2025-02-12","objectID":"/posts/10.mysql%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%97%B6%E5%80%99%E4%BC%9A%E9%80%89%E9%94%99%E7%B4%A2%E5%BC%95/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"10 | MySQL 为什么有时候会选错索引？","uri":"/posts/10.mysql%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%97%B6%E5%80%99%E4%BC%9A%E9%80%89%E9%94%99%E7%B4%A2%E5%BC%95/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 问题 问：在构造第一个例子的过程中，通过 session A 的配合，让 session B 删除数据后又重新插入了一遍数据，然后就发现 explain 结果中，rows 字段从 10001 变成 37000 多。 而如果没有 session A 的配合，只是单独执行 delete from t、call idata()、explain 这三句话，会看到 rows 字段其实还是 10000 左右。这是什么原因呢？ 答：delete 语句删掉了所有的数据，然后再通过 call idata() 插入了 10 万行数据，看上去是覆盖了原来的 10 万行。 但是，session A 开启了事务并没有提交，所以之前插入的 10 万行数据是不能删除的。这样，之前的数据每一行数据都有两个版本，旧版本是 delete 之前的数据，新版本是标记为 deleted 的数据。这样，索引 a 上的数据其实就有两份。 可能你会觉得主键上的数据也不能删，那没有使用 force index 的语句，使用 explain 命令看到的扫描行数为什么还是 100000 左右？是的，不过这个是主键，主键是直接按照表的行数来估计的。而表的行数，优化器直接用的是 show table status 的值。 ","date":"2025-02-12","objectID":"/posts/10.mysql%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%97%B6%E5%80%99%E4%BC%9A%E9%80%89%E9%94%99%E7%B4%A2%E5%BC%95/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"10 | MySQL 为什么有时候会选错索引？","uri":"/posts/10.mysql%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%97%B6%E5%80%99%E4%BC%9A%E9%80%89%E9%94%99%E7%B4%A2%E5%BC%95/"},{"categories":["Golang"],"content":"了解下 Go 语言中如何访问私有成员。","date":"2025-02-12","objectID":"/posts/1.go%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AE%E7%A7%81%E6%9C%89%E6%88%90%E5%91%98/","tags":["Golang"],"title":"1.Go 语言中如何访问私有成员？","uri":"/posts/1.go%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AE%E7%A7%81%E6%9C%89%E6%88%90%E5%91%98/"},{"categories":["Golang"],"content":" 摘要 本文主要探讨了 Go 语言中如何访问私有成员？ ","date":"2025-02-12","objectID":"/posts/1.go%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AE%E7%A7%81%E6%9C%89%E6%88%90%E5%91%98/:0:0","tags":["Golang"],"title":"1.Go 语言中如何访问私有成员？","uri":"/posts/1.go%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AE%E7%A7%81%E6%9C%89%E6%88%90%E5%91%98/"},{"categories":["Golang"],"content":"1 答案 在 Go 语言中，以小写字母开头的标识符是私有成员，私有成员（字段、方法、函数等）遵循语言的可见性规则，仅在定义它的包内可见，包外无法访问这些私有成员。如果想要访问私有成员，主要包括以下三种方式： 在同一个包内，可以直接访问小写字母开头的私有成员。 在其他包中，无法直接访问私有成员，但可以通过公开的接口来间接访问私有成员。 使用反射来绕过 Go 语言的封装机制访问和修改私有字段。（不建议使用） ","date":"2025-02-12","objectID":"/posts/1.go%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AE%E7%A7%81%E6%9C%89%E6%88%90%E5%91%98/:1:0","tags":["Golang"],"title":"1.Go 语言中如何访问私有成员？","uri":"/posts/1.go%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AE%E7%A7%81%E6%9C%89%E6%88%90%E5%91%98/"},{"categories":["Golang"],"content":"2 扩展知识 ","date":"2025-02-12","objectID":"/posts/1.go%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AE%E7%A7%81%E6%9C%89%E6%88%90%E5%91%98/:2:0","tags":["Golang"],"title":"1.Go 语言中如何访问私有成员？","uri":"/posts/1.go%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AE%E7%A7%81%E6%9C%89%E6%88%90%E5%91%98/"},{"categories":["Golang"],"content":"2.1 访问私有成员的规则 可见性规则： 私有成员：以小写字母开头的标识符是私有的，仅在定义它的包内可见。包外无法访问这些私有成员。 公开成员：以大写字母开头的标识符是公开的，可以在任何包中访问。 示例代码 1）私有成员的访问（包内） package example // 结构体定义，字段 age 是私有的 type Person struct { name string age int } // 包内函数，能够访问私有字段 func NewPerson(name string, age int) Person { return Person{name: name, age: age} } func GetPersonAge(p Person) int { return p.age } 2）通过公开方法访问私有成员（包外） package main import ( \"fmt\" \"example\" // 假设 example 是定义 Person 的包 ) func main() { p := example.NewPerson(\"John\", 30) // 不能直接访问 p.age，因为 age 是私有的 // fmt.Println(p.age) // 编译错误 // 可以通过包内公开的函数访问私有成员 age := example.GetPersonAge(p) fmt.Println(\"Age:\", age) // 输出: Age: 30 } 3）通过反射访问私有成员 在 Go 语言中，可以使用反射（reflect 包）来访问和修改私有字段。虽然直接访问私有字段违背了封装原则，但反射提供了这种能力。 package main import ( \"fmt\" \"reflect\" ) type Person struct { name string age int } func main() { p := Person{name: \"John\", age: 30} // 获取指向 p 的指针的反射值，Elem 方法用于获取指针指向的值。 v := reflect.ValueOf(\u0026p).Elem() // 获取私有字段 name nameField := v.FieldByName(\"name\") fmt.Println(\"name (private):\", nameField.String()) } 或 package main import ( \"fmt\" \"reflect\" \"unsafe\" ) type Person struct { name string age int } func main() { p := Person{name: \"John\", age: 30} // 获取指向 p 的指针的反射值，Elem 方法用于获取指针指向的值。 value := reflect.ValueOf(\u0026p).Elem() // 通过 FieldByName 方法获取私有字段的值 field := value.FieldByName(\"name\") // 使用 unsafe.Pointer 和反射来操作私有字段 realField := reflect.NewAt(field.Type(), unsafe.Pointer(field.UnsafeAddr())).Elem() // 输出私有字段的值 fmt.Println(\"name (private):\", realField.String()) } 注意点： 安全性：虽然可以通过反射访问和修改私有字段，但这种做法可能导致程序设计上的问题，破坏了封装性。因此，应谨慎使用，并尽量避免在生产代码中使用这种技术，除非确实有必要。 性能：反射操作通常比直接访问字段要慢，因此在性能敏感的代码中应避免频繁使用反射。 ","date":"2025-02-12","objectID":"/posts/1.go%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AE%E7%A7%81%E6%9C%89%E6%88%90%E5%91%98/:2:1","tags":["Golang"],"title":"1.Go 语言中如何访问私有成员？","uri":"/posts/1.go%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AE%E7%A7%81%E6%9C%89%E6%88%90%E5%91%98/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文深入探讨了普通索引和唯一索引在不同业务场景下的选择，重点从性能角度对比了它们在查询和更新语句中的影响。","date":"2025-02-11","objectID":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/","tags":["MySQL 实战 45 讲","MySQL"],"title":"09 | 普通索引和唯一索引，应该怎么选择？","uri":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文深入探讨了普通索引和唯一索引在不同业务场景下的选择，重点从性能角度对比了它们在查询和更新语句中的影响。在查询过程中，普通索引需要额外的查找和判断操作，但由于 InnoDB 的数据是按数据页为单位读写，性能差距微乎其微。而在更新过程中，普通索引可以利用 change buffer 来减少磁盘读取，从而提升性能。 在不同的业务场景下，应该选择普通索引，还是唯一索引？假设你在维护一个市民系统，每个人都有一个唯一的身份证号，而且业务代码已经保证了不会写入两个重复的身份证号。如果市民系统需要按照身份证号查姓名，就会执行类似这样的 SQL 语句： select name from CUser where id_card = 'xxxxxxxyyyyyyzzzzz'; 所以，你一定会考虑在 id_card 字段上建索引。由于身份证号字段比较大，不建议你把身份证号当做主键，那么现在有两个选择，要么给 id_card 字段创建唯一索引，要么创建一个普通索引。如果业务代码已经保证了不会写入重复的身份证号，那么这两个选择逻辑上都是正确的。那么，从性能的角度考虑，选择唯一索引还是普通索引呢？选择的依据是什么呢？ 接下来，就从这两种索引对查询语句和更新语句的性能影响来进行分析。 ","date":"2025-02-11","objectID":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"09 | 普通索引和唯一索引，应该怎么选择？","uri":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 查询过程 假设，执行查询的语句是 select id from T where k=5。这个查询语句在索引树上查找的过程，先是通过 B+ 树从树根开始，按层搜索到叶子节点，也就是图中右下角的这个数据页，然后可以认为数据页内部通过二分法来定位记录。 对于普通索引来说，查找到满足条件的第一个记录 (5,500) 后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录。 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。 那么，这个不同带来的性能差距会有多少呢？答案是，微乎其微。 InnoDB 的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB。 因为引擎是按页读写的，所以说，当找到 k=5 的记录的时候，它所在的数据页就都在内存里了。那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。 当然，如果 k=5 这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些。 但是之前计算过，对于整型字段，一个数据页可以放近千个 key，因此出现这种情况的概率会很低。所以，计算平均性能差异时，仍可以认为这个操作成本对于现在的 CPU 来说可以忽略不计。 ","date":"2025-02-11","objectID":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"09 | 普通索引和唯一索引，应该怎么选择？","uri":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 更新过程 为了说明普通索引和唯一索引对更新语句性能的影响这个问题，需要你介绍一下 change buffer。 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。 需要说明的是，虽然名字叫作 change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。 将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。 显然，如果能够将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。那么，什么条件下可以使用 change buffer 呢？ 对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400) 这个记录，就要先判断现在表中是否已经存在 k=4 的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。 因此，唯一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用。 change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。 现在我们再一起来看看如果要在这张表中插入一个新记录 (4,400) 的话，InnoDB 的处理流程是怎样的。 第一种情况是，这个记录要更新的目标页在内存中。这时，InnoDB 的处理流程如下： 对于唯一索引来说，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束； 对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束。 这样看来，普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的 CPU 时间。 第二种情况是，这个记录要更新的目标页不在内存中。这时，InnoDB 的处理流程如下： 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束； 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。 将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。 ","date":"2025-02-11","objectID":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"09 | 普通索引和唯一索引，应该怎么选择？","uri":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 change buffer 的使用场景 通过上面的分析，你已经清楚了使用 change buffer 对更新过程的加速作用，也清楚了 change buffer 只限于用在普通索引的场景下，而不适用于唯一索引。那么，现在有一个问题就是：普通索引的所有场景，使用 change buffer 都可以起到加速作用吗？ 因为 merge 的时候是真正进行数据更新的时刻，而 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。 因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。 反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。所以，对于这种业务模式来说，change buffer 反而起到了副作用。 ","date":"2025-02-11","objectID":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"09 | 普通索引和唯一索引，应该怎么选择？","uri":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 索引选择和实践 普通索引和唯一索引应该怎么选择。其实，这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。所以，建议你尽量选择普通索引。如果所有的更新后面，都马上伴随着对这个记录的查询，那你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能。 在实际使用中，你会发现，普通索引和 change buffer 的配合使用，对于数据量大的表的更新优化还是很明显的。 特别地，在使用机械硬盘时，change buffer 这个机制的收效是非常显著的。所以，当有一个类似“历史数据”的库，并且出于成本考虑用的是机械硬盘时，应该特别关注这些表里的索引，尽量使用普通索引，然后把 change buffer 尽量开大，以确保这个“历史数据”表的数据写入速度。 ","date":"2025-02-11","objectID":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"09 | 普通索引和唯一索引，应该怎么选择？","uri":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 change buffer 和 redo log 理解了 change buffer 的原理，可能会联想到前面文章中介绍过的 redo log 和 WAL。WAL 提升性能的核心机制，也的确是尽量减少随机读写，这两个概念确实容易混淆。所以，这里把它们放到了同一个流程里来说明，便于区分这两个概念。现在，要在表上执行这个插入语句： mysql\u003e insert into t(id,k) values(id1,k1),(id2,k2); 这里，假设当前 k 索引树的状态，查找到位置后，k1 所在的数据页在内存 (InnoDB buffer pool) 中，k2 所在的数据页不在内存中。如下图所示是带 change buffer 的更新状态图。 分析这条更新语句，你会发现它涉及了四个部分：内存、redo log（ib_log_fileX）、数据表空间（t.ibd）、系统表空间（ibdata1）。这条更新语句做了如下的操作（按照图中的数字顺序）： Page 1 在内存中，直接更新内存； Page 2 没有在内存中，就在内存的 change buffer 区域，记录下“我要往 Page 2 插入一行”这个信息 将上述两个动作记入 redo log 中（图中 3 和 4）。 做完上面这些，事务就可以完成了。所以，你会看到，执行这条更新语句的成本很低，就是写了两处内存，然后写了一处磁盘（两次操作合在一起写了一次磁盘），而且还是顺序写的。同时，图中的两个虚线箭头，是后台操作，不影响更新的响应时间。那在这之后的读请求，要怎么处理呢？ 比如，现在要执行 select * from t where k in (k1, k2)。这里，画了这两个读请求的流程图。如果读语句发生在更新语句后不久，内存中的数据都还在，那么此时的这两个读操作就与系统表空间（ibdata1）和 redo log（ib_log_fileX）无关了。所以，图中就没画出这两部分。 从图中可以看到： 读 Page 1 的时候，直接从内存返回。WAL 之后如果读数据，是不是一定要读盘，是不是一定要从 redo log 里面把数据更新以后才可以返回？其实是不用的。可以看一下图 3 的这个状态，虽然磁盘上还是之前的数据，但是这里直接从内存返回结果，结果是正确的。 要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果。 可以看到，直到需要读 Page 2 的时候，这个数据页才会被读入内存。 所以，如果要简单地对比这两个机制在提升更新性能上的收益的话，redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。 ","date":"2025-02-11","objectID":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"09 | 普通索引和唯一索引，应该怎么选择？","uri":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"6 小结 由于唯一索引用不上 change buffer 的优化机制，因此如果业务可以接受，从性能角度出发建议你优先考虑非唯一索引。 ","date":"2025-02-11","objectID":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/:6:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"09 | 普通索引和唯一索引，应该怎么选择？","uri":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"7 问题 问：在第二幅图中可以看到，change buffer 一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致 change buffer 丢失呢？change buffer 丢失可不是小事儿，再从磁盘读入数据可就没有了 merge 过程，就等于是数据丢失了。会不会出现这种情况呢？ 答：不会丢失，虽然是只更新内存，但是在事务提交的时候，把 change buffer 的操作也记录到 redo log 里了，所以崩溃恢复的时候，change buffer 也能找回来。 ","date":"2025-02-11","objectID":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/:7:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"09 | 普通索引和唯一索引，应该怎么选择？","uri":"/posts/9.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文深入探讨了事务隔离级别对于事务可见性的影响，并重点介绍了可重复读隔离级别下的事务视图和行锁的概念。","date":"2025-02-11","objectID":"/posts/8.%E4%BA%8B%E5%8A%A1%E5%88%B0%E5%BA%95%E6%98%AF%E9%9A%94%E7%A6%BB%E7%9A%84%E8%BF%98%E6%98%AF%E4%B8%8D%E9%9A%94%E7%A6%BB%E7%9A%84/","tags":["MySQL 实战 45 讲","MySQL"],"title":"08 | 事务到底是隔离的还是不隔离的？","uri":"/posts/8.%E4%BA%8B%E5%8A%A1%E5%88%B0%E5%BA%95%E6%98%AF%E9%9A%94%E7%A6%BB%E7%9A%84%E8%BF%98%E6%98%AF%E4%B8%8D%E9%9A%94%E7%A6%BB%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文深入探讨了事务隔离级别对于事务可见性的影响，并重点介绍了可重复读隔离级别下的事务视图和行锁的概念。文章详细解释了在 MySQL 中 MVCC 实现时使用的一致性读视图的概念，以及 InnoDB 如何利用多版本数据实现“秒级创建快照”的能力。 如果是可重复读隔离级别，事务 T 启动的时候会创建一个视图 read-view，之后事务 T 执行期间，即使有其他事务修改了数据，事务 T 看到的仍然跟在启动时看到的一样。也就是说，一个在可重复读隔离级别下执行的事务，好像与世无争，不受外界影响。 但是一个事务要更新一行，如果刚好有另外一个事务拥有这一行的行锁，它又不能这么超然了，会被锁住，进入等待状态。问题是，既然进入了等待状态，那么等到这个事务自己获取到行锁要更新数据的时候，它读到的值又是什么呢？ 举一个例子，下面是一个只有两行的表的初始化语句。 mysql\u003e CREATE TABLE `t` ( `id` int(11) NOT NULL, `k` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; insert into t(id, k) values(1,1),(2,2); 这里，需要注意的是事务的启动时机。begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。如果想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。 第一种启动方式，一致性视图是在执行第一个快照读语句时创建的； 第二种启动方式，一致性视图是在执行 start transaction with consistent snapshot 时创建的。 例子中如果没有特别说明，都是默认 autocommit=1。在这个例子中，事务 C 没有显式地使用 begin/commit，表示这个 update 语句本身就是一个事务，语句完成的时候会自动提交。事务 B 在更新了行之后查询 ; 事务 A 在一个只读事务中查询，并且时间顺序上是在事务 B 的查询之后。此时事务 B 查到的 k 的值是 3，而事务 A 查到的 k 的值是 1。 在 MySQL 里，有两个“视图”的概念： 一个是 view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。它没有物理结构，作用是事务执行期间用来定义“能看到什么数据”。 ","date":"2025-02-11","objectID":"/posts/8.%E4%BA%8B%E5%8A%A1%E5%88%B0%E5%BA%95%E6%98%AF%E9%9A%94%E7%A6%BB%E7%9A%84%E8%BF%98%E6%98%AF%E4%B8%8D%E9%9A%94%E7%A6%BB%E7%9A%84/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"08 | 事务到底是隔离的还是不隔离的？","uri":"/posts/8.%E4%BA%8B%E5%8A%A1%E5%88%B0%E5%BA%95%E6%98%AF%E9%9A%94%E7%A6%BB%E7%9A%84%E8%BF%98%E6%98%AF%E4%B8%8D%E9%9A%94%E7%A6%BB%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 “快照”在 MVCC 里是怎么工作的？ 在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。这时，你会说这看上去不太现实啊。如果一个库有 100G，那么启动一个事务，MySQL 就要拷贝 100G 的数据出来，这个过程得多慢啊。可是，平时的事务执行起来很快啊。实际上，并不需要拷贝出这 100G 的数据。先来看看这个快照是怎么实现的。 InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。 而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。 也就是说，数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 row trx_id。如下图所示，就是一个记录被多个事务连续更新后的状态。 图中虚线框里是同一行数据的 4 个版本，当前最新版本是 V4，k 的值是 22，它是被 transaction id 为 25 的事务更新的，因此它的 row trx_id 也是 25。 语句更新会生成 undo log（回滚日志），那么，undo log 在哪呢？实际上，图 中的三个虚线箭头，就是 undo log；而 V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。比如，需要 V2 的时候，就是通过 V4 依次执行 U3、U2 算出来。 明白了多版本和 row trx_id 的概念后，再来想一下，InnoDB 是怎么定义那个“100G”的快照的。按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见。 因此，一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。 当然，如果“上一个版本”也不可见，那就得继续往前找。还有，如果是这个事务自己更新的数据，它自己还是要认的。 在实现上，InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。 这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。而数据版本的可见性规则，就是基于数据的 row trx_id 和这个一致性视图的对比结果得到的。这个视图数组把所有的 row trx_id 分成了几种不同的情况。 这样，对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能： 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的； 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的； 如果落在黄色部分，那就包括两种情况 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见； 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。 举例说明：黄色区间是包含了已经提交的事务的，已经提交的事务的 id 可以比最低水位大，但小于最高水位。比如，有 5，6，7，8，9 这 5 个事务，9 是当前事务，5，6，8 是正在执行中的事务，7 是已经提交的事务 id。那么当前视图数组是[5 6 8 9]，最低水位是 5，最高水位是 10。事务 7 落在落在黄色区间，不在数组中，但已提交，所以也可见。 比如对于上图中的数据来说，如果有一个事务，它的低水位是 18，那么当它访问这一行数据时，就会从 V4 通过 U3 计算出 V3，所以在它看来，这一行的值是 11。 有了这个声明后，系统里面随后发生的更新，是不是就跟这个事务看到的内容无关了呢？因为之后的更新，生成的版本一定属于上面的 2 或者 3(a) 的情况，而对它来说，这些新的数据版本是不存在的，所以这个事务的快照，就是“静态”的了。 所以你现在知道了，InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。接下来，继续看一下第一幅图中的三个事务，分析下事务 A 的语句返回的结果，为什么是 k=1。这里，我们做如下假设： 事务 A 开始前，系统里面只有一个活跃事务 ID 是 99； 事务 A、B、C 的版本号分别是 100、101、102，且当前系统里只有这四个事务； 三个事务开始前，(1,1）这一行数据的 row trx_id 是 90。 这样，事务 A 的视图数组就是[99,100], 事务 B 的视图数组是[99,100,101], 事务 C 的视图数组是[99,100,101,102]。 为了简化分析，先把其他干扰语句去掉，只画出跟事务 A 查询逻辑有关的操作： 从图中可以看到，第一个有效更新是事务 C，把数据从 (1,1) 改成了 (1,2)。这时候，这个数据的最新版本的 row trx_id 是 102，而 90 这个版本已经成为了历史版本。 第二个有效更新是事务 B，把数据从 (1,2) 改成了 (1,3)。这时候，这个数据的最新版本（即 row trx_id）是 101，而 102 又成为了历史版本。 在事务 A 查询的时候，其实事务 B 还没有提交，但是它生成的 (1,3) 这个版本已经变成当前版本了。但这个版本对事务 A 必须是不可见的，否则就变成脏读了。 现在事务 A 要来读数据了，它的视图数组是[99,100]。当然了，读数据都是从当前版本读起的。所以，事务 A 查询语句的读数据流程是这样的： 找到 (1,3) 的时候，判断出 row trx_id=101，比高水位大，处于红色区域，不可见； 接着，找到上一个历史版本，一看 row trx_id=102，比高水位大，处于红色区域，不可见； 再往前找，终于找到了（1,1)，它的 row trx_id=90，比低水位小，处于绿色区域，可见。 这样执行下来，虽然期间这一行数据被修改过，但是事务 A 不论在什么时候查询，看到这行数据的结果都是一致的，所以称之为一致性读。这个判断规则是从代码逻辑直接转译过来的，但是正如你所见，用于人肉分析可见性很麻烦。 所以需要翻译一下。一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况： 版本未提交，不可见； 版本已提交，但是是在视图创建后提交的，不可见； 版本已提交，而且是在视图创建前提交的，可见。 用这个规则来判断上图中的查询结果，事务 A 的查询语句的视图数组是在事务 A 启动的时候生成的，这时候： (1,3) 还没提交，属于情况 1，不可见； (1,2) 虽然提交了，但是是在视图数组创建之后提交的，属于情况 2，不可见； (1,1) 是在视图数组创建之前提交的，可见。 ","date":"2025-02-11","objectID":"/posts/8.%E4%BA%8B%E5%8A%A1%E5%88%B0%E5%BA%95%E6%98%AF%E9%9A%94%E7%A6%BB%E7%9A%84%E8%BF%98%E6%98%AF%E4%B8%8D%E9%9A%94%E7%A6%BB%E7%9A%84/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"08 | 事务到底是隔离的还是不隔离的？","uri":"/posts/8.%E4%BA%8B%E5%8A%A1%E5%88%B0%E5%BA%95%E6%98%AF%E9%9A%94%E7%A6%BB%E7%9A%84%E8%BF%98%E6%98%AF%E4%B8%8D%E9%9A%94%E7%A6%BB%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 更新逻辑 但是，事务 B 的 update 语句，如果按照一致性读，好像结果不对哦？下图中，事务 B 的视图数组是先生成的，之后事务 C 才提交，不是应该看不见 (1,2) 吗，怎么能算出 (1,3) 来？ 是的，如果事务 B 在更新之前查询一次数据，这个查询返回的 k 的值确实是 1。但是，当它要去更新数据的时候，就不能再在历史版本上更新了，否则事务 C 的更新就丢失了。因此，事务 B 此时的 set k=k+1 是在（1,2）的基础上进行的操作。 所以，这里就用到了这样一条规则：更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。 因此，在更新的时候，当前读拿到的数据是 (1,2)，更新后生成了新版本的数据 (1,3)，这个新版本的 row trx_id 是 101。所以，在执行事务 B 查询语句的时候，一看自己的版本号是 101，最新数据的版本号也是 101，是自己的更新，可以直接使用，所以查询得到的 k 的值是 3。 其实，除了 update 语句外，select 语句如果加锁，也是当前读。所以，如果把事务 A 的查询语句 select * from t where id=1 修改一下，加上 lock in share mode 或 for update，也都可以读到版本号是 101 的数据，返回的 k 的值是 3。下面这两个 select 语句，就是分别加了读锁（S 锁，共享锁）和写锁（X 锁，排他锁）。 mysql\u003e select k from t where id=1 lock in share mode; mysql\u003e select k from t where id=1 for update; 再往前一步，假设事务 C 不是马上提交的，而是变成了下面的事务 C’，会怎么样呢？ 事务 C’的不同是，更新后并没有马上提交，在它提交前，事务 B 的更新语句先发起了。前面说过了，虽然事务 C’还没提交，但是 (1,2) 这个版本也已经生成了，并且是当前的最新版本。那么，事务 B 的更新语句会怎么处理呢？ 事务 C’没提交，也就是说 (1,2) 这个版本上的写锁还没释放。而事务 B 是当前读，必须要读最新版本，而且必须加锁，因此就被锁住了，必须等到事务 C’释放这个锁，才能继续它的当前读。 事务的可重复读的能力是怎么实现的？ 可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。 而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是： 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图； 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。 那么，在读提交隔离级别下，事务 A 和事务 B 的查询语句查到的 k，分别应该是多少呢？ 这里需要说明一下，“start transaction with consistent snapshot; ”的意思是从这个语句开始，创建一个持续整个事务的一致性快照。所以，在读提交隔离级别下，这个用法就没意义了，等效于普通的 start transaction。 下面是读提交时的状态图，可以看到这两个查询语句的创建视图数组的时机发生了变化，就是图中的 read view 框。（注意：这里，用的还是事务 C 的逻辑直接提交，而不是事务 C’） 这时，事务 A 的查询语句的视图数组是在执行这个语句的时候创建的，时序上 (1,2)、(1,3) 的生成时间都在创建这个视图数组的时刻之前。但是，在这个时刻： (1,3) 还没提交，属于情况 1，不可见； (1,2) 提交了，属于情况 3，可见。 所以，这时候事务 A 查询语句返回的是 k=2。显然地，事务 B 查询结果 k=3。 ","date":"2025-02-11","objectID":"/posts/8.%E4%BA%8B%E5%8A%A1%E5%88%B0%E5%BA%95%E6%98%AF%E9%9A%94%E7%A6%BB%E7%9A%84%E8%BF%98%E6%98%AF%E4%B8%8D%E9%9A%94%E7%A6%BB%E7%9A%84/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"08 | 事务到底是隔离的还是不隔离的？","uri":"/posts/8.%E4%BA%8B%E5%8A%A1%E5%88%B0%E5%BA%95%E6%98%AF%E9%9A%94%E7%A6%BB%E7%9A%84%E8%BF%98%E6%98%AF%E4%B8%8D%E9%9A%94%E7%A6%BB%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 小结 InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。 对于可重复读，查询只承认在事务启动前就已经提交完成的数据； 对于读提交，查询只承认在语句启动前就已经提交完成的数据； 而当前读，总是读取已经提交完成的最新版本。 为什么表结构不支持“可重复读”？这是因为表结构没有对应的行数据，也没有 row trx_id，因此只能遵循当前读的逻辑。当然，MySQL 8.0 已经可以把表结构放在 InnoDB 字典里了，也许以后会支持表结构的可重复读。 ","date":"2025-02-11","objectID":"/posts/8.%E4%BA%8B%E5%8A%A1%E5%88%B0%E5%BA%95%E6%98%AF%E9%9A%94%E7%A6%BB%E7%9A%84%E8%BF%98%E6%98%AF%E4%B8%8D%E9%9A%94%E7%A6%BB%E7%9A%84/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"08 | 事务到底是隔离的还是不隔离的？","uri":"/posts/8.%E4%BA%8B%E5%8A%A1%E5%88%B0%E5%BA%95%E6%98%AF%E9%9A%94%E7%A6%BB%E7%9A%84%E8%BF%98%E6%98%AF%E4%B8%8D%E9%9A%94%E7%A6%BB%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 问题 问：用下面的表结构和初始化语句作为试验环境，事务隔离级别是可重复读。现在，要把所有“字段 c 和 id 值相等的行”的 c 值清零，但是却发现了一个“诡异”的、改不掉的情况。请构造出这种情况，并说明其原理。 mysql\u003e CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; insert into t(id, c) values(1,1),(2,2),(3,3),(4,4); 复现出来以后，请再思考一下，在实际的业务开发中有没有可能碰到这种情况？应用代码会不会掉进这个“坑”里，又是怎么解决的呢？ 这样，session A 看到的就是上面截图的效果了。 这个操作序列跑出来，session A 看的内容也是能够复现截图的效果的。这个 session B’启动的事务比 A 要早。 用新的方式来分析 session B’的更新为什么对 session A 不可见就是：在 session A 视图数组创建的瞬间，session B’是活跃的，属于“版本未提交，不可见”这种情况。 ","date":"2025-02-11","objectID":"/posts/8.%E4%BA%8B%E5%8A%A1%E5%88%B0%E5%BA%95%E6%98%AF%E9%9A%94%E7%A6%BB%E7%9A%84%E8%BF%98%E6%98%AF%E4%B8%8D%E9%9A%94%E7%A6%BB%E7%9A%84/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"08 | 事务到底是隔离的还是不隔离的？","uri":"/posts/8.%E4%BA%8B%E5%8A%A1%E5%88%B0%E5%BA%95%E6%98%AF%E9%9A%94%E7%A6%BB%E7%9A%84%E8%BF%98%E6%98%AF%E4%B8%8D%E9%9A%94%E7%A6%BB%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文主要讨论了 MySQL 的行锁对数据库性能的影响以及如何通过合理的事务设计来减少锁冲突，提升并发度。","date":"2025-02-11","objectID":"/posts/7.%E8%A1%8C%E9%94%81%E5%8A%9F%E8%BF%87%E6%80%8E%E4%B9%88%E5%87%8F%E5%B0%91%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D/","tags":["MySQL 实战 45 讲","MySQL"],"title":"07 | 行锁功过：怎么减少行锁对性能的影响？","uri":"/posts/7.%E8%A1%8C%E9%94%81%E5%8A%9F%E8%BF%87%E6%80%8E%E4%B9%88%E5%87%8F%E5%B0%91%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文主要讨论了 MySQL 的行锁对数据库性能的影响以及如何通过合理的事务设计来减少锁冲突，提升并发度。文章首先介绍了行锁的概念和两阶段锁协议，强调了行锁在事务结束时才会释放的特点。通过一个电影票在线交易业务的例子，说明了如何合理安排事务中的操作顺序以减少锁等待，提升并发度。 MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。 顾名思义，行锁就是针对数据表中行记录的锁。这很好理解，比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更新。 当然，数据库中还有一些没那么一目了然的概念和设计，这些概念如果理解和使用不当，容易导致程序出现非预期行为，比如两阶段锁。 ","date":"2025-02-11","objectID":"/posts/7.%E8%A1%8C%E9%94%81%E5%8A%9F%E8%BF%87%E6%80%8E%E4%B9%88%E5%87%8F%E5%B0%91%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"07 | 行锁功过：怎么减少行锁对性能的影响？","uri":"/posts/7.%E8%A1%8C%E9%94%81%E5%8A%9F%E8%BF%87%E6%80%8E%E4%B9%88%E5%87%8F%E5%B0%91%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 从两阶段锁说起 先举个例子。在下面的操作序列中，事务 B 的 update 语句执行时会是什么现象呢？假设字段 id 是表 t 的主键。 这个问题的结论取决于事务 A 在执行完两条 update 语句后，持有哪些锁，以及在什么时候释放。可以验证一下：实际上事务 B 的 update 语句会被阻塞，直到事务 A 执行 commit 之后，事务 B 才能继续执行。现在可以明确的是事务 A 持有的两个记录的行锁，都是在 commit 的时候才释放的。 也就是说，在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。 知道了这个设定，对我们使用事务有什么帮助呢？那就是，如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。举个例子。 假设你负责实现一个电影票在线交易业务，顾客 A 要在影院 B 购买电影票。我们简化一点，这个业务需要涉及到以下操作： 从顾客 A 账户余额中扣除电影票价； 给影院 B 的账户余额增加这张电影票价； 记录一条交易日志。 也就是说，要完成这个交易，需要 update 两条记录，并 insert 一条记录。当然，为了保证交易的原子性，要把这三个操作放在一个事务中。那么，你会怎样安排这三个语句在事务中的顺序呢？ 试想如果同时有另外一个顾客 C 要在影院 B 买票，那么这两个事务冲突的部分就是语句 2 了。因为它们要更新同一个影院账户的余额，需要修改同一行数据。 根据两阶段锁协议，不论怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果把语句 2 安排在最后，比如按照 3、1、2 这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。 好了，现在由于正确的设计，影院余额这一行的行锁在一个事务中不会停留很长时间。但是，这并没有完全解决你的困扰。 如果这个影院做活动，可以低价预售一年内所有的电影票，而且这个活动只做一天。于是在活动时间开始的时候，你的 MySQL 就挂了。你登上服务器一看，CPU 消耗接近 100%，但整个数据库每秒就执行不到 100 个事务。这是什么原因呢？这就要说到死锁和死锁检测了。 ","date":"2025-02-11","objectID":"/posts/7.%E8%A1%8C%E9%94%81%E5%8A%9F%E8%BF%87%E6%80%8E%E4%B9%88%E5%87%8F%E5%B0%91%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"07 | 行锁功过：怎么减少行锁对性能的影响？","uri":"/posts/7.%E8%A1%8C%E9%94%81%E5%8A%9F%E8%BF%87%E6%80%8E%E4%B9%88%E5%87%8F%E5%B0%91%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 死锁和死锁检测 当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。这里用数据库中的行锁举个例子。 这时候，事务 A 在等待事务 B 释放 id=2 的行锁，而事务 B 在等待事务 A 释放 id=1 的行锁。事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略： 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。 在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。 但是，又不可能直接把这个时间设置成一个很小的值，比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。 所以，正常情况下还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。 可以想象一下这个过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。那如果是上面说到的所有事务都要更新同一行的场景呢？ 每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。因此，就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。 根据上面的分析，来讨论一下，怎么解决由这种热点行更新导致的性能问题呢？问题的症结在于，死锁检测要耗费大量的 CPU 资源。 一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。 另一个思路是控制并发度。根据上面的分析，你会发现如果并发能够控制住，比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。 因此，这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；如果你的团队有能修改 MySQL 源码的人，也可以做在 MySQL 里面。基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在 InnoDB 内部就不会有大量的死锁检测工作了。但是如果团队里暂时没有数据库方面的专家，不能实现这样的方案，能不能从设计上优化这个问题呢？ 你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。 这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成 0 的时候，代码要有特殊处理。 ","date":"2025-02-11","objectID":"/posts/7.%E8%A1%8C%E9%94%81%E5%8A%9F%E8%BF%87%E6%80%8E%E4%B9%88%E5%87%8F%E5%B0%91%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"07 | 行锁功过：怎么减少行锁对性能的影响？","uri":"/posts/7.%E8%A1%8C%E9%94%81%E5%8A%9F%E8%BF%87%E6%80%8E%E4%B9%88%E5%87%8F%E5%B0%91%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 小结 如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁的申请时机尽量往后放。 但是，调整语句顺序并不能完全避免死锁。所以引入了死锁和死锁检测的概念，以及提供了三个方案，来减少死锁对数据库的影响。减少死锁的主要方向，就是控制访问相同资源的并发事务量。 ","date":"2025-02-11","objectID":"/posts/7.%E8%A1%8C%E9%94%81%E5%8A%9F%E8%BF%87%E6%80%8E%E4%B9%88%E5%87%8F%E5%B0%91%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"07 | 行锁功过：怎么减少行锁对性能的影响？","uri":"/posts/7.%E8%A1%8C%E9%94%81%E5%8A%9F%E8%BF%87%E6%80%8E%E4%B9%88%E5%87%8F%E5%B0%91%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 问题 问：如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到： 第一种，直接执行 delete from T limit 10000; 第二种，在一个连接中循环执行 20 次 delete from T limit 500; 第三种，在 20 个连接中同时执行 delete from T limit 500。 你会选择哪一种方法呢？为什么呢？ 答：第二种方式是相对较好的。 第一种方式（即：直接执行 delete from T limit 10000）里面，单个语句占用时间长，锁的时间也比较长；而且大事务还会导致主从延迟。 第三种方式（即：在 20 个连接中同时执行 delete from T limit 500），会人为造成锁冲突。 ","date":"2025-02-11","objectID":"/posts/7.%E8%A1%8C%E9%94%81%E5%8A%9F%E8%BF%87%E6%80%8E%E4%B9%88%E5%87%8F%E5%B0%91%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"07 | 行锁功过：怎么减少行锁对性能的影响？","uri":"/posts/7.%E8%A1%8C%E9%94%81%E5%8A%9F%E8%BF%87%E6%80%8E%E4%B9%88%E5%87%8F%E5%B0%91%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"MySQL 的锁设计旨在处理并发访问问题，主要分为全局锁、表级锁和行锁。","date":"2025-02-11","objectID":"/posts/6.%E5%85%A8%E5%B1%80%E9%94%81%E5%92%8C%E8%A1%A8%E9%94%81%E7%BB%99%E8%A1%A8%E5%8A%A0%E4%B8%AA%E5%AD%97%E6%AE%B5%E6%80%8E%E4%B9%88%E6%9C%89%E8%BF%99%E4%B9%88%E5%A4%9A%E9%98%BB%E7%A2%8D/","tags":["MySQL 实战 45 讲","MySQL"],"title":"06 | 全局锁和表锁：给表加个字段怎么有这么多阻碍？","uri":"/posts/6.%E5%85%A8%E5%B1%80%E9%94%81%E5%92%8C%E8%A1%A8%E9%94%81%E7%BB%99%E8%A1%A8%E5%8A%A0%E4%B8%AA%E5%AD%97%E6%AE%B5%E6%80%8E%E4%B9%88%E6%9C%89%E8%BF%99%E4%B9%88%E5%A4%9A%E9%98%BB%E7%A2%8D/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 MySQL 的锁设计旨在处理并发访问问题，主要分为全局锁、表级锁和行锁。全局锁通过 Flush tables with read lock (FTWRL) 命令实现对整个数据库实例的加锁，常用于全库逻辑备份，而表级锁一般是在数据库引擎不支持行锁的时候才会被用到。 数据库锁设计的初衷是处理并发问题。作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。 ","date":"2025-02-11","objectID":"/posts/6.%E5%85%A8%E5%B1%80%E9%94%81%E5%92%8C%E8%A1%A8%E9%94%81%E7%BB%99%E8%A1%A8%E5%8A%A0%E4%B8%AA%E5%AD%97%E6%AE%B5%E6%80%8E%E4%B9%88%E6%9C%89%E8%BF%99%E4%B9%88%E5%A4%9A%E9%98%BB%E7%A2%8D/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"06 | 全局锁和表锁：给表加个字段怎么有这么多阻碍？","uri":"/posts/6.%E5%85%A8%E5%B1%80%E9%94%81%E5%92%8C%E8%A1%A8%E9%94%81%E7%BB%99%E8%A1%A8%E5%8A%A0%E4%B8%AA%E5%AD%97%E6%AE%B5%E6%80%8E%E4%B9%88%E6%9C%89%E8%BF%99%E4%B9%88%E5%A4%9A%E9%98%BB%E7%A2%8D/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 全局锁 顾名思义，全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。以前有一种做法，是通过 FTWRL 确保不会有其他线程对数据库做更新，然后对整个库做备份。注意，在备份过程中整个库完全处于只读状态。但是让整库都只读，听上去就很危险： 如果在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆； 如果在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。 看来加全局锁不太好。但是细想一下，备份为什么要加锁呢？先来看一下不加锁会有什么问题。 假设你现在要维护一个购买系统，关注的是用户账户余额表和用户课程表。 现在发起一个逻辑备份。假设备份期间，有一个用户，他购买了一门课程，业务逻辑里就要扣掉他的余额，然后往已购课程里面加上一门课。 如果时间顺序上是先备份账户余额表 (u_account)，然后用户购买，然后备份用户课程表 (u_course)，会怎么样呢？可以看一下这个图： 可以看到，这个备份结果里，用户 A 的数据状态是“账户余额没扣，但是用户课程表里面已经多了一门课”。如果后面用这个备份来恢复数据的话，用户 A 就发现，自己赚了。 作为用户可别觉得这样可真好啊，可以试想一下：如果备份表的顺序反过来，先备份用户课程表再备份账户余额表，又可能会出现什么结果？ 也就是说，不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。其实是有一个方法能够拿到一致性视图的，就是在可重复读隔离级别下开启一个事务。 官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。 有了这个功能，为什么还需要 FTWRL 呢？一致性读是好，但前提是引擎要支持这个隔离级别。比如，对于 MyISAM 这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，就需要使用 FTWRL 命令了。 所以，single-transaction 方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法。这往往是 DBA 要求业务开发人员使用 InnoDB 替代 MyISAM 的原因之一。 既然要全库只读，为什么不使用 set global readonly=true 的方式呢？确实 readonly 方式也可以让全库进入只读状态，但还是建议用 FTWRL 方式，主要有两个原因： 一是，在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，不建议使用。 二是，在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。 业务的更新不只是增删改数据（DML)，还有可能是加字段等修改表结构的操作（DDL）。不论是哪种方法，一个库被全局锁上以后，要对里面任何一个表做加字段操作，都是会被锁住的。 ","date":"2025-02-11","objectID":"/posts/6.%E5%85%A8%E5%B1%80%E9%94%81%E5%92%8C%E8%A1%A8%E9%94%81%E7%BB%99%E8%A1%A8%E5%8A%A0%E4%B8%AA%E5%AD%97%E6%AE%B5%E6%80%8E%E4%B9%88%E6%9C%89%E8%BF%99%E4%B9%88%E5%A4%9A%E9%98%BB%E7%A2%8D/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"06 | 全局锁和表锁：给表加个字段怎么有这么多阻碍？","uri":"/posts/6.%E5%85%A8%E5%B1%80%E9%94%81%E5%92%8C%E8%A1%A8%E9%94%81%E7%BB%99%E8%A1%A8%E5%8A%A0%E4%B8%AA%E5%AD%97%E6%AE%B5%E6%80%8E%E4%B9%88%E6%9C%89%E8%BF%99%E4%B9%88%E5%A4%9A%E9%98%BB%E7%A2%8D/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 表级锁 MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。 表锁的语法是 lock tables … read/write。与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。 举个例子，如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。 在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于 InnoDB 这种支持行锁的引擎，一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。 另一类表级的锁是 MDL（metadata lock)。MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。 因此，在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。 读锁之间不互斥，因此可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 虽然 MDL 锁是系统默认会加的，但却是不能忽略的一个机制。比如下面这个例子：给一个小表加个字段，导致整个库挂了。给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。在对大表操作的时候，你肯定会特别小心，以免对线上服务造成影响。而实际上，即使是小表，操作不慎也会出问题。来看一下下面的操作序列，假设表 t 是一个小表。 备注：这里的实验环境是 MySQL 5.6。 可以看到 session A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需要的也是 MDL 读锁，因此可以正常执行。 之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。 如果只有 session C 自己被阻塞还没什么关系，但是之后所有要在表 t 上新申请 MDL 读锁的请求也会被 session C 阻塞。所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了。 如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。 事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。那么，如何安全地给小表加字段？ 首先要解决长事务，事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，可以查到当前执行中的事务。如果要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。 但考虑一下这个场景。如果要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，该怎么做呢？ 这时候 kill 可能未必管用，因为新的请求马上就来了。比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程。 MariaDB 已经合并了 AliSQL 的这个功能，所以这两个开源分支目前都支持 DDL NOWAIT/WAIT n 这个语法。 ALTER TABLE tbl_name NOWAIT add column ... ALTER TABLE tbl_name WAIT N add column ... ","date":"2025-02-11","objectID":"/posts/6.%E5%85%A8%E5%B1%80%E9%94%81%E5%92%8C%E8%A1%A8%E9%94%81%E7%BB%99%E8%A1%A8%E5%8A%A0%E4%B8%AA%E5%AD%97%E6%AE%B5%E6%80%8E%E4%B9%88%E6%9C%89%E8%BF%99%E4%B9%88%E5%A4%9A%E9%98%BB%E7%A2%8D/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"06 | 全局锁和表锁：给表加个字段怎么有这么多阻碍？","uri":"/posts/6.%E5%85%A8%E5%B1%80%E9%94%81%E5%92%8C%E8%A1%A8%E9%94%81%E7%BB%99%E8%A1%A8%E5%8A%A0%E4%B8%AA%E5%AD%97%E6%AE%B5%E6%80%8E%E4%B9%88%E6%9C%89%E8%BF%99%E4%B9%88%E5%A4%9A%E9%98%BB%E7%A2%8D/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 小结 全局锁主要用在逻辑备份过程中。对于全部是 InnoDB 引擎的库，建议你选择使用–single-transaction 参数，对应用会更友好。 表锁一般是在数据库引擎不支持行锁的时候才会被用到的。如果你发现你的应用程序里有 lock tables 这样的语句，你需要追查一下，比较可能的情况是： 要么是你的系统现在还在用 MyISAM 这类不支持事务的引擎，那要安排升级换引擎； 要么是你的引擎升级了，但是代码还没升级。把 lock tables 和 unlock tables 改成 begin 和 commit，问题就解决了。 MDL 会直到事务提交才释放，在做表结构变更的时候，一定要小心不要导致锁住线上查询和更新。 ","date":"2025-02-11","objectID":"/posts/6.%E5%85%A8%E5%B1%80%E9%94%81%E5%92%8C%E8%A1%A8%E9%94%81%E7%BB%99%E8%A1%A8%E5%8A%A0%E4%B8%AA%E5%AD%97%E6%AE%B5%E6%80%8E%E4%B9%88%E6%9C%89%E8%BF%99%E4%B9%88%E5%A4%9A%E9%98%BB%E7%A2%8D/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"06 | 全局锁和表锁：给表加个字段怎么有这么多阻碍？","uri":"/posts/6.%E5%85%A8%E5%B1%80%E9%94%81%E5%92%8C%E8%A1%A8%E9%94%81%E7%BB%99%E8%A1%A8%E5%8A%A0%E4%B8%AA%E5%AD%97%E6%AE%B5%E6%80%8E%E4%B9%88%E6%9C%89%E8%BF%99%E4%B9%88%E5%A4%9A%E9%98%BB%E7%A2%8D/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 问题 问：备份一般都会在备库上执行，在用–single-transaction 方法做逻辑备份的过程中，如果主库上的一个小表做了一个 DDL，比如给一个表上加了一列。这时候，从备库上会看到什么现象呢？ 答：假设这个 DDL 是针对表 t1 的，这里把备份过程中几个关键的语句列出来： Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ; Q2:START TRANSACTION WITH CONSISTENT SNAPSHOT； /* other tables */ Q3:SAVEPOINT sp; /* 时刻 1 */ Q4:show create table `t1`; /* 时刻 2 */ Q5:SELECT * FROM `t1`; /* 时刻 3 */ Q6:ROLLBACK TO SAVEPOINT sp; /* 时刻 4 */ /* other tables */ 在备份开始的时候，为了确保 RR（可重复读）隔离级别，再设置一次 RR 隔离级别 (Q1); 启动事务，这里用 WITH CONSISTENT SNAPSHOT 确保这个语句执行完就可以得到一个一致性视图（Q2)； 设置一个保存点，这个很重要（Q3）； show create 是为了拿到表结构 (Q4)，然后正式导数据（Q5），回滚到 SAVEPOINT sp，在这里的作用是释放 t1 的 MDL 锁（Q6）。 参考答案如下： 如果在 Q4 语句执行之前到达，现象：没有影响，备份拿到的是 DDL 后的表结构。 如果在“时刻 2”到达，则表结构被改过，Q5 执行的时候，报 Table definition has changed, please retry transaction，现象：mysqldump 终止； 如果在“时刻 2”和“时刻 3”之间到达，mysqldump 占着 t1 的 MDL 读锁，binlog 被阻塞，现象：主从延迟，直到 Q6 执行完成。 从“时刻 4”开始，mysqldump 释放了 MDL 读锁，现象：没有影响，备份拿到的是 DDL 前的表结构。 ","date":"2025-02-11","objectID":"/posts/6.%E5%85%A8%E5%B1%80%E9%94%81%E5%92%8C%E8%A1%A8%E9%94%81%E7%BB%99%E8%A1%A8%E5%8A%A0%E4%B8%AA%E5%AD%97%E6%AE%B5%E6%80%8E%E4%B9%88%E6%9C%89%E8%BF%99%E4%B9%88%E5%A4%9A%E9%98%BB%E7%A2%8D/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"06 | 全局锁和表锁：给表加个字段怎么有这么多阻碍？","uri":"/posts/6.%E5%85%A8%E5%B1%80%E9%94%81%E5%92%8C%E8%A1%A8%E9%94%81%E7%BB%99%E8%A1%A8%E5%8A%A0%E4%B8%AA%E5%AD%97%E6%AE%B5%E6%80%8E%E4%B9%88%E6%9C%89%E8%BF%99%E4%B9%88%E5%A4%9A%E9%98%BB%E7%A2%8D/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文深入探讨了 MySQL 索引相关的概念，包括覆盖索引、最左前缀原则和索引下推。","date":"2025-02-10","objectID":"/posts/5.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8B/","tags":["MySQL 实战 45 讲","MySQL"],"title":"05 | 深入浅出索引（下）","uri":"/posts/5.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8B/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文深入探讨了 MySQL 索引相关的概念，包括覆盖索引、最左前缀原则和索引下推。覆盖索引能减少树的搜索次数，提升查询性能；最左前缀原则强调索引的复用能力，建议为高频请求创建联合索引；索引下推优化可减少回表次数，提高查询效率。 在下面这个表 T 中，如果执行 select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行？下面是这个表的初始化语句。 mysql\u003e create table T ( ID int primary key, k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT '', index k(k)) engine=InnoDB; insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg'); 现在，我们一起来看看这条 SQL 查询语句的执行流程： 在 k 索引树上找到 k=3 的记录，取得 ID = 300； 再到 ID 索引树查到 ID=300 对应的 R3； 在 k 索引树取下一个值 k=5，取得 ID=500； 再回到 ID 索引树查到 ID=500 对应的 R4； 在 k 索引树取下一个值 k=6，不满足条件，循环结束。 在这个过程中，回到主键索引树搜索的过程，称为回表。可以看到，这个查询过程读了 k 索引树的 3 条记录（步骤 1、3 和 5），回表了两次（步骤 2 和 4）。 在这个例子中，由于查询结果所需要的数据只在主键索引上有，所以不得不回表。那么，有没有可能经过索引优化，避免回表过程呢？ ","date":"2025-02-10","objectID":"/posts/5.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8B/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"05 | 深入浅出索引（下）","uri":"/posts/5.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8B/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 覆盖索引 如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，称为覆盖索引。 由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。 需要注意的是，在引擎内部使用覆盖索引在索引 k 上其实读了三个记录，R3~R5（对应的索引 k 上的记录项），但是对于 MySQL 的 Server 层来说，它就是找引擎拿到了两条记录，因此 MySQL 认为扫描行数是 2。 基于上面覆盖索引的说明，来讨论一个问题：在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？假设这个市民表的定义是这样的： CREATE TABLE `tuser` ( `id` int(11) NOT NULL, `id_card` varchar(32) DEFAULT NULL, `name` varchar(32) DEFAULT NULL, `age` int(11) DEFAULT NULL, `ismale` tinyint(1) DEFAULT NULL, PRIMARY KEY (`id`), KEY `id_card` (`id_card`), KEY `name_age` (`name`,`age`) ) ENGINE=InnoDB 我们知道，身份证号是市民的唯一标识。也就是说，如果有根据身份证号查询市民信息的需求，只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是不是浪费空间？ 如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。 当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。这正是业务 DBA，或者称为业务数据架构师的工作。 ","date":"2025-02-10","objectID":"/posts/5.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8B/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"05 | 深入浅出索引（下）","uri":"/posts/5.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8B/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 最左前缀原则 如果为每一种查询都设计一个索引，索引是不是太多了。如果现在要按照市民的身份证号去查他的家庭地址呢？虽然这个查询需求在业务中出现的概率不高，但总不能让它走全表扫描吧？反过来说，单独为一个不频繁的请求创建一个（身份证号，地址）的索引又感觉有点浪费。应该怎么做呢？ 这里先说结论吧。B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。为了直观地说明这个概念，用（name，age）这个联合索引来分析。 可以看到，索引项是按照索引定义里面出现的字段顺序排序的。当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到 ID4，然后向后遍历得到所有需要的结果。 如果要查的是所有名字第一个字是“张”的人，SQL 语句的条件是\"where name like‘张 %’\"。这时，也能够用上这个索引，查找到第一个符合条件的记录是 ID3，然后向后遍历，直到不满足条件为止。 可以看到，不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。 基于上面对最左前缀索引的说明，来讨论一个问题：在建立联合索引的时候，如何安排索引内的字段顺序。 这里的评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引了。因此，第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。 这段开头的问题里，要为高频请求创建 (身份证号，姓名）这个联合索引，并用这个索引支持“根据身份证号查询地址”的需求。那么，如果既有联合查询，又有基于 a、b 各自的查询呢？查询条件里面只有 b 的语句，是无法使用 (a,b) 这个联合索引的，这时候不得不维护另外一个索引，也就是说需要同时维护 (a,b)、(b) 这两个索引。 这时候，要考虑的原则就是空间了。比如上面这个市民表的情况，name 字段是比 age 字段大的，那就建议你创建一个（name,age) 的联合索引和一个 (age) 的单字段索引。 ","date":"2025-02-10","objectID":"/posts/5.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8B/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"05 | 深入浅出索引（下）","uri":"/posts/5.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8B/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 索引下推 最左前缀可以用于在索引中定位记录。那些不符合最左前缀的部分，会怎么样呢？还是以市民表的联合索引（name, age）为例。如果现在有一个需求：检索出表中“名字第一个字是张，而且年龄是 10 岁的所有男孩”。那么，SQL 语句是这么写的： mysql\u003e select * from tuser where name like '张%' and age=10 and ismale=1; 这个语句在搜索索引树的时候，只能用“张”，找到第一个满足条件的记录 ID3。当然，这还不错，总比全表扫描要好。然后呢？当然是判断其他条件是否满足。 在 MySQL 5.6 之前，只能从 ID3 开始一个个回表。到主键索引上找出数据行，再对比字段值。 而 MySQL 5.6 引入的索引下推优化（index condition pushdown)，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 下面两幅图是这两个过程的执行流程图。 在上面这两个图里面，每一个虚线箭头表示回表一次。 第一幅中，在 (name,age) 索引里面去掉了 age 的值，这个过程 InnoDB 并不会去看 age 的值，只是按顺序把“name 第一个字是’张’”的记录一条条取出来回表。因此，需要回表 4 次。 第二幅图跟第一幅图的区别是，InnoDB 在 (name,age) 索引内部就判断了 age 是否等于 10，对于不等于 10 的记录，直接判断并跳过。只需要对 ID4、ID5 这两条记录回表取数据判断，就只需要回表 2 次。 ","date":"2025-02-10","objectID":"/posts/5.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8B/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"05 | 深入浅出索引（下）","uri":"/posts/5.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8B/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 小结 在满足语句需求的情况下，尽量少地访问资源是数据库设计的重要原则之一。在使用数据库的时候，尤其是在设计表结构时，也要以减少资源消耗作为目标。 ","date":"2025-02-10","objectID":"/posts/5.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8B/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"05 | 深入浅出索引（下）","uri":"/posts/5.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8B/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 问题 问：实际上主键索引也是可以使用多个字段的。DBA 小吕在入职新公司的时候，就发现自己接手维护的库里面，有这么一个表，表结构定义类似这样的： CREATE TABLE `geek` ( `a` int(11) NOT NULL, `b` int(11) NOT NULL, `c` int(11) NOT NULL, `d` int(11) NOT NULL, PRIMARY KEY (`a`,`b`), KEY `c` (`c`), KEY `ca` (`c`,`a`), KEY `cb` (`c`,`b`) ) ENGINE=InnoDB; 公司的同事告诉他说，由于历史原因，这个表需要 a、b 做联合主键，这个小吕理解了。但是小吕又纳闷了，既然主键包含了 a、b 这两个字段，那意味着单独在字段 c 上创建一个索引，就已经包含了三个字段了呀，为什么要创建“ca”“cb”这两个索引？同事告诉他，是因为他们的业务里面有这样的两种语句： select * from geek where c=N order by a limit 1; select * from geek where c=N order by b limit 1; 这位同事的解释对吗，为了这两个查询模式，这两个索引是否都是必须的？为什么呢？ 答：结论是 ca 可以去掉，cb 需要保留。 ","date":"2025-02-10","objectID":"/posts/5.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8B/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"05 | 深入浅出索引（下）","uri":"/posts/5.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8B/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文介绍了 MySQL 选择 B+ 树结构来存储数据的原因，并分析了数据库索引的重要性以及三种常见的索引模型：哈希表、有序数组和搜索树。","date":"2025-02-10","objectID":"/posts/4.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8A/","tags":["MySQL 实战 45 讲","MySQL"],"title":"04 | 深入浅出索引（上）","uri":"/posts/4.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文介绍了 MySQL 选择 B+ 树结构来存储数据的原因，并分析了数据库索引的重要性以及三种常见的索引模型：哈希表、有序数组和搜索树。文章重点介绍了 InnoDB 采用的 B+ 树索引模型，以及 B+ 树在维护索引有序性和索引维护过程中的作用。 一句话简单来说，索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。一本 500 页的书，如果想快速找到其中的某一个知识点，在不借助目录的情况下，那估计可得找一会儿。同样，对于数据库的表而言，索引其实就是它的“目录”。 ","date":"2025-02-10","objectID":"/posts/4.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8A/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"04 | 深入浅出索引（上）","uri":"/posts/4.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 索引的常见模型 索引的出现是为了提高查询效率，但是实现索引的方式却有很多种，所以这里也就引入了索引模型的概念。可以用于提高读写效率的数据结构很多，先介绍三种常见、也比较简单的数据结构，它们分别是哈希表、有序数组和搜索树。下面主要从使用的角度，简单分析一下这三种模型的区别。 哈希表是一种以键 - 值（key-value）存储数据的结构，只要输入待查找的键即 key，就可以找到其对应的值即 Value。哈希的思路很简单，把值放在数组里，用一个哈希函数把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置。 不可避免地，多个 key 值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方法是，拉出一个链表。假设，现在维护着一个身份证信息和姓名的表，需要根据身份证号查找对应的名字，这时对应的哈希索引的示意图如下所示： 图中，User2 和 User4 根据身份证号算出来的值都是 N，但没关系，后面还跟了一个链表。假设，这时候要查 ID_card_n2 对应的名字是什么，处理步骤就是：首先，将 ID_card_n2 通过哈希函数算出 N；然后，按顺序遍历，找到 User2。 需要注意的是，图中四个 ID_card_n 的值并不是递增的，这样做的好处是增加新的 User 时速度会很快，只需要往后追加。但缺点是，因为不是有序的，所以哈希索引做区间查询的速度是很慢的。可以设想下，如果现在要找身份证号在[ID_card_X, ID_card_Y]这个区间的所有用户，就必须全部扫描一遍了。所以，哈希表这种结构适用于只有等值查询的场景，比如 Memcached 及其他一些 NoSQL 引擎。 而有序数组在等值查询和范围查询场景中的性能就都非常优秀。还是上面这个根据身份证号查名字的例子，如果使用有序数组来实现的话，示意图如下所示： 这里假设身份证号没有重复，这个数组就是按照身份证号递增的顺序保存的。这时候如果要查 ID_card_n2 对应的名字，用二分法就可以快速得到，这个时间复杂度是 O(log(N))。 同时很显然，这个索引结构支持范围查询。要查身份证号在[ID_card_X, ID_card_Y]区间的 User，可以先用二分法找到 ID_card_X（如果不存在 ID_card_X，就找到大于 ID_card_X 的第一个 User），然后向右遍历，直到查到第一个大于 ID_card_Y 的身份证号，退出循环。 如果仅仅看查询效率，有序数组就是最好的数据结构了。但是，在需要更新数据的时候就麻烦了，往中间插入一个记录就必须得挪动后面所有的记录，成本太高。所以，有序数组索引只适用于静态存储引擎，比如要保存的是 2017 年某个城市的所有人口信息，这类不会再修改的数据。 二叉搜索树也是课本里的经典数据结构了。还是上面根据身份证号查名字的例子，如果我们用二叉搜索树来实现的话，示意图如下所示： 二叉搜索树的特点是：父节点左子树所有结点的值小于父节点的值，右子树所有结点的值大于父节点的值。这样如果要查 ID_card_n2 的话，按照图中的搜索顺序就是按照 UserA -\u003e UserC -\u003e UserF -\u003e User2 这个路径得到。这个时间复杂度是 O(log(N))。 当然为了维持 O(log(N)) 的查询复杂度，就需要保持这棵树是平衡二叉树。为了做这个保证，更新的时间复杂度也是 O(log(N))。 树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。 可以想象一下一棵 100 万节点的平衡二叉树，树高 20。一次查询可能需要访问 20 个数据块。在机械硬盘时代，从磁盘随机读一个数据块需要 10 ms 左右的寻址时间。也就是说，对于一个 100 万行的表，如果使用二叉树来存储，单独访问一个行可能需要 20 个 10 ms 的时间，这个查询可真够慢的。 为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，就不应该使用二叉树，而是要使用“N 叉”树。这里，“N 叉”树中的“N”取决于数据块的大小。 以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。这棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。N 叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。 不管是哈希还是有序数组，或者 N 叉树，它们都是不断迭代、不断优化的产物或者解决方案。数据库技术发展到今天，跳表、LSM 树等数据结构也被用于引擎设计中。 数据库底层存储的核心就是基于这些数据模型的。每碰到一个新数据库，需要先关注它的数据模型，这样才能从理论上分析出这个数据库的适用场景。 在 MySQL 中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。由于 InnoDB 存储引擎在 MySQL 数据库中使用最为广泛，下面就以 InnoDB 为例，分析一下其中的索引模型。 ","date":"2025-02-10","objectID":"/posts/4.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8A/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"04 | 深入浅出索引（上）","uri":"/posts/4.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 InnoDB 的索引模型 在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。每一个索引在 InnoDB 里面对应一棵 B+ 树。 假设，有一个主键列为 ID 的表，表中有字段 k，并且在 k 上有索引。这个表的建表语句是： mysql\u003e create table T( id int primary key, k int not null, name varchar(16), index (k))engine=InnoDB; 表中 R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，两棵树的示例示意图如下。 从图中不难看出，根据叶子节点的内容，索引类型分为主键索引和非主键索引。 主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。 非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。 根据上面的索引结构说明，我们来讨论一个问题：基于主键索引和普通索引的查询有什么区别？ 如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树； 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。 也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，在应用中应该尽量使用主键查询。 ","date":"2025-02-10","objectID":"/posts/4.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8A/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"04 | 深入浅出索引（上）","uri":"/posts/4.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 索引维护 B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例，如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。如果新插入的 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。 而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。 除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。 当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。 基于上面的索引维护过程说明，我们来讨论一个案例： 你可能在一些建表规范里面见到过类似的描述，要求建表语句里一定要有自增主键。当然事无绝对，我们来分析一下哪些场景下应该使用自增主键，而哪些场景下不应该。 自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的：NOT NULL PRIMARY KEY AUTO_INCREMENT。插入新记录的时候可以不指定 ID 的值，系统会获取当前 ID 最大值加 1 作为下一条记录的 ID 值。 也就是说，自增主键的插入数据模式，正符合了前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。 而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。除了考虑性能外，还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？ 由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约 20 个字节，而如果用整型做主键，则只要 4 个字节，如果是长整型（bigint）则是 8 个字节。 显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的： 只有一个索引； 该索引必须是唯一索引。 这就是典型的 KV 场景。由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。这时候就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。 ","date":"2025-02-10","objectID":"/posts/4.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8A/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"04 | 深入浅出索引（上）","uri":"/posts/4.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 小结 本文分析了数据库引擎可用的数据结构，介绍了 InnoDB 采用的 B+ 树结构，以及为什么 InnoDB 要这么选择。B+ 树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数。由于 InnoDB 是索引组织表，一般情况下会建议你创建一个自增主键，这样非主键索引占用的空间最小。但事无绝对，也讨论了使用业务逻辑字段做主键的应用场景。 ","date":"2025-02-10","objectID":"/posts/4.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8A/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"04 | 深入浅出索引（上）","uri":"/posts/4.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 问题 问：对于上面例子中的 InnoDB 表 T，如果要重建索引 k，你的两个 SQL 语句可以这么写： alter table T drop index k; alter table T add index(k); 如果要重建主键索引，也可以这么写： alter table T drop primary key; alter table T add primary key(id); 对于上面这两个重建索引的作法，说出你的理解。如果有不合适的，为什么，更好的方法是什么？ 答：重建索引 k 的做法是合理的，可以达到省空间的目的。但是，重建主键的过程不合理。不论是删除主键还是创建主键，都会将整个表重建。所以连着执行这两个语句的话，第一个语句就白做了。这两个语句，你可以用这个语句代替：alter table T engine=InnoDB。 ","date":"2025-02-10","objectID":"/posts/4.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8A/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"04 | 深入浅出索引（上）","uri":"/posts/4.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95-%E4%B8%8A/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"本文深入介绍了事务隔离的重要性以及 MySQL 中事务隔离的实现方式。","date":"2025-02-10","objectID":"/posts/3.%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81/","tags":["MySQL 实战 45 讲","MySQL"],"title":"03 | 事务隔离：为什么你改了我还看不见？","uri":"/posts/3.%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文深入介绍了事务隔离的重要性以及 MySQL 中事务隔离的实现方式。通过生动的转账例子引出了事务的概念，强调了事务的一致性和原子性。随后详细介绍了隔离级别的概念，包括读未提交、读提交、可重复读和串行化，并通过具体例子解释了不同隔离级别下的行为差异。 提到事务，大家肯定不陌生，和数据库打交道的时候，总是会用到事务。最经典的例子就是转账，你要给朋友小王转 100 块钱，而此时你的银行卡只有 100 块钱。 转账过程具体到程序里会有一系列的操作，比如查询余额、做加减法、更新余额等，这些操作必须保证是一体的，不然等程序查完之后，还没做减法之前，你这 100 块钱，完全可以借着这个时间差再查一次，然后再给另外一个朋友转账，如果银行这么整，不就乱了么？这时就要用到“事务”这个概念了。 简单来说，事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在 MySQL 中，事务支持是在引擎层实现的。你现在知道，MySQL 是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如 MySQL 原生的 MyISAM 引擎就不支持事务，这也是 MyISAM 被 InnoDB 取代的重要原因之一。 ","date":"2025-02-10","objectID":"/posts/3.%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"03 | 事务隔离：为什么你改了我还看不见？","uri":"/posts/3.%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 隔离性与隔离级别 提到事务，大家肯定会想到 ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），文本主要说的是其中的 I，也就是“隔离性”。 当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。 在谈隔离级别之前，首先要知道，隔离得越严实，效率就会越低。因此很多时候，都要在二者之间寻找一个平衡点。SQL 标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable）。 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 其中“读提交”和“可重复读”比较难理解，用一个例子说明这几种隔离级别。假设数据表 T 中只有一列，其中一行的值为 1，下面是按照时间顺序执行两个事务的行为。 mysql\u003e create table T(c int) engine=InnoDB; insert into T(c) values(1); 来看看在不同的隔离级别下，事务 A 会有哪些不同的返回结果，也就是图里面 V1、V2、V3 的返回值分别是什么。 若隔离级别是“读未提交”，则 V1 的值就是 2。这时候事务 B 虽然还没有提交，但是结果已经被 A 看到了。因此，V2、V3 也都是 2。 若隔离级别是“读提交”，则 V1 是 1，V2 的值是 2。事务 B 的更新在提交后才能被 A 看到。所以，V3 的值也是 2。 若隔离级别是“可重复读”，则 V1、V2 是 1，V3 是 2。之所以 V2 还是 1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。 若隔离级别是“串行化”，则在事务 B 执行“将 1 改成 2”的时候，会被锁住。直到事务 A 提交后，事务 B 才可以继续执行。所以从 A 的角度看，V1、V2 值是 1，V3 的值是 2。 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。 可以看到在不同的隔离级别下，数据库行为是有所不同的。Oracle 数据库的默认隔离级别其实就是“读提交”，因此对于一些从 Oracle 迁移到 MySQL 的应用，为保证数据库隔离级别的一致，一定要记得将 MySQL 的隔离级别设置为“读提交”。 配置的方式是，将启动参数 transaction-isolation 的值设置成 READ-COMMITTED。可以用 show variables 来查看当前的值。 mysql\u003e show variables like 'transaction_isolation'; +-----------------------+----------------+ | Variable_name | Value | +-----------------------+----------------+ | transaction_isolation | READ-COMMITTED | +-----------------------+----------------+ 总结来说，存在即合理，每种隔离级别都有自己的使用场景，要根据自己的业务情况来定。你可能会问那什么时候需要“可重复读”的场景呢？来看一个数据校对逻辑的案例。 假设你在管理一个个人银行账户表。一个表存了账户余额，一个表存了账单明细。到了月底要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响校对结果。 这时候使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。 ","date":"2025-02-10","objectID":"/posts/3.%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"03 | 事务隔离：为什么你改了我还看不见？","uri":"/posts/3.%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 事务隔离的实现 理解了事务的隔离级别，再来看看事务隔离具体是怎么实现的。这里展开说明“可重复读”。在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。 当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。 同时你会发现，即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的。 但是回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。 基于上面的说明，一起来讨论一下为什么建议尽量不要使用长事务。长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。可能会存在数据只有 20GB，而回滚段有 200GB 的库。最终只好为了清理回滚段，重建整个库。除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。 ","date":"2025-02-10","objectID":"/posts/3.%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"03 | 事务隔离：为什么你改了我还看不见？","uri":"/posts/3.%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 事务的启动方式 如前面所述，长事务有这些潜在风险，建议你尽量避免。其实很多时候业务开发同学并不是有意使用长事务，通常是由于误用所致。MySQL 的事务启动方式有以下几种： 显式启动事务语句，begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。 set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到主动执行 commit 或 rollback 语句，或者断开连接。 有些客户端连接框架会默认连接成功后先执行一个 set autocommit=0 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。因此，建议你总是使用 set autocommit=1, 通过显式语句的方式来启动事务。 可能有的人会纠结“多一次交互”的问题。对于一个需要频繁使用事务的业务，第二种方式每个事务在开始时都不需要主动执行一次“begin”，减少了语句的交互次数。如果你有这个顾虑，建议你使用 commit work and chain 语法。 在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。 可以在 information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。 select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))\u003e60 ","date":"2025-02-10","objectID":"/posts/3.%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"03 | 事务隔离：为什么你改了我还看不见？","uri":"/posts/3.%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 小结 这篇文章里面，介绍了 MySQL 的事务隔离级别的现象和实现，根据实现原理分析了长事务存在的风险，以及如何用正确的方式避免长事务。 ","date":"2025-02-10","objectID":"/posts/3.%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"03 | 事务隔离：为什么你改了我还看不见？","uri":"/posts/3.%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 问题 问：你现在知道了系统里面应该避免长事务，如果你是业务开发负责人同时也是数据库负责人，你会有什么方案来避免出现或者处理这种情况呢？ 答： 首先，从应用开发端来看： 确认是否使用了 set autocommit=0。这个确认工作可以在测试环境中开展，把 MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，目标就是把它改成 1。 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin/commit 框起来。有些业务并没有这个需要，但是也把好几个 select 语句放到了事务中。这种只读事务可以去掉。 业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。 其次，从数据库端来看： 监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / 或者 kill； Percona 的 pt-kill 这个工具不错，推荐使用； 在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题； 如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。 ","date":"2025-02-10","objectID":"/posts/3.%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"03 | 事务隔离：为什么你改了我还看不见？","uri":"/posts/3.%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"一条 SQL 更新语句是如何执行的？","date":"2025-02-09","objectID":"/posts/2.%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B8%80%E6%9D%A1sql%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/","tags":["MySQL 实战 45 讲","MySQL"],"title":"02 | 日志系统：一条 SQL 更新语句是如何执行的？","uri":"/posts/2.%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B8%80%E6%9D%A1sql%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文深入介绍了 MySQL 更新语句的执行流程和日志系统设计，包括重做日志和归档日志的特点，以及两阶段提交的重要性。可以从中了解 MySQL 的日志系统设计和执行过程，以及保证数据安全和一致性的关键参数设置建议。 一条查询语句的执行过程一般是经过连接器、分析器、优化器、执行器等功能模块，最后到达存储引擎。那么，一条更新语句的执行流程又是怎样的呢？ 之前你可能经常听 DBA 同事说，MySQL 可以恢复到半个月内任意一秒的状态，惊叹的同时，是不是心中也会不免会好奇，这是怎样做到的呢？我们还是从一个表的一条更新语句说起，下面是这个表的创建语句，这个表有一个主键 ID 和一个整型字段 c： mysql\u003e create table T(ID int primary key, c int); 如果要将 ID=2 这一行的值加 1，SQL 语句就会这么写： mysql\u003e update T set c=c+1 where ID = 2; 首先，可以确定的说，查询语句的那一套流程，更新语句也是同样会走一遍。如下图所示： 你执行语句前要先连接数据库，这是连接器的工作。 在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表 T 上所有缓存结果都清空。这也就是一般不建议使用查询缓存的原因。 接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用 ID 这个索引。然后，执行器负责具体执行，找到这一行，然后更新。 与查询流程不一样的是，更新流程还涉及两个重要的日志模块：redo log（重做日志）和 binlog（归档日志）。如果接触 MySQL，那这两个词肯定是绕不过的。 ","date":"2025-02-09","objectID":"/posts/2.%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B8%80%E6%9D%A1sql%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"02 | 日志系统：一条 SQL 更新语句是如何执行的？","uri":"/posts/2.%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B8%80%E6%9D%A1sql%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 重要的日志模块：redo log 在 MySQL 里有一个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了解决这个问题，MySQL 的设计者就用了类似酒店掌柜粉板的思路来提升更新效率。而粉板和账本配合的整个过程，其实就是 MySQL 里经常说到的 WAL 技术，WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。 具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。 如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎么办呢？这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间。 与此类似，InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。 write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。 要理解 crash-safe 这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板上的数据明确赊账账目。 ","date":"2025-02-09","objectID":"/posts/2.%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B8%80%E6%9D%A1sql%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"02 | 日志系统：一条 SQL 更新语句是如何执行的？","uri":"/posts/2.%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B8%80%E6%9D%A1sql%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 重要的日志模块：binlog MySQL 整体来看，其实就有两块：一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面说到的 redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。你可能会问，为什么会有两份日志呢？ 因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。 这两种日志有以下三点不同。 redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1”。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 有了对这两个日志的概念性理解，再来看执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程。 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。 下面是这个 update 语句的执行流程图，图中浅色框表示是在 InnoDB 内部执行的，深色框表示是在执行器中执行的。 最后三步看上去有点“绕”，将 redo log 的写入拆成了两个步骤：prepare 和 commit，这就是\"两阶段提交\"。 ","date":"2025-02-09","objectID":"/posts/2.%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B8%80%E6%9D%A1sql%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"02 | 日志系统：一条 SQL 更新语句是如何执行的？","uri":"/posts/2.%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B8%80%E6%9D%A1sql%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 两阶段提交 为什么必须有“两阶段提交”呢？这是为了让两份日志之间的逻辑一致。要说明这个问题，得从文章开头的那个问题说起：怎样让数据库恢复到半个月内任意一秒的状态？ binlog 会记录所有的逻辑操作，并且是采用“追加写”的形式。如果 DBA 承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有 binlog，同时系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。 当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做： 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库； 然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。 这样临时库就跟误删之前的线上库一样了，然后就可以把表数据从临时库取出来，按需要恢复到线上库去。说完了数据恢复过程，再回来说说，为什么日志需要“两阶段提交”。这里不妨用反证法来进行解释。 由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redo log 再写 binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。 仍然用前面的 update 语句来做例子。假设当前 ID=2 的行，字段 c 的值是 0，再假设执行 update 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash，会出现什么情况呢？ 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。 可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。你可能会说，这个概率是不是很低，平时也没有什么动不动就需要恢复临时库的场景呀？ 其实不是的，不只是误操作后需要用这个过程来恢复数据。当需要扩容的时候，也就是需要再多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用 binlog 来实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。 简单说，redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。 ","date":"2025-02-09","objectID":"/posts/2.%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B8%80%E6%9D%A1sql%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"02 | 日志系统：一条 SQL 更新语句是如何执行的？","uri":"/posts/2.%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B8%80%E6%9D%A1sql%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 小结 redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数建议设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。 sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数也建议设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。 还介绍了与 MySQL 日志系统密切相关的“两阶段提交”。两阶段提交是跨系统维持数据逻辑一致性时常用的一个方案，即使不做数据库内核开发，日常开发中也有可能会用到。 ","date":"2025-02-09","objectID":"/posts/2.%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B8%80%E6%9D%A1sql%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"02 | 日志系统：一条 SQL 更新语句是如何执行的？","uri":"/posts/2.%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B8%80%E6%9D%A1sql%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 问题 问：定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？ 答：好处是“最长恢复时间”更短。在一天一备的模式里，最坏情况下需要应用一天的 binlog。比如，你每天 0 点做一次全量备份，而要恢复出一个到昨天晚上 23 点的备份。一周一备最坏情况就要应用一周的 binlog 了。 当然这个是有成本的，因为更频繁全量备份需要消耗更多存储空间，所以这个 RTO（恢复目标时间）是成本换来的，就需要根据业务重要性来评估了。 ","date":"2025-02-09","objectID":"/posts/2.%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B8%80%E6%9D%A1sql%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"02 | 日志系统：一条 SQL 更新语句是如何执行的？","uri":"/posts/2.%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B8%80%E6%9D%A1sql%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"一条 SQL 查询语句是如何执行的？","date":"2025-02-08","objectID":"/posts/1.%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/","tags":["MySQL 实战 45 讲","MySQL"],"title":"01 | 基础架构：一条 SQL 查询语句是如何执行的？","uri":"/posts/1.%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":" 摘要 本文将把 MySQL 拆解一下，看看里面都有哪些“零件”，希望借由这个拆解过程，让你对 MySQL 有更深入的理解。 平时使用数据库，看到的通常都是一个整体。比如，你有个最简单的表，表里只有一个 ID 字段，在执行下面这个查询语句时： mysql\u003e select * from T where ID = 10； 我们看到的只是输入一条语句，返回一个结果，却不知道这条语句在 MySQL 内部的执行过程。下面的是 MySQL 的基本架构示意图，从中可以清楚地看到 SQL 语句在 MySQL 的各个功能模块中的执行过程。 大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。 Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。 也就是说，在执行 create table 建表的时候，如果不指定引擎类型，默认使用的就是 InnoDB。不过，也可以通过指定存储引擎的类型来选择别的引擎，比如在 create table 语句中使用 engine=memory, 来指定使用内存引擎创建表。不同存储引擎的表数据存取方式不同，支持的功能也不同。从图中不难看出，不同的存储引擎共用一个 Server 层，也就是从连接器到执行器的部分。 ","date":"2025-02-08","objectID":"/posts/1.%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/:0:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"01 | 基础架构：一条 SQL 查询语句是如何执行的？","uri":"/posts/1.%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"1 连接器 第一步，需要会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的： mysql -h$ip -P$port -u$user -p 输完命令之后，就需要在交互对话里面输入密码。虽然密码也可以直接跟在 -p 后面写在命令行中，但这样可能会导致密码泄露。如果连的是生产服务器，强烈建议不要这么做。连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是输入的用户名和密码。 如果用户名或密码不对，就会收到一个\"Access denied for user\"的错误，然后客户端程序结束执行。 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。 这就意味着，一个用户成功建立连接后，即使用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。 连接完成后，如果没有后续的动作，这个连接就处于空闲状态，可以在 show processlist 命令中看到它。文本中这个图是 show processlist 的结果，其中的 Command 列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。 客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒：Lost connection to MySQL server during query。这时候如果要继续，就需要重连，然后再执行请求了。 数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。建立连接的过程通常是比较复杂的，所以建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。 但是全部使用长连接后，可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。怎么解决这个问题呢？可以考虑以下两种方案。 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 ","date":"2025-02-08","objectID":"/posts/1.%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/:1:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"01 | 基础架构：一条 SQL 查询语句是如何执行的？","uri":"/posts/1.%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"2 查询缓存 连接建立完成后，就可以执行 select 语句了。执行逻辑就会来到第二步：查询缓存。 MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。 如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。可以看到，如果查询命中缓存，MySQL 不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。 但是大多数情况下建议不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能很费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。 好在 MySQL 也提供了这种“按需使用”的方式。可以将参数 query_cache_type 设置成 DEMAND，这样对于默认的 SQL 语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定，像下面这个语句一样： mysql\u003e select SQL_CACHE * from T where ID = 10； 需要注意的是，MySQL 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻底没有这个功能了。 ","date":"2025-02-08","objectID":"/posts/1.%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/:2:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"01 | 基础架构：一条 SQL 查询语句是如何执行的？","uri":"/posts/1.%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"3 分析器 如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析。 分析器先会做“词法分析”。输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。 MySQL 从输入的\"select\"这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID”。 做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断输入的这个 SQL 语句是否满足 MySQL 语法。 如果语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如下面这个语句 select 少打了开头的字母“s”。 mysql\u003e elect * from t where ID = 1; ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'elect * from t where ID=1' at line 1 一般语法错误会提示第一个出现错误的位置，所以要关注的是紧接“use near”的内容。 ","date":"2025-02-08","objectID":"/posts/1.%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/:3:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"01 | 基础架构：一条 SQL 查询语句是如何执行的？","uri":"/posts/1.%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"4 优化器 经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如执行下面这样的语句，这个语句是执行两个表的 join： mysql\u003e select * from t1 join t2 using(ID) where t1.c = 10 and t2.d = 20; 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。 这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。 ","date":"2025-02-08","objectID":"/posts/1.%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/:4:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"01 | 基础架构：一条 SQL 查询语句是如何执行的？","uri":"/posts/1.%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"5 执行器 MySQL 通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。 开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。 mysql\u003e select * from T where ID = 10; ERROR 1142 (42000): SELECT command denied to user 'b'@'localhost' for table 'T' 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。比如这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的： 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 至此，这个语句就执行完成了。 对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。 你会在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。 在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的。 ","date":"2025-02-08","objectID":"/posts/1.%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/:5:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"01 | 基础架构：一条 SQL 查询语句是如何执行的？","uri":"/posts/1.%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/"},{"categories":["MySQL 实战 45 讲","MySQL"],"content":"6 问题 问：如果表 T 中没有字段 k，而执行了这个语句 select * from T where k=1, 那肯定是会报“不存在这个列”的错误： “Unknown column‘k’in‘where clause’”。这个错误是在上面提到的哪个阶段报出来的呢？ 答：分析器 ","date":"2025-02-08","objectID":"/posts/1.%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/:6:0","tags":["MySQL 实战 45 讲","MySQL"],"title":"01 | 基础架构：一条 SQL 查询语句是如何执行的？","uri":"/posts/1.%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/"},{"categories":null,"content":"本页面记录了自「一个 PHP 菜鸟的心路历程」博客创建以来，所有的打赏记录。感谢大家的支持！","date":"2024-11-17","objectID":"/reward/","tags":null,"title":"赞赏记录","uri":"/reward/"},{"categories":null,"content":" 给博主买杯卡布奇诺～ 赞赏 支付宝 微信 本页面记录了自「一个 PHP 菜鸟的心路历程」博客创建以来，所有的打赏记录。感谢大家的支持！❤️ 总计 ¥666.00 单笔最大 w2lz 的 ¥666.00 w2lz ¥666.00 w2lz 通过 支付宝 打赏了 ¥666.00 备注：希望一切顺顺利利！ 2024-11-17 - 次阅读 ","date":"2024-11-17","objectID":"/reward/:0:0","tags":null,"title":"赞赏记录","uri":"/reward/"},{"categories":null,"content":"互联网的广大朋友们，欢迎光临我的小博客！欢迎留言！","date":"2024-11-17","objectID":"/guestbook/","tags":null,"title":"留言","uri":"/guestbook/"},{"categories":null,"content":"- 次阅读 Welcome 互联网的广大朋友们，欢迎光临我的小博客！欢迎留言！ 温馨提示，音乐自动播放，请带好耳机～ From playlist, Powered By mmt-netease 给博主买杯卡布奇诺～ 赞赏 支付宝 微信 - 次阅读 ","date":"2024-11-17","objectID":"/guestbook/:0:0","tags":null,"title":"留言","uri":"/guestbook/"},{"categories":null,"content":"「一个 PHP 菜鸟的心路历程」的友情链接","date":"2024-11-17","objectID":"/friends/","tags":null,"title":"友情链接","uri":"/friends/"},{"categories":null,"content":"本页共 - 次阅读 ","date":"2024-11-17","objectID":"/friends/:0:0","tags":null,"title":"友情链接","uri":"/friends/"},{"categories":null,"content":"基本信息 网络 ID：w2lz 头像：https://blog.yingnan.wang/images/avatar.jpg URL：https://blog.yingnan.wang 描述：一个 PHP 菜鸟的心路历程 - 「Talk is cheap. Show me the code.」（或者你对我的看法😉） ","date":"2024-11-17","objectID":"/friends/:1:0","tags":null,"title":"友情链接","uri":"/friends/"},{"categories":null,"content":"友链要求 友情提醒：那些不尊重他人劳动成果，转载不加出处的，或恶意行为的网站，还请你不要来进行交换了。 原创博客文章至少 10 篇以上，内容健康，无违法违规内容。 网站创建时间至少 3 个月以上，且长期保持更新。（频率至少一年 1 篇） 目前仅限个人非商业博客/网站。（商务合作邮件联系） 满足上述基本要求，互换友链请按以下格式在评论区留言： ```yaml - nickname: \u003cyour nickname\u003e avatar: \u003cyour avatar\u003e url: \u003cyour site link\u003e description: \u003cdescription of your site\u003e ``` ","date":"2024-11-17","objectID":"/friends/:2:0","tags":null,"title":"友情链接","uri":"/friends/"},{"categories":null,"content":"失效链接 公示一个月后删除，如更换域名请及时联系！ ","date":"2024-11-17","objectID":"/friends/:3:0","tags":null,"title":"友情链接","uri":"/friends/"},{"categories":null,"content":" 不卑不亢，不矜不伐，戒骄戒躁 不嗔不怒，不争不弃，独善其身 —— 自我期许 ","date":"2024-11-17","objectID":"/about/:0:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"关于作者 人知道的越多，就会发现无知的越多。有更广袤的世界可以探索，真是莫大的快乐！ 一个北漂的内蒙汉子，主要从事Golang、PHP和Java相关的开发工作。 👨‍💻 一名后端开发工程师 🫶 一个 PHP 菜鸟的心路历程 作者 📚 专业：计算机科学与技术  关注 Follow：w2lz  微信公众号：一个 PHP 菜鸟的心路历程 ","date":"2024-11-17","objectID":"/about/:1:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"关于博客 博客内容主要以后端开发方向为主，分享一些有趣程序、技巧、开发教程、心情和学习记录等。 你可以通过我的微信公众号、GitHub 或 RSS 来订阅本博客。 /** * Licensed under CC BY-NC-SA 4.0 * @since 2018-05-28 20:01:01 */ package main import \"fmt\" type Blog struct { Name string `json:\"name\"` Author string `json:\"author\"` Url string `json:\"url\"` } func main() { blog := Blog{ Name: \"一个 PHP 菜鸟的心路历程\", Author: \"王二愣子\", Url: \"https://blog.yingnan.wang\", } fmt.Println(blog.Name) } 博客历史 博客 使用 Hugo 搭建，使用 FixIt 主题，取名为“一个 PHP 菜鸟的心路历程”。 2024-11-17 23:00:00 ","date":"2024-11-17","objectID":"/about/:2:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"商务合作 广告投放、其他平台同步授权等事宜请邮件与我联系。 📮 邮箱：wangyingnan@88.com（备注来意） - 次阅读 ","date":"2024-11-17","objectID":"/about/:3:0","tags":null,"title":"关于","uri":"/about/"},{"categories":["GitHub"],"content":"w2lz/w2lz.github.io: 博客构建档（HTML \u0026 Markdown）","date":"2024-11-17","objectID":"/projects/w2lz/w2lz.github.io/","tags":[],"title":"博客构建档（HTML \u0026 Markdown）","uri":"/projects/w2lz/w2lz.github.io/"},{"categories":["GitHub"],"content":"一个 PHP 菜鸟的心路历程 共计 28 篇文章 ","date":"2024-11-17","objectID":"/projects/w2lz/w2lz.github.io/:1:0","tags":[],"title":"博客构建档（HTML \u0026 Markdown）","uri":"/projects/w2lz/w2lz.github.io/"},{"categories":["GitHub"],"content":"2025 共计 26 篇文章 02-16 25 | MySQL 是怎么保证高可用的？ 02-16 24 | MySQL 是怎么保证主备一致的？ 02-16 23 | MySQL 是怎么保证数据不丢的？ 02-16 22 | MySQL 有哪些“饮鸩止渴”提高性能的方法？ 02-16 21 | 为什么我只改一行的语句，锁这么多？ 02-16 20 | 幻读是什么，幻读有什么问题？ 02-16 19 | 为什么我只查一行的语句，也执行这么慢？ 02-16 18 | 为什么这些 SQL 语句逻辑相同，性能却差异巨大？ 02-16 17 | 如何正确地显示随机消息？ 02-16 16 | “order by”是怎么工作的？ 02-12 15 | 答疑文章（一）：日志和索引相关问题 02-12 14 | count(*) 这么慢，我该怎么办？ 02-12 13 | 为什么表数据删掉一半，表文件大小不变？ 02-12 12 | 为什么我的 MySQL 会“抖”一下？ 02-12 11 | 怎么给字符串字段加索引？ 02-12 10 | MySQL 为什么有时候会选错索引？ 02-12 1.Go 语言中如何访问私有成员？ 02-11 09 | 普通索引和唯一索引，应该怎么选择？ 02-11 08 | 事务到底是隔离的还是不隔离的？ 02-11 07 | 行锁功过：怎么减少行锁对性能的影响？ 02-11 06 | 全局锁和表锁：给表加个字段怎么有这么多阻碍？ 02-10 05 | 深入浅出索引（下） 02-10 04 | 深入浅出索引（上） 02-10 03 | 事务隔离：为什么你改了我还看不见？ 02-09 02 | 日志系统：一条 SQL 更新语句是如何执行的？ 02-08 01 | 基础架构：一条 SQL 查询语句是如何执行的？ ","date":"2024-11-17","objectID":"/projects/w2lz/w2lz.github.io/:2:0","tags":[],"title":"博客构建档（HTML \u0026 Markdown）","uri":"/projects/w2lz/w2lz.github.io/"},{"categories":["GitHub"],"content":"2024 共计 2 篇文章 11-17 博客构建档（HTML \u0026 Markdown） 11-17 一个 PHP 菜鸟的心路历程 ","date":"2024-11-17","objectID":"/projects/w2lz/w2lz.github.io/:3:0","tags":[],"title":"博客构建档（HTML \u0026 Markdown）","uri":"/projects/w2lz/w2lz.github.io/"},{"categories":["GitHub"],"content":"w2lz/hugo blog: 一个 PHP 菜鸟的心路历程","date":"2024-11-17","objectID":"/projects/w2lz/hugo-blog/","tags":[],"title":"一个 PHP 菜鸟的心路历程","uri":"/projects/w2lz/hugo-blog/"},{"categories":["GitHub"],"content":"一个 PHP 菜鸟的心路历程 站名“一个 PHP 菜鸟的心路历程”，主要是为了纪念刚入行的日子。 博客基于 Hugo 和 FixIt 搭建，建站的初衷不是为了炫耀所知，而是记录无知。 博客内容主要以 Web 后端开发方向为主，分享一些有趣程序、技巧、开发教程、心情和学习记录等。 你可以通过我的微信公众号、GitHub 或 RSS 来订阅本博客。 ","date":"2024-11-17","objectID":"/projects/w2lz/hugo-blog/:1:0","tags":[],"title":"一个 PHP 菜鸟的心路历程","uri":"/projects/w2lz/hugo-blog/"},{"categories":["GitHub"],"content":"Content 归档 分类 合集 标签 ","date":"2024-11-17","objectID":"/projects/w2lz/hugo-blog/:2:0","tags":[],"title":"一个 PHP 菜鸟的心路历程","uri":"/projects/w2lz/hugo-blog/"},{"categories":["GitHub"],"content":"Source 博客相关源码： Hugo FixIt 相关 更多 ","date":"2024-11-17","objectID":"/projects/w2lz/hugo-blog/:3:0","tags":[],"title":"一个 PHP 菜鸟的心路历程","uri":"/projects/w2lz/hugo-blog/"},{"categories":["GitHub"],"content":"Roadmap ","date":"2024-11-17","objectID":"/projects/w2lz/hugo-blog/:4:0","tags":[],"title":"一个 PHP 菜鸟的心路历程","uri":"/projects/w2lz/hugo-blog/"},{"categories":["GitHub"],"content":"Project setup 本博客已部署到 Vercel 和 GitHub Pages，工作流如下图所示： ▸ .github/ # GitHub configuration ▸ .scripts/ # custom scripts ▸ .shell/ # shell commands for hugo project, entrance: hugo_main.sh ▸ archetypes/ # page archetypes (like scaffolds of archetypes) ▸ assets/ # css, js, third-party libraries etc. ▸ config/ # configuration files ▸ content/ # markdown files for hugo project ▸ private/ # private submodule for encrypted content ▸ data/ # blog data (allow: yaml, json, toml), e.g. friends.yml ▸ public/ # build directory ▸ static/ # static files, e.g. favicon.ico ▸ themes/ # theme submodules ","date":"2024-11-17","objectID":"/projects/w2lz/hugo-blog/:5:0","tags":[],"title":"一个 PHP 菜鸟的心路历程","uri":"/projects/w2lz/hugo-blog/"},{"categories":["GitHub"],"content":"System requirements Go Hugo: \u003e= 0.134.1 (extended version) ","date":"2024-11-17","objectID":"/projects/w2lz/hugo-blog/:5:1","tags":[],"title":"一个 PHP 菜鸟的心路历程","uri":"/projects/w2lz/hugo-blog/"},{"categories":["GitHub"],"content":"Clone 首先点上 Star 😜，然后下载源码： git clone --recursive git@github.com:w2lz/hugo-blog.git \u0026\u0026 cd hugo-blog 下载源码后，可以通过下面的方式启动这个博客。 ","date":"2024-11-17","objectID":"/projects/w2lz/hugo-blog/:5:2","tags":[],"title":"一个 PHP 菜鸟的心路历程","uri":"/projects/w2lz/hugo-blog/"},{"categories":["GitHub"],"content":"Hugo # Development environment hugo server --disableFastRender --navigateToChanged --bind 0.0.0.0 -O # Production environment hugo server --disableFastRender --navigateToChanged --environment production --bind 0.0.0.0 -O ","date":"2024-11-17","objectID":"/projects/w2lz/hugo-blog/:5:3","tags":[],"title":"一个 PHP 菜鸟的心路历程","uri":"/projects/w2lz/hugo-blog/"},{"categories":["GitHub"],"content":"License 此存储库中的文本、图像和视频等内容采用 CC BY-NC-SA 4.0 许可 此存储库中的代码采用 MIT 许可 content/private 目录不在任何许可范围内 ","date":"2024-11-17","objectID":"/projects/w2lz/hugo-blog/:6:0","tags":[],"title":"一个 PHP 菜鸟的心路历程","uri":"/projects/w2lz/hugo-blog/"},{"categories":["GitHub"],"content":"Author w2lz ","date":"2024-11-17","objectID":"/projects/w2lz/hugo-blog/:7:0","tags":[],"title":"一个 PHP 菜鸟的心路历程","uri":"/projects/w2lz/hugo-blog/"},{"categories":null,"content":"一个 PHP 菜鸟的心路历程博客私密文章 ","date":"0001-01-01","objectID":"/readme/:1:0","tags":null,"title":"","uri":"/readme/"}]